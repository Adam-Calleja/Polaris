[
  {
    "id": "HPCSSUP-99134",
    "summary": "Please find attached for processing our Purchase Order MA2-4362647",
    "query": "Hello,\nPlease find attached for processing our PO MA2-4362647\nWe would be grateful if you would reply to this email with confirmation of receipt.\nIf required, the University's EORI number is 823 847 609 000.\nGoods-In opening hours are Monday to Friday 8am to 4pm (Closed for UK Bank Holidays).\nKind Regards\nStephen Green-Malloy\nSenior Stores Technician\n[cid:image001.jpg@01DC9B59.E6530F70]",
    "expected_answer": "CLASSIFICATION\nCategory: Procurement / Purchase Order Submission\nTechnical HPC Action Required (based on visible text): Unknown (cannot determine from message alone)\nFinance/Procurement Action Required: Yes (PO processing workflow)\nUrgency: Low\n\nQUICK ASSESSMENT\nThe sender has submitted Purchase Order MA2-4362647 and asked for confirmation of receipt. The message also provides EORI information (if needed) and Goods-In opening hours. Based on the visible ticket text alone, the safe and correct handling is to:\n1) confirm receipt,\n2) review the attached PO to determine what action is required (e.g., invoicing/procurement processing and whether any HPC-specific actions such as crediting resource hours apply),\n3) route the PO to the appropriate Finance/Procurement workflow,\n4) request any missing details required to proceed.\n\nACTION STEPS (HELPDESK)\n\n1) Verify attachment availability (internal)\n   - Confirm a PO document is attached and readable in the ticketing system.\n   - Confirm the PO number MA2-4362647 appears on the document.\n\n2) Determine required processing path (internal)\n   - From the PO document, identify:\n     a) supplier / contact details (if not already clear),\n     b) what is being purchased (e.g., equipment/services vs. HPC resource/usage credit),\n     c) quantity/amount (if applicable),\n     d) any project/account/cost-centre information required for processing.\n   - If the PO indicates an HPC-specific deliverable (e.g., purchase of compute/resource hours), escalate/route to the HPC finance workflow and request confirmation of the target project/account and quantity if not explicit.\n   - If the PO is general procurement, route to the standard Procurement/Finance mailbox.\n\n3) Forward to Finance/Procurement (internal)\n   - Forward the PO (and ticket context) to the appropriate Finance/Procurement contact point.\n   - Include PO number in subject and internal note for traceability.\n\n4) Respond to sender with confirmation of receipt (external)\n   - Send a brief confirmation that the PO was received and is being processed.\n   - Do NOT claim any internal processing step has completed unless it has been verified (e.g., do not claim hours were credited, accounts updated, invoices raised, etc.).\n\n5) Ticket update and closure (internal)\n   - Resolve the ticket once confirmation of receipt has been sent and the PO has been routed appropriately.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\nAsk only if the PO/attachment does not clearly specify these items:\n- Which project/account/cost centre should this be associated with?\n- What exactly is being purchased (e.g., HPC resource/compute time vs. goods/services)?\n- If compute/resource is being purchased: how many hours/units should be applied?\n\nEXAMPLE CUSTOMER REPLY (CONFIRMATION OF RECEIPT)\n\nSubject: Re: Purchase Order MA2-4362647 – Confirmation of Receipt\n\nDear Stephen,\n\nThank you for your email.\n\nWe confirm receipt of Purchase Order MA2-4362647. We will review the attached PO and have forwarded it to the appropriate team for processing. If any further information is required, we will contact you.\n\nKind regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not state or imply that any resource/account changes have been made unless explicitly confirmed by the operator performing the action.\n- Do not guess quantities, project codes, or required technical steps from the email alone when attachments are not visible to the assistant.\n- EORI and Goods-In details do not require action unless Finance/Procurement requests them as part of the processing workflow.",
    "metadata": {
      "keyword": "PO/ purchase order",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98836",
    "summary": "Missing Files After RDS Directory Change?",
    "query": "Hello,\nI am a user of CSD3 (ejs237, dp012) and recently noticed that I can no longer access my files at\n/rds/user/ejs237/rds-dirac-dp012/ejs237/.\nI was informed that the directory has been moved to\n/rds/project/dirac_vol2/rds-dirac-dp012, but this location does not appear to be valid or accessible in my case.\nCould you please let me know where my previous data have been moved?\nThank you very much for your help.\nBest regards,\nEun-jin Shin",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / Path & Symlink / RDS\nTechnical Action Required: Yes (diagnose path/symlink + confirm correct new location)\nEscalation Required: Possibly (if storage path mapping needs staff-side confirmation)\nUrgency: Medium (data access blocked)\n\nQUICK ASSESSMENT\nUser can no longer access files under /rds/user/ejs237/rds-dirac-dp012/ejs237/ and has been told the directory moved to /rds/project/dirac_vol2/rds-dirac-dp012, but that new location is not valid/accessible for them. On CSD3, /rds is Lustre-based storage and user-visible paths under /rds/user/ejs237/ are often exposed via convenience symlinks under ~/rds. Therefore, the most likely cause is that the user is following an outdated or broken path/symlink after a storage reorganisation. Next steps are to (1) inspect the relevant symlinks, (2) identify the correct new target path, and (3) update the user’s link(s) accordingly.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm what the user is actually accessing\n   Ask the user to run:\n     hostname\n     ls -ld /rds/user/ejs237\n     ls -l ~/rds\n   Purpose: confirm they are on CSD3 and see whether ~/rds contains a symlink for the relevant project area.\n\n2) Identify whether the path is a symlink and where it points\n   Ask the user to run (adjust name to the relevant project directory label they used before):\n     ls -l ~/rds\n     readlink -f ~/rds/rds-dirac-dp012\n     ls -ld ~/rds/rds-dirac-dp012\n   Expected signal:\n     - If readlink fails or points to a non-existent location, the link is broken/outdated.\n     - If it points somewhere valid but permissions deny, it may be a membership/ACL issue.\n\n3) Determine the correct new location (helpdesk-side confirmation)\n   Because the user reports /rds/project/dirac_vol2/rds-dirac-dp012 “does not appear valid”, do NOT assume the target.\n   Use internal knowledge/tools (or storage team confirmation) to identify the correct canonical path for this project’s data on the current filesystem layout.\n\n4) Update the user-facing link/path (only once the correct target is confirmed)\n   If the correct target path has been confirmed, instruct:\n     cd ~/rds\n     rm rds-dirac-dp012\n     ln -s <CONFIRMED_TARGET_PATH> rds-dirac-dp012\n     ls -ld rds-dirac-dp012\n   Notes:\n     - Use rm only for the symlink in ~/rds, not for real directories.\n     - If rds-dirac-dp012 is not in ~/rds but the user relies on a deep /rds/user path, provide the correct canonical path they should use going forward.\n\n5) Validate access\n   Ask the user to run:\n     ls -lah ~/rds/rds-dirac-dp012\n     ls -lah <CONFIRMED_TARGET_PATH>\n   Confirm they can see expected directories/files.\n\nIF THIS FAILS (LIKELY BRANCHES)\n\nA) Permission denied at the new target\n   - Confirm the user’s project membership and directory ACLs/groups.\n   - Ask for:\n       id\n       groups\n       ls -ld <CONFIRMED_TARGET_PATH>\n   - Escalate to storage admins if ACL/group membership needs adjustment.\n\nB) Target path truly does not exist\n   - The “dirac_vol2” path the user was given may be incorrect or incomplete.\n   - Re-check the correct storage mapping for that project (staff-side), then provide the correct path and update symlink accordingly.\n\nC) User is using the wrong environment/path for I/O\n   - Remind them that active data should live on /rds (not /home) and that /rds/user areas may be presented via ~/rds symlinks for convenience.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- What exact error do you see when accessing the old and new paths? (e.g., \"No such file or directory\" vs \"Permission denied\")\n- What is the exact project directory name you used previously under /rds/user/ejs237/ ?\n- Please paste output of:\n    ls -l ~/rds\n    readlink -f ~/rds/rds-dirac-dp012\n\nEXAMPLE CUSTOMER REPLY (USER-FACING)\n\nSubject: Re: Missing Files After RDS Directory Change?\n\nHello Eun-jin,\n\nThanks for getting in touch. On CSD3, many project directories under /rds/user/ejs237/ are typically accessed via convenience symlinks under ~/rds.\n\nCould you please run the following and paste the output?\n\n  ls -l ~/rds\n  readlink -f ~/rds/rds-dirac-dp012\n\nThis will let us confirm whether the path you’re using is a broken/outdated symlink following the directory move. Once we confirm the correct new location for your project data, we can point your ~/rds link to the right place.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not guess the new canonical target path; confirm it before instructing link changes.\n- Avoid destructive commands on /rds; only remove symlinks in ~/rds when you are certain they are symlinks.\n- Do not claim files were moved or recovered unless you have explicit evidence of the correct new location and access has been validated.",
    "metadata": {
      "keyword": "RDS/ Research Data Storage",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98820",
    "summary": "RDS and RCS licence renewal",
    "query": "Dear Storage Services team,\nWe would like to renew our licences for both the RDS and RCS  (account 80/81) for another year.\nOn top of this is it possible to change the ownership of these licences to Maria (Cc’ed) who is our new head of bioinformatics?\nMany thanks\nBrian\n**\nDr. Brian Lam\nAssistant Research Professor\nInstitute of Metabolic Science-Metabolic Research Laboratories\nMedical Research Council Metabolic Diseases Unit\nUniversity of Cambridge\nInstitute of Metabolic Science\nLevel 4, Box 289, Addenbrooke's Hospital\nCambridge CB2 0QQ\nUnited Kingdom\nPhone: +44 (0)1223 768628\nEmail: yhbl2@cam.ac.uk<[yhbl2@cam.ac.uk](mailto:yhbl2@cam.ac.uk)>",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / Licence Management\nTechnical Action Required: No\nPortal Action Required: Yes (Self Service)\nUrgency: Low\n\nQUICK ASSESSMENT\nUser requests renewal of RDS and RCS licences and transfer of licence ownership. These actions are handled via the Self Service Storage Portal.\n\nACTION\nDirect user to:\nhttps://selfservice.uis.cam.ac.uk/\n\nDo not process renewal or ownership changes manually via ticket.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Brian,\n\nThese actions can be taken within the relevant storage licences on the Self Service Storage Portal:\nhttps://selfservice.uis.cam.ac.uk/\n\nPlease let us know if you encounter any difficulties.\n\nBest regards,  \nHPC Support",
    "metadata": {
      "keyword": "RDS/ Research Data Storage",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98814",
    "summary": "Login issues",
    "query": "Dear officer,\nI have some issues with login via this page. My colleague has added me as the co-leader in this project.\nHowever, when I click on the \" the “Sign in with Keycloak”\" it didn't transfer me to the University of Liverpool credentials page. I tried to login anyway with my liverpool account but it doesn't work.\nCould you kindly help me how should I login to this portal? Thank you very much for your time and help. Have a nice day.\nBest regards,\nYingjie Wang\n________________________________\nFrom: airr-admin@hpc.cam.ac.uk <airr-admin@hpc.cam.ac.uk>\nSent: Friday, January 30, 2026 13:07\nTo: Wang, Yingjie [yjwang] <Yingjie.Wang2@liverpool.ac.uk>\nSubject: Invitation to Multi-Agent Semantic Annealing Attack on LLMs Prompts, Phase 2 project\nCAUTION: This email originated outside of the University. Do not click links unless you can verify the source of this email and know the content is safe. Check sender address, hover over URLs, and don't open suspicious email attachments.\nHello!\nYi Dong has invited you to join Multi-Agent Semantic Annealing Attack on LLMs Prompts, Phase 2 project in Co-investigator (can invite people) role.\nPlease visit this page<[https://access.hpc.cam.ac.uk/invitation/61e3977a84024e4296081056ee22cda1/](https://access.hpc.cam.ac.uk/invitation/61e3977a84024e4296081056ee22cda1/)> to sign up and accept your invitation. Please note: this invitation expires at 06.02.2026 13:07!\nKind regards,\nResearch Computing Services\nUniversity Information Services\nUniversity of Cambridge\nPlease email support@hpc.cam.ac.uk<[support@hpc.cam.ac.uk](mailto:support@hpc.cam.ac.uk)> if any problems are encountered.",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / Portal Login\nTechnical Action Required: No (user guidance only)\nEscalation Required: No\nUrgency: Medium (access blocked)\n\nQUICK ASSESSMENT\nUser is attempting to log in via “Sign in with Keycloak” and expects to authenticate using University of Liverpool credentials. The correct login method for external users is via the “MyAccessID” option beneath the login boxes.\n\nACTION\nInstruct user to:\n- Click the “MyAccessID” button under the login fields.\n- Authenticate using their institutional credentials.\n- Follow official documentation if required.\n\nDo not:\n- Reset credentials.\n- Modify account settings.\n- Escalate unless MyAccessID authentication fails.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Yingjie,\n\nYou need to click the “MyAccessID” button underneath the login boxes to log in with your university credentials.\n\nPlease see this guidance:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/pvc.html#users-accessing-open-on-demand-via-the-portal\n\nBest regards,\nHPC Support",
    "metadata": {
      "keyword": "login",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98799",
    "summary": "Copy-paste text between local and SRCP",
    "query": "Dear HPC support team,\nI was wondering if there is a way to copy paste text between local machine and the SRCP virtual desktop in browser? I've tried using the clipboard icon on the TurboVNC side toolbar, which is supposed to work as a medium for this, but it doesn't seem to sync properly (only picks up text from my local machine, cannot be pasted into the virtual desktop).\nIs there an alternative method for transferring text?\nAlternatively, is there a way to log into SRCP via ssh in the terminal? That would make text transfers and edits much easier.\nBest,\nShilin",
    "expected_answer": "CLASSIFICATION\nCategory: SRCP / Secure Desktop / Data Transfer\nTechnical Action Required: No (policy clarification only)\nSecurity Impact: Yes (clipboard isolation is intentional)\nUrgency: Low\n\nQUICK ASSESSMENT\nUser is attempting to copy/paste text between their local machine and the SRCP virtual desktop (TurboVNC) and reports that clipboard sync does not function bidirectionally. They also ask whether SSH access to SRCP is possible as an alternative.\n\nClipboard isolation is enforced by design under SRCP security policy. Direct copy/paste between local and SRCP environments is not permitted. SSH access is not offered as a workaround.\n\nACTION\nInform user that:\n- Copy/paste from outside SRCP into the platform is not permitted due to security policy.\n- Data/text transfer must occur via approved secure channels.\n- Provide documentation link for Secure Windows Desktop (SWD) guidance.\n\nDo not:\n- Suggest policy bypass.\n- Recommend enabling clipboard sharing.\n- Offer SSH access as an alternative method.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Shilin,\n\nThe copy-paste function from outside of the SRCP is not possible, as this would violate SRCP security policies.  \n\nYou can transfer data or files into the platform using the approved secure channels described here:\nhttps://docs.hpc.cam.ac.uk/srcp/swd/index.html\n\nBest regards,  \nHPC Support",
    "metadata": {
      "keyword": "SRCP/ Secure Research Computing Platform",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98754",
    "summary": "Quote for RDS storage costs",
    "query": "Good morning,\nWe are extending the duration of our team’s RCS and RDS licenses for one more year.\nI have already obtained a quote for duration extension of our RCS storage (storage-6063) from the Self Service. This is good.\nI would like to now request a bespoke quote for duration extension of our RDS licence that cannot be generated via the Self Service.\nWe have paid for 700TB of RDS until 21 April 2026 [https://selfservice.uis.cam.ac.uk/storage/project/114/](https://selfservice.uis.cam.ac.uk/storage/project/114/)\nFirstly, to align our RDS with RCS expiry date, we would like to rebase this date so that they both expire on the same date.\nSecondly, we only require 400TB of RDS for next year and I have already reduced our Lustre data to below 400TB.\nTherefore, we need a quote for our next year’s RDS licence to look like this:\nDuration Extension              Research Data Store     400 TB  Serena Nik-Zainal (sn206)       28 Feb 2026     28 Feb 2027\nCould you please provide a quote for these RDS storage costs?\nI believe it should cost around:\n400TB*£39.36 per TB per year = £15,744\nMinus 52 days between 28 Feb 2026 and 21 April 2026 for which we have paid already:\n52/365*(700TB*£39.36) = £3,925\nTotal £15,744-£3,925 = £11,819\nOnce I have received your final quote, I will contact our finance team to produce a Purchase Order.\nRegards\nYasin",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RDS Licence / Custom Quote\nTechnical Action Required: Yes (licence update + filesystem change)\nFinance Action Required: Yes (formal quotation + PO required)\nEscalation Required: Yes (Storage Services team)\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser requests:\n- Custom quote for RDS duration extension\n- Reduction from 700TB to 400TB\n- Rebase of licence expiry date\n- Confirmation of cost calculation\n- Later: deletion of specific directories\n\nSelf Service cannot generate this quote; Storage Services must issue a formal quotation and reference.\n\nPUBLISHED PRICING (Research Data Store)\nMinimum quantity: 1TB\nMinimum term: 1 year\nStandard price: £54.00 per TB per year\nVolume price: £39.36 per TB per year\n\nPROVISIONAL ESTIMATE (BASED ON PUBLISHED PRICING)\n\n400TB × £39.36 = £15,744.00\n\nPro-rata adjustment for 52 days already paid on 700TB:\n700 × £39.36 = £27,552.00\n52/365 × £27,552.00 ≈ £3,925.12\n\nEstimated payable:\n£15,744.00 − £3,925.12 ≈ £11,818.88\n\n(Note: Minor rounding differences may occur depending on calculation method.)\n\nACTION\n\n1) Escalate to Storage Services\n   - Confirm revised capacity (400TB)\n   - Confirm new start/end dates\n   - Calculate official extension cost and pro-rata adjustment\n   - Issue formal quotation with reference number\n\n2) Await PO\n   - Do not update portal or filesystem until PO is received.\n\n3) Upon receipt of PO\n   - Update storage licence in portal (capacity + duration)\n   - Apply corresponding filesystem quota changes.\n\nEXAMPLE CUSTOMER REPLY (PROVISIONAL ESTIMATE)\n\nHi Yasin,\n\nBased on the published RDS volume pricing of £39.36 per TB per year, a 400TB extension for one year would be:\n\n400 × £39.36 = £15,744.00\n\nApplying a pro-rata adjustment for the 52 days already paid on the existing 700TB allocation:\n\nEstimated adjustment ≈ £3,925.12\n\nEstimated total payable ≈ £11,818.88\n\nPlease note this is an estimate based on published pricing. A formal quotation with reference number will be issued by the Storage Services team shortly.\n\nKind regards,\nHPC Support\n\nEXAMPLE CUSTOMER REPLY (FORMAL QUOTE)\n\nHi Yasin,\n\nMany thanks for getting in touch. Please find the formal quotation below:\n\n[Quote block with reference number and confirmed totals]\n\nKind regards,\nStorage Services Team\n\nEXAMPLE CUSTOMER REPLY (POST-PO)\n\nHi Yasin,\n\nMany thanks. I have updated your project in the portal and on the filesystem.\n\nKind regards,\nRCS Team",
    "metadata": {
      "keyword": "quote",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98750",
    "summary": "Purchase Order for RDS Duration Extension",
    "query": "Good morning,\nWe are extending the duration of our team’s RCS and RDS licenses for one more year.\nI have already obtained a quote for duration extension of our RCS storage (storage-6063) from the Self Service. This is good.\nI would like to now request a bespoke quote for duration extension of our RDS licence that cannot be generated via the Self Service.\nWe have paid for 700TB of RDS until 21 April 2026 [https://selfservice.uis.cam.ac.uk/storage/project/114/](https://selfservice.uis.cam.ac.uk/storage/project/114/)\nFirstly, to align our RDS with RCS expiry date, we would like to rebase this date so that they both expire on the same date.\nSecondly, we only require 400TB of RDS for next year and I have already reduced our Lustre data to below 400TB.\nTherefore, we need a Purchase Order so that our next year’s RDS licence would look like this:\nDuration Extension              Research Data Store     400 TB  Serena Nik-Zainal (sn206)       28 Feb 2026     28 Feb 2027\nI believe it should cost around:\n400TB*£39.36 per TB per year = £15,744\nMinus 52 days between 28 Feb 2026 and 21 April 2026 for which we have paid already:\n52/365*(700TB*£39.36) = £3,925\nTotal £15,744-£3,925 = £11,819\nCould you kindly produce this Purchase Order for us?\nRegards\nYasin",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RDS Licence / Procurement (PO)\nTechnical Action Required: No\nFinance Action Required: Yes (user departmental finance raises PO)\nEscalation Required: Yes (storage reduction + bespoke quote request to Storage Services)\nUrgency: Low\n\nQUICK ASSESSMENT\nUser is asking HPC Support to produce a Purchase Order for an RDS duration extension and storage reduction. HPC Support cannot raise departmental purchase orders because we do not have access to the user’s funding points. The user must request the PO via their department finance team. The storage reduction / bespoke RDS quote request should be submitted to support@hpc.cam.ac.uk for escalation to the relevant Storage Services team.\n\nACTION\n- Explain we cannot produce POs on behalf of users.\n- Tell user to contact their departmental finance team to raise the PO.\n- Tell user to submit the storage reduction / bespoke RDS quote request to support@hpc.cam.ac.uk so it can be escalated.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Yasin,\n\nWe do not produce purchase orders for users of our services, as we do not have access to the funding points used for departmental purchase orders. You will need to contact your finance team within your department to raise the PO.\n\nPlease submit your enquiry for reducing storage to support@hpc.cam.ac.uk so we can escalate it to the relevant team.\n\nBest regards,\nHPC Support",
    "metadata": {
      "keyword": "PO/ purchase order",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98706",
    "summary": "MFA reset request",
    "query": "Dear HPC,\nI switched phone but did not port the MFA information to my new device.\nWould you be able to help me reset this?\nMany thanks,\nBart.",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / MFA (TOTP) / Reset\nTechnical Action Required: Possibly (staff-side MFA reset after identity verification)\nEscalation Required: No\nUrgency: Medium (access blocked)\n\nQUICK ASSESSMENT\nUser switched phone and no longer has their MFA codes. First check whether they can transfer authenticator codes from the old phone. If not, MFA reset is possible but requires identity verification (quick video call + photo ID). After reset, user must re-enrol by logging in (SSH and/or web) to obtain a new QR code.\n\nACTION\n\n1) Ask if they still have the old phone\n   - If yes: advise transferring codes using vendor guidance:\n     - Microsoft Authenticator backup/restore\n     - Google Authenticator transfer/export\n\n2) If transfer not possible: arrange MFA reset with identity verification\n   - Set up a short MS Teams video call (≤ 5 minutes)\n   - User presents photo ID for verification\n   - Offer times within service desk hours (Mon–Fri 9am–5pm, excl. holidays)\n\n3) After reset: instruct user how to obtain a new QR / TOTP\n   - SSH method:\n     ssh <CRSid>@multi.hpc.cam.ac.uk\n     Enter UIS/Raven password when prompted\n     Scan the displayed QR code into authenticator app\n     Expect a token labelled similar to “CSD3:SSH Login”\n   - Web portal method:\n     Log in as normal; a QR code will be shown to enrol\n     Expect a token labelled similar to “CSD3:<CRSid>”\n\n4) Provide official MFA documentation link\n\nEXAMPLE CUSTOMER REPLY (INITIAL)\n\nHello Bart,\n\nHave you still got your old phone? If so, you may be able to transfer the authenticator codes to your new phone.\n- Microsoft Authenticator: https://support.microsoft.com/en-us/account-billing/back-up-account-credentials-in-microsoft-authenticator-bb939936-7a8d-4e88-bc43-49bc1a700a40\n- Google Authenticator: https://support.google.com/accounts/answer/1066447?hl=en&co=GENIE.Platform%3DAndroid\n\nIf you can’t transfer the codes over, we can reset your MFA. To do so we’ll need a quick video call (no more than 5 minutes) where you can present photo ID for verification. Please let me know a suitable time and date (Service Desk hours are 9am–5pm, Mon–Fri, excluding holidays) and I can send an MS Teams invite.\n\nBest regards,\nHPC Support\n\nEXAMPLE CUSTOMER REPLY (POST-RESET INSTRUCTIONS)\n\nHello Bart,\n\nThis is to confirm that your MFA has been reset. To enrol a new TOTP via SSH, please run:\n\nssh <CRSid>@multi.hpc.cam.ac.uk\n\nYou will be prompted for your UIS/Raven password and then shown a QR code to scan into your authenticator app. If all goes well, you should see a token labelled “CSD3:SSH Login” (or similar).\n\nIf you use the web interface, log in as normal and a QR code will appear for you to scan. This token is typically labelled “CSD3:<CRSid>”.\n\nDocumentation:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not reset MFA without identity verification.\n- Do not request sensitive documents over email; verification should be done via the agreed video call process.\n- Do not claim MFA has been reset unless the staff-side action has been completed.",
    "metadata": {
      "keyword": "MFA",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98611",
    "summary": "Question about apparent discrepancy between quota and disk usage on RCS",
    "query": "Hi HPC Support Team,\nI’m writing to ask about a discrepancy I’ve noticed between the reported quota usage and the disk usage shown by du on our RCS space.\nWhen I check the usage for rcs1 (rcs-zz485-sem-lab-cold) using quota, it reports approximately 1.17 TB in use. However, when I run du -h on the same RCS directory, it reports a total size of around 29 TB. In addition, I can confirm that my own subdirectory alone (/woody) is significantly larger than the ~45 GB indicated by the quota output.\nThis has made us a bit concerned about: whether all data on the RCS space is being correctly accounted for by the quota system, and whether there is any issue with how usage is being reported.\nCould you please advise on why there might be, and whether this is expected behaviour for RCS storage?\nMany thanks for your help.\nAll the best,\nWoody\n[cid:2e4c979f-7f13-4964-85b8-9b8614326b0f@GBRP265.PROD.OUTLOOK.COM]\n[cid:e1468c0f-87a3-40d2-99da-259426e89246@GBRP265.PROD.OUTLOOK.COM]",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RCS / Quota Reporting\nTechnical Action Required: No (user guidance only)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser reports discrepancy between:\n- `quota` showing ~1.17 TB used\n- `du -h` showing ~29 TB\n- Their own subdirectory appearing much larger than the quota-reported figure\n\nOn RCS (Research Cold Store), this behaviour is expected. RCS is tape-backed cold storage, and quota reporting is not updated/cached in real time due to the nature of the underlying system. Additionally, `du` by default reports allocated block usage, whereas quota on RCS reflects the apparent file size accounted by the storage system.\n\nThe correct way to estimate logical usage on RCS is to use:\n\n    du --apparent-size\n\nACTION STEPS (HELPDESK)\n\n1) Clarify storage type\n   - Confirm the path is on RCS (e.g. /rcs/project/...).\n   - Explain that RCS behaves differently from RDS or /home.\n\n2) Explain expected discrepancy\n   - RCS is tape-backed cold storage.\n   - Quota values are not continuously refreshed.\n   - `du -h` reports on-disk block usage.\n   - Sparse files, compression, and tape staging can cause differences.\n\n3) Provide correct diagnostic command\n   Ask the user to run:\n     du -sh --apparent-size /rcs/project/<project-name>\n   or for a subdirectory:\n     du -sh --apparent-size /rcs/project/<project-name>/woody\n\n4) Reassure\n   - This does not indicate data loss.\n   - This does not indicate unaccounted storage.\n   - This is expected behaviour for RCS.\n\n5) Close once confirmed resolved.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- Please confirm the exact RCS path you are checking.\n- Please share the output of:\n    quota\n    du -sh /rcs/project/<project>\n    du -sh --apparent-size /rcs/project/<project>\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Quota vs Disk Usage on RCS\n\nHello Woody,\n\nThank you for your message.\n\nThis behaviour is expected on RCS. Because RCS is tape-backed cold storage, quota information is not updated in real time, and the `quota` command does not always reflect the apparent size reported by standard `du`.\n\nTo obtain a figure that more closely matches the logical file size used for quota purposes, please run:\n\n  du -sh --apparent-size /rcs/project/rcs-zz485-sem-lab-cold\n\nYou can also apply this to individual subdirectories (e.g. your `/woody` directory).\n\nThe discrepancy you are seeing does not indicate missing or unaccounted data — it is a result of how RCS storage and quota reporting operate.\n\nPlease let us know if you have any further concerns.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply quota malfunction unless confirmed by storage administrators.\n- Do not suggest data loss without evidence.\n- Avoid recommending destructive commands.\n- Ensure users understand that RCS differs from RDS in quota behaviour.",
    "metadata": {
      "keyword": "quota",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98608",
    "summary": "Enquiry about my account name",
    "query": "Dear Henry,\nI hope this email finds you well!\nI would like to load lammps gpu in my account, but I got the error of \"sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified\". My slurm script is attached. Could you please let me know the correct format of the slurm script?\nMany thanks!\nBest Regards,\nRuitian\n-------------------------------------------------\nDr. Ruitian He\nPostdoctoral Research Assistant\nOxford Thermofluids Institute\nDepartment of Engineering Science\nUniversity of Oxford\nOxford, OX2 0ES",
    "expected_answer": "CLASSIFICATION\nCategory: Software / GPU Compatibility / Dawn (PVC)\nTechnical Action Required: No (capability clarification only)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser asks whether the GPU version of DeepMD can be used on Dawn, noting that it requires a compatible NVIDIA driver.\n\nDawn nodes use Intel Ponte Vecchio (PVC) GPUs, not NVIDIA GPUs. NVIDIA CUDA drivers are not available on Dawn. Therefore, software that requires NVIDIA GPUs / CUDA will not run on Dawn unless it supports Intel GPUs (e.g. via oneAPI, SYCL, or other compatible backend).\n\nACTION\n\n1) Clarify hardware\n   - Dawn uses Intel PVC GPUs.\n   - NVIDIA drivers / CUDA are not available.\n\n2) State compatibility condition\n   - If DeepMD GPU version requires NVIDIA CUDA, it will not run on Dawn.\n   - If DeepMD provides a CPU-only build or an Intel GPU-compatible backend, that may be possible instead.\n\n3) Suggest alternatives\n   - If NVIDIA GPUs are required, recommend using an NVIDIA partition (e.g. Ampere/A100) instead.\n   - Direct user to Dawn hardware documentation.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: DeepMD GPU version on Dawn\n\nHello Ruitian,\n\nDawn uses Intel Ponte Vecchio (PVC) GPUs rather than NVIDIA GPUs. As such, NVIDIA drivers and CUDA are not available on this system.\n\nIf the GPU version of DeepMD requires NVIDIA/CUDA specifically, it will not be compatible with Dawn. You may wish to check whether DeepMD provides either:\n- a CPU-only build, or\n- support for Intel GPUs (e.g. via oneAPI/SYCL).\n\nIf NVIDIA GPUs are required, you would need to use an NVIDIA-based partition (e.g. Ampere).\n\nYou can find details of Dawn hardware here:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/pvc.html#hardware\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply NVIDIA drivers can be installed on Dawn.\n- Do not suggest unsupported driver installation.\n- Do not guarantee software compatibility without confirmed vendor support.",
    "metadata": {
      "keyword": "Dawn",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98575",
    "summary": "Query on extending SL3 CPU usage and annual reset",
    "query": "Dear HPC Group,\nI am Wenhao Xu, a PhD student in the Department of Clinical Neurosciences, working in Prof. Peter Smielewski’s group. I am writing to ask about extending our SL3 CPU allocation, as my current usage on account wx266 has reached the limit.\nAccording to mybalance records, I have used 380,694 CPU hours under Account SMIELEWSKI-SL3-CPU over the last year, and I would be grateful if you could advise:\n1.\nWhether it is possible to request additional CPU hours for continued work this year; and\n1.\nWhether SL3 free usage reset annually, and if so, when the reset could occur.\nIf you need any additional details, please let me know.\nMany thanks for your help.\nBest,\n–\nWenhao Xu\nPhD candidate, Department of Clinical Neurosciences\nClare Hall, University of Cambridge",
    "expected_answer": "CLASSIFICATION\nCategory: SL3 / CPU Credits / Service Level Policy\nTechnical Action Required: No\nFinance Action Required: No\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser reports reaching SL3 CPU allocation and asks:\n1) Whether additional CPU hours can be requested.\n2) Whether SL3 free usage resets annually and when.\n\nSL3 CPU credits are allocated at the project account level (shared across all users on that project). Credits are refreshed quarterly (not annually) at the start of each academic quarter.\n\nACTION\n\n1) Clarify allocation model\n   - SL3 CPU credits are assigned to the project account.\n   - Usage is shared across all members of that project.\n   - Individual users do not receive personal allocations.\n\n2) Clarify refresh schedule\n   - Credits refresh at the start of each academic quarter:\n     • 1 February – 30 April\n     • 1 May – 31 July\n     • 1 August – 31 October\n     • 1 November – 31 January\n\n3) Direct to policy documentation\n   - Service levels and SL3 policy page.\n\n4) Do not:\n   - Manually add credits.\n   - Promise additional allocation.\n   - Escalate unless there is a policy exception.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: SL3 CPU allocation and reset\n\nHello Wenhao,\n\nSL3 CPU credits are allocated to the project account and are shared between all users who have access to that account — they are not assigned per individual user.\n\nSL3 project credits are refreshed at the start of each academic quarter (three-month periods):\n- 1 February – 30 April\n- 1 May – 31 July\n- 1 August – 31 October\n- 1 November – 31 January\n\nYou can read more about SL3 service levels and policies here:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/policies.html#service-levels\n\nPlease let us know if you have any further questions.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply additional CPU hours can be granted outside policy.\n- Do not treat SL3 credits as personal allocations.\n- Do not promise resets outside the defined quarterly schedule.",
    "metadata": {
      "keyword": "usage",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98537",
    "summary": "Interactive Jupyter Notebooks crashing before loading",
    "query": "Hi Team,\nI am having an issue with my interactive OnDemand notebooks. They seem to reach the point of Starting and then crash before they can actually begin. Here is an output.log:\n```\n/var/spool/slurm/slurmd/job21509569/slurm_script: line 3: module: command not found\nScript starting...\nWaiting for Jupyter Notebook server to open port 61834...\nTIMING - Starting main script at: Fri Jan 30 15:00:10 GMT 2026\nTIMING - Starting wait at: Fri Jan 30 15:00:10 GMT 2026\n/home/tsl30/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/csd3/output/cad586e3-86c7-48bb-b093-8249dc27ee3f/script.sh: line 14: module: command not found\n/home/tsl30/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/csd3/output/cad586e3-86c7-48bb-b093-8249dc27ee3f/script.sh: line 17: module: command not found\n/home/tsl30/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/csd3/output/cad586e3-86c7-48bb-b093-8249dc27ee3f/script.sh: line 20: module: command not found\nTIMING - Starting jupyter at: Fri Jan 30 15:00:10 GMT 2026\n+ jupyter notebook --config=/home/tsl30/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/csd3/output/cad586e3-86c7-48bb-b093-8249dc27ee3f/config.py\n/home/tsl30/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/csd3/output/cad586e3-86c7-48bb-b093-8249dc27ee3f/script.sh: line 27: jupyter: command not found\nTimed out waiting for Jupyter Notebook server to open port 61834!\nTIMING - Wait ended at: Fri Jan 30 15:01:11 GMT 2026\nCleaning up...\n```\nI do not understand what this means! Help!!\nBest wishes,\nTim\nSent from Outlook for Mac",
    "expected_answer": "CLASSIFICATION\nCategory: Open OnDemand / Jupyter / Service Disruption\nTechnical Action Required: Possibly (service-side investigation)\nEscalation Required: Yes (if filesystem/service incident ongoing)\nUrgency: Medium (interactive workflows blocked)\n\nQUICK ASSESSMENT\nUser reports that OnDemand Jupyter notebooks reach “Starting” state and then crash. The output log shows:\n\n- `module: command not found`\n- `jupyter: command not found`\n- Timeout waiting for notebook port\n\nThese errors normally indicate that:\n- The module environment was not initialised properly, or\n- The underlying filesystem/environment (e.g. RDS) was unavailable at session start.\n\nFrom the ticket resolution, there was an active RDS filesystem disruption at the time. Therefore this was a service-side incident, not a user misconfiguration.\n\nACTION (HELPDESK)\n\n1) Check for active incidents\n   - Verify whether RDS or related shared filesystems are degraded.\n   - Confirm whether other users are reporting similar failures.\n   - Check service status / internal comms.\n\n2) Do NOT ask the user to debug their environment immediately\n   - The presence of multiple `module: command not found` entries in an OnDemand-generated script strongly suggests an environment failure rather than a user error.\n   - Avoid requesting script changes unless no incident exists.\n\n3) Confirm disruption (if applicable)\n   - If RDS or environment disruption is confirmed:\n     - Inform user that a service issue is ongoing.\n     - Provide reassurance that notebooks should function once resolved.\n\n4) If no incident is active (alternate branch)\n   - Check:\n       echo $MODULEPATH\n       which module\n       module list\n       which jupyter\n   - Confirm correct partition/environment in OnDemand form.\n   - Escalate to platform admins if environment initialisation is broken.\n\nEXAMPLE CUSTOMER REPLY (SERVICE INCIDENT CONFIRMED)\n\nSubject: Re: Interactive Jupyter Notebooks crashing\n\nHello Tim,\n\nWe were experiencing some disruption on the RDS filesystem at the time you attempted to start the notebook, which would explain the environment failing to initialise.\n\nThis should hopefully be resolved soon. \n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not attribute failure to user configuration when a known service disruption exists.\n- Do not suggest modifying system scripts under /ondemand.\n- Avoid unnecessary troubleshooting steps during confirmed incidents.\n- If repeated reports occur without an active incident, escalate to platform administrators.",
    "metadata": {
      "keyword": "crash",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98532",
    "summary": "[ukaea] Partition Query",
    "query": "I'm helping someone with an issue which has involved the `ukaea-icl-himem` nodes and I noticed that it seems that the nodes for that partition ( `cpu-q-[429-440,442-444]` ) are also in the general `ukaea-icl` pool (see below). This has meant that although their empty I'm not able to test the issue on that partition. I just wanted to make sure this is the expected setup?\n`[ir-swan1@login-p-1 jez_scripts]$ sinfo -p ukaea-icl-himem`\n`PARTITION       AVAIL  TIMELIMIT  NODES  STATE NODELIST`\n`ukaea-icl-himem    up 7-00:00:00      1  maint cpu-q-441`\n`ukaea-icl-himem    up 7-00:00:00     15  alloc cpu-q-[429-440,442-444]`\n`[ir-swan1@login-p-1 jez_scripts]$ squeue --format \"%i,%20P,%j\" -w cpu-q-[429-440,442-444]`\n`JOBID,PARTITION           ,NAME`\n`21455318,ukaea-icl           ,050m0400`\n`21455285,ukaea-icl           ,050m0400`\n`21455303,ukaea-icl           ,050m0400`\n`21455282,ukaea-icl           ,050m0400`",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / Partition Configuration / Scheduler Policy\nTechnical Action Required: No (configuration clarification only)\nEscalation Required: No (unless policy change requested)\nUrgency: Low\n\nQUICK ASSESSMENT\nUser observes that nodes in the `ukaea-icl-himem` partition (cpu-q-[429-440,442-444]) are also appearing in the general `ukaea-icl` partition. As a result, although `ukaea-icl-himem` appears empty, the nodes are running jobs from the general partition, preventing immediate testing.\n\nThis configuration is intentional.\n\nThe higher-memory nodes are shared between:\n- `ukaea-icl-himem` (higher memory per CPU requirement)\n- `ukaea-icl` (general partition)\n\nThe scheduler is configured so that:\n- Himem nodes may run jobs from the general partition when no himem jobs are waiting.\n- If himem jobs are queued, the scheduler will prioritise them appropriately.\n- Nodes may therefore be busy with general jobs even if the himem queue is empty at that moment.\n\nThis mirrors the configuration used on Cambridge icelake/icelake-himem and cclake/cclake-himem partitions.\n\nACTION\n\n1) Confirm configuration is intentional.\n2) Explain scheduler behaviour clearly:\n   - Himem nodes are not reserved exclusively.\n   - They backfill general jobs when idle.\n   - Himem jobs will be prioritised when submitted.\n3) Do not suggest misconfiguration unless confirmed by cluster admins.\n4) Escalate only if the user is requesting a policy/configuration change rather than clarification.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: ukaea-icl-himem partition behaviour\n\nHello Jez,\n\nThe overlap between `ukaea-icl-himem` and `ukaea-icl` is intentional and mirrors how the Cambridge icelake/icelake-himem partitions are configured.\n\nThe higher-memory nodes are allowed to run jobs from the general `ukaea-icl` partition when there are no himem jobs waiting. This avoids leaving those nodes idle. If there is demand for the himem partition, the scheduler will prioritise those jobs accordingly.\n\nThis means that although the himem queue may appear empty at a given moment, the nodes can still be occupied by general jobs and you may need to wait for those jobs to complete before himem resources become available.\n\nI hope this clarifies the setup.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply scheduler misconfiguration without confirmation.\n- Do not promise dedicated reservation of himem nodes unless policy changes are approved.\n- Avoid recommending workarounds that bypass scheduling policy.",
    "metadata": {
      "keyword": "UKAEA",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98497",
    "summary": "File Permission Query",
    "query": "Hi, \nI wonder if I can get some advice. \nI am one of the data managers for the project rds-HvBBlpwx4dc, and need some advice on the best way to manage the file permissions. \nThe space is used for a number of projects that our group actively work on, however when trying the manage the space and run commands such as du -sh /rds/project/rds-HvBBlpwx4dc/ I get a lot of permission denied for folders. \nIn addition to this if new members of the group starts I cannot easily give them access to group data if the file was generated by a different member of the group and have to result in asking that user to run commands on my behalf to adjust the file permissions. \nWhats the best way to mitigate this issue ?\nI look forward to hearing from you \nBest wishes\nJames",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RDS / Permissions & ACLs\nTechnical Action Required: No (user guidance only)\nEscalation Required: No (unless ACLs/groups are misconfigured)\nUrgency: Low–Medium (group collaboration friction)\n\nQUICK ASSESSMENT\nUser is a data manager for RDS project rds-HvBBlpwx4dc and reports:\n\n1) Running:\n   du -sh /rds/project/rds-HvBBlpwx4dc/\n   results in multiple “Permission denied” errors.\n\n2) Difficulty granting access to new group members because files created by one user are not automatically accessible to others.\n\nThis indicates standard Unix permission behaviour combined with missing or inconsistent default ACL configuration.\n\nRDS projects are expected to use:\n- Project-level Unix groups (e.g. rds-*-managers / rds-*-users)\n- POSIX ACLs (setfacl) to enforce consistent group access\n- Default ACLs (d:) to ensure newly created files inherit correct permissions\n\nACTION STEPS (HELPDESK)\n\n1) Confirm project group structure\n   - Verify the user is in the appropriate project groups:\n       id\n   - Typical structure:\n       rds-HvBBlpwx4dc-managers\n       rds-HvBBlpwx4dc-users\n\n2) Explain why \"Permission denied\" appears in du\n   - If subdirectories are owned by individuals with restrictive permissions (e.g. 700),\n     other project members cannot traverse them.\n   - du attempts to enter all directories and reports errors when traversal fails.\n\n3) Recommend standard RDS permission model\n   Refer user to:\n   https://docs.hpc.cam.ac.uk/storage/rds/permissions.html\n\n   Explain that project directories should:\n   - Be group-owned by the appropriate rds-* group\n   - Have group execute bit set on directories (required for traversal)\n\n4) Provide ACL-based solution (recommended approach)\n\n   To grant group read/execute on an existing directory tree:\n       setfacl -R -m g:rds-HvBBlpwx4dc-users:rx /rds/project/rds-HvBBlpwx4dc\n\n   To ensure new files and directories inherit correct permissions:\n       setfacl -R -d -m g:rds-HvBBlpwx4dc-users:rx /rds/project/rds-HvBBlpwx4dc\n\n   Clarify:\n       The extra `d:` sets the *default ACL*.\n       This ensures that newly created items inherit the correct group permissions.\n\n5) Optional: set default directory mask (if appropriate)\n   Users may also ensure directories are created with group execute bit enabled\n   (e.g. via umask 002 within collaborative groups).\n\n6) Do NOT:\n   - Suggest 777 permissions.\n   - Suggest changing ownership unless necessary.\n   - Escalate unless group membership is incorrect at system level.\n\nIF PROBLEM PERSISTS\n\n- Check actual ACLs:\n    getfacl /rds/project/rds-HvBBlpwx4dc\n- Confirm directory group ownership:\n    ls -ld /rds/project/rds-HvBBlpwx4dc\n- Verify new member is in correct Unix group.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: File permission management on RDS project\n\nHello James,\n\nOn RDS projects we recommend following the permissions model described here:\nhttps://docs.hpc.cam.ac.uk/storage/rds/permissions.html\n\nIn general, project members should be in either the rds-HvBBlpwx4dc-managers or rds-HvBBlpwx4dc-users group so that access is controlled at the group level rather than per individual user.\n\nTo ensure that new files and directories automatically inherit the correct group permissions, you can set a default ACL on the project directory, for example:\n\n  setfacl -R -d -m g:rds-HvBBlpwx4dc-users:rx /rds/project/rds-HvBBlpwx4dc\n\nThe extra `d:` sets the default ACL, which applies to newly created items and ensures they inherit the correct group permissions.\n\nYou may also need to update existing files/directories using:\n\n  setfacl -R -m g:rds-HvBBlpwx4dc-users:rx /rds/project/rds-HvBBlpwx4dc\n\nThis approach should prevent the need to manually adjust permissions each time a new group member joins.\n\nPlease let us know if you would like us to check the current ACLs on the project directory.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Avoid recommending overly permissive modes (e.g. 777).\n- Default ACLs are the preferred collaborative model on RDS.\n- Ensure group-based access rather than per-user manual fixes.\n- Only escalate if Unix group membership is incorrect at system level.",
    "metadata": {
      "keyword": "permissions",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98324",
    "summary": "Public Key Authentication",
    "query": "Dear Support Team,\nPlease could my public key be authorised for use: AAAAC3NzaC1lZDI1NTE5AAAAIBxSn0jwGloBiJ+etrtwLgbA37Z2ZIEXlPuaf2KTjVuu\nYours,\nAyoife Dada.",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / SSH / Public Key\nTechnical Action Required: No (user self-service)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser is an internal Cambridge account holder (CRSid email visible in ticket metadata).\nInternal users authenticate via UIS/Raven and can install their own SSH public keys.\n\nHPC Support does not manually install public keys.\n\nACTION\n\nUser should install their own public key using:\n\n    ssh-copy-id <CRSid>@login.hpc.cam.ac.uk\n\nor\n\n    ssh-copy-id <CRSid>@multi.hpc.cam.ac.uk\n\nThey will be prompted for their UIS/Raven password. This appends the key to:\n\n    ~/.ssh/authorized_keys\n\n\nEXAMPLE CUSTOMER REPLY\n\nDear Ayoife,\n\nSince you are an internal user, you can authenticate with your UIS password and install your SSH public key yourself.\n\nFrom your local machine, please run:\n\n  ssh-copy-id <CRSid>@login.hpc.cam.ac.uk\n\nYou will be prompted for your password, and this will append your key to your CSD3 account.\n\nPlease let us know if you encounter any issues.\n\nBest wishes,\nHPC Support\n\n\nSAFETY / POLICY NOTES\n- Never request or handle private keys.\n- Do not accept SSH keys via email for manual installation.\n- Do not manually install public keys on behalf of users; internal users must use the supported self-service method (e.g. ssh-copy-id).",
    "metadata": {
      "keyword": "public key",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98311",
    "summary": "Compiling LAMMPS on Ampere",
    "query": "Dear CSD3 support,\nI am trying to compile the newest release version of LAMMPS on Ampere with the GPU package. Following the instructions on [https://docs.hpc.cam.ac.uk/hpc/software-packages/lammps.html](https://docs.hpc.cam.ac.uk/hpc/software-packages/lammps.html) leads to an error when running a benchmark:\nCuda driver error 1 in call at file '/home/dc-bole2/git/lammps_amp/lib/gpu/geryon/nvd_kernel.h' in line 340.\nCuda driver error 1 in call at file '/home/dc-bole2/git/lammps_amp/lib/gpu/geryon/nvd_kernel.h' in line 340.\nCuda driver error 1 in call at file '/home/dc-bole2/git/lammps_amp/lib/gpu/geryon/nvd_kernel.h' in line 340.\nCuda driver error 1 in call at file '/home/dc-bole2/git/lammps_amp/lib/gpu/geryon/nvd_kernel.h' in line 340.\n--------------------------------------------------------------------------\nMPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD\nwith errorcode -1.\nNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nYou may or may not see output from other processes, depending on\nexactly when Open MPI kills them.\n--------------------------------------------------------------------------\nThis is working without any issues when using version stable_29Aug2024_update2 as per the instructions. By any chance, do you happen to have any information on how to compile a more recent version of LAMMPS?\nMany thanks,\nMax",
    "expected_answer": "CLASSIFICATION\nCategory: Software / Compilation / GPU (Ampere)\nTechnical Action Required: No (user to retry with updated stack)\nEscalation Required: Possibly (if issue persists after stack update)\nUrgency: Low–Medium\n\nQUICK ASSESSMENT\nUser reports CUDA driver errors when compiling a newer version of LAMMPS with the GPU package on Ampere. The older stable version works.\n\nLikely cause: mismatch between CUDA/toolchain versions and the currently loaded software stack.\n\nACTION\n\nAsk the user to retry compilation using the updated module stack:\n\n    module load rhel8/ampere/base\n    module load gcc/14.3.0/vlhhcp6m\n    module load cmake/3.31.10/gcc/7ddsybx7\n    module load cuda/12.8.1/gcc/kdeps6ab\n    module load openmpi/4.1.8/gcc/hemliivg\n\nThen re-run the build and benchmark test.\n\nIf the issue persists after loading the updated stack, request:\n- Full build output\n- Exact LAMMPS version\n- Full error message from runtime test\n\nEXAMPLE CUSTOMER REPLY\n\nHi Max,\n\nCould you please try compiling again using the newest Ampere software stack:\n\n  module load rhel8/ampere/base\n  module load gcc/14.3.0/vlhhcp6m\n  module load cmake/3.31.10/gcc/7ddsybx7\n  module load cuda/12.8.1/gcc/kdeps6ab\n  module load openmpi/4.1.8/gcc/hemliivg\n\nAfter loading these modules, please rebuild and test again.\n\nLet us know how you get on.\n\nBest wishes,\nHPC Support\n\n\nSAFETY / POLICY NOTES\n- Do not recommend installing custom CUDA drivers.\n- Do not suggest modifying system-level GPU drivers.\n- Keep troubleshooting limited to supported module stacks unless escalation is required.",
    "metadata": {
      "keyword": "compiler",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98296",
    "summary": "Repeated password requests",
    "query": "Hi there,\nI am attempting to download CT scan folders from the server rcs.rja50.umzc.ctdata. These folders contain approximately 3000 images each, and I am being requested to input a password before the transfer of every single file. This has happened on multiple machines and has not been an issue previously. It is delaying accessing files significantly and meaning I have to constantly attend downloads. Is there a way to fix this? Thanks in advance.\nBest wishes,\nKet\nKeturah Smithson (she/her), Managing Operator\nCambridge Biotomography Centre, Department of Zoology\nUniversity of Cambridge\nCB2 3EJ\nTel: (01223 7) 65405\n[https://www.cbc.zoo.cam.ac.uk/](https://www.cbc.zoo.cam.ac.uk/)",
    "expected_answer": "CLASSIFICATION\nCategory: Data Transfer / FileZilla / SFTP Client Behaviour\nTechnical Action Required: No (user workflow guidance)\nEscalation Required: No\nUrgency: Low–Medium\n\nQUICK ASSESSMENT\nUser is downloading folders containing ~3000 images each and is prompted to enter a password for every individual file transfer. This is a common drawback of some GUI SFTP clients (e.g., FileZilla), which can prompt for credentials per transfer when handling many small files.\n\nThis is not evidence of an HPC authentication issue.\n\nACTION\nAdvise the user to compress many small files into a single archive (tar or zip) before downloading, then transfer the single archive instead of thousands of individual files.\n\nExample (run on the remote server in the directory containing the CT scans):\n\n  tar -czf ct_scans.tar.gz <ct_scan_folder>/\n\nThen download `ct_scans.tar.gz` as a single file.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Ket,\n\nUnfortunately this is one of the drawbacks if you are using e.g., FileZilla. You will be prompted to enter credentials for every transfer.\n\nWe normally suggest compressing the files into a tar or zip to make the transfer smoother and easier, then downloading the single archive file. For example:\n\n  tar -czf ct_scans.tar.gz <ct_scan_folder>/\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply authentication malfunction unless there is additional evidence (e.g. failed logins elsewhere).\n- Avoid recommending insecure transfer methods.\n- Keep guidance aligned with supported/helpdesk-standard workflows.",
    "metadata": {
      "keyword": "password",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98292",
    "summary": "Fw: DAWN Access Assistance",
    "query": "Dear Support,\nCan someone please address this one from yesterday and today? Thanks.\nRegards,\nMuhammad Ahmed\n________________________________\nFrom: Jun Yao Chan <jun@revax.co.uk>\nSent: 27 January 2026 12:34\nTo: HPC JIRA service desk inbox <jiraservice@hpc.cam.ac.uk>\nCc: Alex Bartlam <Alex@revax.co.uk>; Jack Mander <jack@revax.co.uk>\nSubject: DAWN Access Assistance\nHi,\nI am looking to gain access to the DAWN Supercomputer, some of my colleagues already have access but I am not set up with an account yet. Are you able to set up login details for my account?\nHere is the all the details:\nName: Jun Yao Chan\nEmail: jun@revax.co.uk\nSSH keys: AAAAC3NzaC1lZDI1NTE5AAAAIL+klunbwQNR+EoEuzyqe0y0HGjf25GM3hIfka8DL1Ra\nThanks.\nRegards,\nJun",
    "expected_answer": "CLASSIFICATION\nCategory: Account Provisioning / DAWN / Project Access\nTechnical Action Required: Yes (account provisioning once invitation accepted)\nEscalation Required: Possibly (confirm invitation + provisioning state)\nUrgency: Medium (user awaiting access)\n\nQUICK ASSESSMENT\nUser is requesting access to DAWN and has provided name, email, and SSH key.\n\nCorrect process for DAWN access:\n1) User must be invited to the DAWN project by the PI via Waldur.\n2) User must accept the invitation in the DAWN access portal.\n3) Once accepted, provisioning is triggered and a local DAWN account is created.\n4) User receives an email to set up their password.\n5) User logs into the DAWN portal and accepts the project invitation.\n\nHelpdesk should NOT:\n- Manually create accounts before invitation is accepted.\n- Install SSH keys manually from email.\n- Bypass the Waldur provisioning workflow.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm invitation status\n   - Check whether the PI has invited the user in Waldur.\n   - If not invited → instruct user to contact PI.\n\n2) Confirm invitation acceptance\n   - If invited but not accepted → instruct user to log into portal and accept invitation.\n   - Provide documentation link.\n\n3) Confirm provisioning state\n   - If invitation accepted and provisioning ticket exists → confirm local account creation.\n   - Ensure password setup email has been sent.\n\n4) Post-provisioning guidance\n   - Inform user that:\n       • A local DAWN account has been created.\n       • They should set their password via the email received.\n       • They must log into the DAWN access portal and accept the project invitation.\n\n5) Do NOT:\n   - Accept SSH keys via email.\n   - Manually append SSH keys.\n   - Share temporary credentials.\n   - Create accounts outside the provisioning system.\n\nIF USER PROVIDES SSH KEY\n- Acknowledge receipt but do not install manually.\n- Once account exists, user can install key themselves after first login.\n\nEXAMPLE CUSTOMER REPLY (PRE-INVITATION)\n\nHello Jun,\n\nYou need to be invited to the DAWN project by the PI via Waldur. Once this has been done, you will receive an invitation which you must accept before we can create your DAWN login account.\n\nPlease see this guidance:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/pvc.html#users-accessing-open-on-demand-via-the-portal\n\nBest wishes,\nHPC Support\n\nEXAMPLE CUSTOMER REPLY (POST-PROVISIONING)\n\nHello Jun,\n\nWe have created your local DAWN account, and you should have received an email to set up your password.\n\nUsing the email and password, please log in to the DAWN access portal and accept the project invitation.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Never manually install SSH keys received via email.\n- Never request or handle private keys.\n- Do not create accounts outside the approved provisioning workflow.\n- Ensure invitation acceptance precedes account creation.\n- Do not bypass Waldur/project-based access controls.",
    "metadata": {
      "keyword": "Dawn",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98278",
    "summary": "Cannot login to the HPC web portal",
    "query": "Dear HPC support team,\nI have received the email that the HPC login issue has been resolved. But since yesterday evening till now, I still cannot login via the web portal [https://login-web.hpc.cam.ac.uk/](https://login-web.hpc.cam.ac.uk/). I wonder if this issue has been resolved or not.\nBest,\nYiran",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / Web Portal / Browser Behaviour\nTechnical Action Required: No (user-side troubleshooting)\nEscalation Required: No (unless issue persists across users)\nUrgency: Medium (access blocked)\n\nQUICK ASSESSMENT\nUser reports that they still cannot log in via:\n\n  https://login-web.hpc.cam.ac.uk/\n\ndespite being told the login issue was resolved.\n\nThis is most consistent with:\n- Browser caching/session persistence issues, or\n- A stale redirect/session cookie.\n\nNo evidence of account-level authentication failure.\n\nACTION STEPS (HELPDESK)\n\n1) Rule out browser cache/session issue\n   Ask user to:\n   - Clear browser cache and cookies for login-web.hpc.cam.ac.uk\n   - Try an incognito/private window\n   - Try a different browser\n\n2) Provide forced redirect link\n   Ask them to test:\n\n   https://login-web.hpc.cam.ac.uk/nginx/stop?redir=/pun/sys/dashboard/\n\n   This forces the nginx redirect to reset the login path.\n\n3) Confirm behaviour\n   - If redirect link works → likely cached session issue.\n   - If both links fail → escalate to web portal admins.\n\n4) Do NOT:\n   - Reset password unless authentication failure is confirmed.\n   - Modify account without evidence.\n   - Assume global outage unless multiple reports exist.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Yiran,\n\nCould you please try clearing your browser cache or opening the portal in a private/incognito window?\n\nIf that does not resolve it, please try logging in via the following link:\n\n  https://login-web.hpc.cam.ac.uk/nginx/stop?redir=/pun/sys/dashboard/\n\nThis forces a redirect and can resolve cached session issues.\n\nPlease let us know whether this works.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply account compromise.\n- Do not suggest credential changes unless authentication failure is confirmed.\n- Escalate only if issue persists across multiple users or browsers.",
    "metadata": {
      "keyword": "login-web",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98277",
    "summary": "Scheduling jobs during maintenance",
    "query": "Good morning,\nI have a question regarding the cluster’s behavior during reduced power consumption periods. I’ve noticed that jobs submitted during these windows are accepted and scheduled, but they execute at a significantly lower speed. This has led to several jobs timing out before completion, resulting in a loss of compute hours and requiring expensive reruns (even when requesting the same amount of resources as previously successful jobs).\nIs there a way to prevent jobs from starting if the cluster is in a low-power state? Specifically, I’m looking for a requirement or constraint I can add to my job scripts (e.g., a specific #SLURM flag) to ensure they only run when full performance resources are available.\nThank you for your help!\nBest regards,\nClara",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / Cluster Performance / Power Management\nTechnical Action Required: No (policy clarification only)\nEscalation Required: No\nUrgency: Low–Medium\n\nQUICK ASSESSMENT\nUser observes that jobs submitted during reduced power consumption periods appear to run significantly slower and may time out. They ask whether there is a Slurm constraint (e.g. a #SBATCH flag) to prevent jobs from starting when the cluster is in a low-power state.\n\nOn CSD3, Slurm does not start jobs on “degraded performance” nodes. Jobs are scheduled only when requested resources are available. Reduced power states do not cause Slurm to allocate partially performing resources. \n\nApparent slowdowns are more likely caused by:\n- I/O contention (e.g. high demand on RDS shared filesystem),\n- General cluster load,\n- Application-level variability.\n\nThere is no Slurm flag to prevent job start based on power mode, as power state does not change the allocated CPU/GPU specification.\n\nACTION\n\n1) Clarify scheduler behaviour\n   - Slurm will only start jobs once requested resources are available.\n   - Reduced power consumption periods do not change the allocated hardware specification for a job.\n\n2) Explain likely causes of slowdown\n   - Shared filesystem (RDS) performance varies depending on demand.\n   - High I/O workloads across users can impact runtime.\n   - Jobs may therefore run longer than expected.\n\n3) Provide practical mitigation advice\n   - Ensure jobs request realistic walltime.\n   - Consider profiling I/O behaviour.\n   - If appropriate, prevent automatic requeue by adding:\n       #SBATCH --no-requeue\n     (only if user specifically wants jobs not to restart after interruption).\n\n4) Do NOT:\n   - Suggest that jobs are intentionally throttled.\n   - Promise guaranteed full-speed operation during all cluster states.\n   - Suggest non-existent Slurm flags related to power state.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Job behaviour during reduced power periods\n\nHi Clara,\n\nThe cluster power status does not affect how Slurm allocates resources. Slurm will only start your job once the requested CPUs/GPUs become available, and jobs are not started on partially performing hardware.\n\nIf you are seeing slowdowns, this is more likely due to general system load or performance variation on the shared RDS filesystem, whose performance can fluctuate depending on demand.\n\nThere isn’t a Slurm flag that prevents jobs from starting based on power state. If needed, you can ensure your jobs are not requeued automatically by adding:\n\n  #SBATCH --no-requeue\n\nOtherwise, we would recommend reviewing walltime requests and considering whether I/O demand could be contributing to the longer runtimes.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply cluster misconfiguration without evidence.\n- Do not suggest unsupported scheduler constraints.\n- Avoid attributing performance issues to “power throttling” unless formally confirmed by cluster administrators.",
    "metadata": {
      "keyword": "maintenance",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98114",
    "summary": "Unusually long queue time",
    "query": "Dear Support Desk,\n*\nYour name: Jiakai Chen\n*\nYour account/PI: Prof. Sebastian Schemm/ Prof. Simon Driscoll (account: schemm-sl+)\n*\nyour job submission script: Job ID 21047257\n*\nthe working directories or paths you are using (e.g. /rds/user/$USER/hpc-work/foo/bar/): /home/jc2405/Project/neuralgcm/neuralgcm\n*\nas much detail as possible on what you are working on, what you have tried, and error messages you receive:\nThe above script requests for 20 minutes of GPU (1 node) but it has been in the queue for around 15 hours, while it usually takes just around an hour. Can I check if that's expected? Thank you.\nBest,\nKai",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / Scheduling / Queue Monitoring\nTechnical Action Required: No (user guidance only)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser asks whether there is a way to check the expected wait/start time for a queued Slurm job.\n\nSlurm provides an estimated start time calculation based on current queue state and scheduling policy. This estimate can change if cluster load or priorities change.\n\nACTION\n\nAdvise the user to run:\n\n  squeue -u <CRSid> --start\n\nThis will display the estimated start time for their queued jobs.\n\nClarify that:\n- The reported time is an estimate only.\n- It may change depending on demand, priority, and resource availability.\n- There is no guaranteed start time while a job is queued.\n\nDo NOT:\n- Promise a fixed start time.\n- Adjust job priority.\n- Escalate unless there is evidence of scheduler malfunction.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Kai,\n\nTo check the estimated start time for your queued jobs, you can run:\n\n  squeue -u <your CRSid> --start\n\nThis will show Slurm’s current estimate of when each job is expected to begin. Please note that this is only an estimate and may change depending on system demand and scheduling priorities.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not guarantee start times.\n- Avoid implying scheduler fault unless independently verified.\n- Ensure advice aligns with documented Slurm behaviour.",
    "metadata": {
      "keyword": "wait times",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-98039",
    "summary": "Slurm jobs not executing",
    "query": "Dear HPC team,\nI am Tigran, an MPhil student in ACS. I am using the CSD3 server for my MPhil dissertation, the slurm jobs that I submit do not leave the queue and start executing.\nWhen I run mybalance command, it shows that the available hours is 1, is this the cause?\nBest,\nTigran",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / Account Balance / Project Credits\nTechnical Action Required: No (user guidance only)\nEscalation Required: No\nUrgency: Medium (jobs not starting)\n\nQUICK ASSESSMENT\nUser reports that submitted Slurm jobs remain in the queue and notes that `mybalance` shows only 1 computing hour remaining.\n\nIf a project account has insufficient remaining compute hours, jobs charged to that account will not start.\n\nACTION\n\n1) Confirm cause\n   - If `mybalance` shows only 1 remaining compute hour on the specified project account, this is likely preventing the job from running.\n\n2) Advise corrective step\n   - Cancel the queued job.\n   - Resubmit using a project account with sufficient available hours (e.g. an SL3-GPU account if appropriate and available to the user).\n\n3) Do NOT:\n   - Manually add hours.\n   - Override scheduler/accounting restrictions.\n   - Escalate unless the balance appears incorrect.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Tigran,\n\nYes, if `mybalance` shows that there is only 1 computing hour remaining on the LIO-SL2-GPU account, this would prevent your job from starting.\n\nPlease cancel the current job and resubmit it using a project account with sufficient available hours.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply accounting errors without verification.\n- Do not bypass project credit limits.\n- Ensure the user selects an account they are authorised to use.",
    "metadata": {
      "keyword": "Slurm",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97991",
    "summary": "Quote for 200 SL2-GPU hours",
    "query": "Hello,\nI’ve been directed from the UIS support team. I am writing to request a quote for 200 SL2 GPU hours and for the creation of an SL2 GPU account under Dr. Tian Zhao’s name (crsid: txz20). Our current GPU account is ZHAO-SL3-GPU.\nCould you please let me know what information is required from our end? I would also appreciate it if you could outline the process for purchasing these resources and for account creation, including any approvals that may be needed. This will allow us to proceed with raising a purchase order.\nThank you very much for your help.\nKind Regards,\nFiona Charlier",
    "expected_answer": "CLASSIFICATION\nCategory: Procurement / GPU Resource Purchase / Quotation\nTechnical Action Required: No (user to follow published process)\nFinance Action Required: Yes (user departmental PO required)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser requests:\n- A quote for 200 SL2-GPU hours\n- Creation of an SL2-GPU account under a named PI\n\nGPU hours and account creation follow the published charges and procurement process. Pricing and ordering instructions are provided on the official charges page.\n\nACTION\n\n1) Direct user to official pricing page:\n   https://www.hpc.cam.ac.uk/charges\n\n2) Advise user to:\n   - Review the published SL2-GPU pricing.\n   - Follow the process for raising a purchase order via their departmental finance team.\n   - Submit required account creation details as described on the charges page.\n\n3) Do NOT:\n   - Generate ad hoc pricing outside published rates.\n   - Create accounts before procurement process is followed.\n   - Escalate unless pricing discrepancy or policy exception is requested.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Fiona,\n\nPlease follow the instructions and pricing information available on our charges page:\n\nhttps://www.hpc.cam.ac.uk/charges\n\nThis page outlines the process for purchasing SL2-GPU hours and for requesting account creation. Once your departmental finance team has raised the appropriate purchase order, we can proceed accordingly.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not provide unofficial pricing.\n- Do not create accounts without the required procurement steps.\n- Ensure users follow the documented purchase workflow.",
    "metadata": {
      "keyword": "quote",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97962",
    "summary": "SSH login requests TOTP but MFA not set up yet",
    "query": "Dear CSD3 support team,\nI am able to log in to login.hpc.cam.ac.uk with my username and password, after which I am prompted to enter a TOTP code.\nHowever, I have not yet been able to set up CSD3 MFA:\n- I never received a QR code for CSD3 TOTP\n- login-web.hpc.cam.ac.uk reports invalid username or password\n- connecting to multi.hpc.cam.ac.uk closes the connection immediately after password entry\nCould you please help me with this? Thank you in advance!\nMy username: jh2642\nBest regards,\nJin Huang\n—\nJin Huang\nMarie Skłodowska-Curie Postdoctoral Fellow\n@ AFAR Lab<[https://cambridge-afar.github.io/](https://cambridge-afar.github.io/)>, University of Cambridge\n[https://www.cst.cam.ac.uk/people/jh2642](https://www.cst.cam.ac.uk/people/jh2642)",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / Account Provisioning / MFA\nTechnical Action Required: No (account application required)\nEscalation Required: No\nUrgency: Medium (user cannot complete login)\n\nQUICK ASSESSMENT\nUser reports:\n- Able to reach login.hpc.cam.ac.uk\n- Prompted for TOTP\n- Has not received QR code\n- login-web reports invalid username/password\n- multi.hpc.cam.ac.uk disconnects after password\n\nThis strongly indicates that the user does not yet have a fully provisioned CSD3 account with MFA enrolment enabled.\n\nMFA (TOTP) enrolment occurs only after an account has been formally created via the official HPC application process.\n\nACTION\n\n1) Confirm whether the user has applied for an HPC account\n   Direct them to:\n   https://www.hpc.cam.ac.uk/rcs-application\n\n2) Explain that:\n   - Account creation must be triggered via the official application form.\n   - MFA enrolment (QR code) is generated only after the account is provisioned.\n   - The behaviour they are seeing is consistent with an account not yet fully created.\n\n3) Do NOT:\n   - Reset MFA (no MFA exists yet).\n   - Attempt manual account creation via ticket.\n   - Troubleshoot login-web further until account status is confirmed.\n\n4) If the user confirms they have already applied:\n   - Check provisioning status internally.\n   - Escalate only if application is stuck in provisioning.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: SSH login requests TOTP but MFA not set up yet\n\nHello Jin,\n\nHave you previously applied for an account via the official HPC application form?\n\nhttps://www.hpc.cam.ac.uk/rcs-application\n\nIf not, please complete this form with your details and your PI’s details and submit it. This will trigger the account provisioning process. Once your account is created, you will then be able to enrol in MFA (you will be presented with a QR code during login).\n\nIf you have already submitted the form, please let us know and we can check the status for you.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume MFA malfunction when account may not yet exist.\n- Do not attempt to bypass the official account creation workflow.\n- MFA enrolment is tied to successful account provisioning.",
    "metadata": {
      "keyword": "TOTP",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97859",
    "summary": "Running TensorFlow on ampere: \"module load python/3.8.11/gcc-9.4.0-yb6rzr6\" not found.",
    "query": "Dear Support,\nI'm William Lee, working with Mike Roberts e.g. MROBERTS-SL3-GPU and AUTOQC-DAMTP-SL2-GPU.\nI'd like to run TensorFlow on ampere and I've been trying to follow the instructions here:\n[https://docs.hpc.cam.ac.uk/hpc/software-packages/tensorflow.html](https://docs.hpc.cam.ac.uk/hpc/software-packages/tensorflow.html)\nBut the line\nmodule load python/3.8.11/gcc-9.4.0-yb6rzr6\ndoesn't work.\nIs there an updated version of the instructions or a drop-in replacement for this line please?\nBest wishes,\nWilliam.",
    "expected_answer": "CLASSIFICATION\nCategory: Software / TensorFlow / Ampere GPU / Modules\nTechnical Action Required: No (user environment clarification)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nThe user is attempting to follow the official TensorFlow GPU instructions:\nhttps://docs.hpc.cam.ac.uk/hpc/software-packages/tensorflow.html\n\nThey report that:\n\n  module load python/3.8.11/gcc-9.4.0-yb6rzr6\n\n“doesn’t work”.\n\nThe documentation indicates that these steps must be run within an interactive session on an Ampere GPU node. The Python module is available within the Ampere software stack. If the module cannot be loaded, this is most likely due to the environment context (e.g. not being on an Ampere node or the module environment having been altered).\n\nThere is no evidence that the documentation is outdated or that the module has been removed.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm execution context\n\nAsk the user to confirm they are running the instructions inside an interactive session on an Ampere GPU node.\n\nExample:\n\n  srun --partition=ampere --gres=gpu:1 --time=01:00:00 --pty bash\n\nThen check module availability:\n\n  module avail python/3.8.11\n\nExpected output should include:\n\n  python/3.8.11/gcc-9.4.0-yb6rzr6\n\n2) Attempt module load\n\nIf the module appears in module avail, attempt:\n\n  module load python/3.8.11/gcc-9.4.0-yb6rzr6\n\n3) If the module still does not load\n\nAsk the user to provide the exact error message produced when running:\n\n  module load python/3.8.11/gcc-9.4.0-yb6rzr6\n\nand confirm:\n  - The hostname (to verify they are on an Ampere GPU node)\n  - Whether they have modified the module environment (e.g. using module purge)\n\nOnly escalate if:\n  - The module does not appear in module avail while on an Ampere node, or\n  - The load error indicates a system-side module configuration issue.\n\nEXAMPLE CUSTOMER REPLY\n\nHello William,\n\nThe TensorFlow instructions must be run within an interactive session on an Ampere GPU node.\n\nCould you please start an Ampere session, for example:\n\n  srun --partition=ampere --gres=gpu:1 --time=01:00:00 --pty bash\n\nand then run:\n\n  module avail python/3.8.11\n\nYou should see:\n\n  python/3.8.11/gcc-9.4.0-yb6rzr6\n\nIf it appears but does not load, please send us the exact error message and the output of:\n\n  hostname\n\nso we can investigate further.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume the module has been removed without checking module avail.\n- Do not recommend installing a custom Python or modifying system module paths.\n- Keep troubleshooting limited to confirming correct execution context and module visibility.",
    "metadata": {
      "keyword": "module",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97741",
    "summary": "CSD3 login - vm462",
    "query": "Hello,\nFor some reason I cannot log into the CSD3 headnodes. I was logged in yesterday, and after some idle period I had to log back in, and since then I cannot log in.\nI enter my password and MFA details, then it requests for password again (seemingly looking like my password is wrong, though I'm sure I've entered it correctly as I had been logging in and out lately).\nDo you have any ideas? (user vm462)\nCheers,\nVijay\nDr Vijay Mahatma\nResearch Associate\nBattcock Centre for Experimental Astrophysics (Room G04)\nCavendish Laboratory, University of Cambridge",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / SSH Login / Account Lockout\nTechnical Action Required: No (diagnosis + user guidance)\nEscalation Required: No (unless lock persists beyond normal timeout)\nUrgency: Medium (user access blocked)\n\nQUICK ASSESSMENT\nUser reports:\n- Previously able to log into CSD3 headnodes.\n- After an idle period, login fails.\n- After entering password and MFA, system prompts for password again.\n- Appears as though password is incorrect.\n\nThis pattern is consistent with an account lock due to multiple unsuccessful login attempts. On CSD3, repeated failed authentication attempts (including incorrect password or expired credentials) will trigger a temporary lock. The lock clears automatically after a defined timeout period.\n\nA common contributing factor is recent UIS/Raven password changes, which automatically propagate to CSD3 authentication.\n\nACTION STEPS (HELPDESK)\n\n1) Check account status (internal)\n   - Confirm whether the account is in a locked state due to failed attempts.\n   - Confirm there is no wider authentication incident.\n\n2) Inform user of temporary lock behaviour\n   - Explain that repeated failed login attempts can trigger a temporary lock.\n   - State that the lock clears automatically after approximately 30 minutes.\n   - No manual reset is required unless the lock persists.\n\n3) Clarify correct credential usage\n   - User must enter their current UIS/Raven password (not an old HPC-specific password).\n   - After password, they must provide the correct TOTP associated with CSD3 (e.g. SSH token).\n\n4) If issue persists after timeout\n   Request:\n     - Exact hostname used (e.g. login.hpc.cam.ac.uk, login-icelake, multi.hpc.cam.ac.uk)\n     - Exact error message shown after failure\n     - Confirmation that UIS/Raven password works for other services\n\n5) Escalate only if:\n   - Lock does not clear after timeout window\n   - Account shows unexpected authentication failure state\n   - MFA system appears inconsistent\n\nDO NOT:\n- Reset the password.\n- Reset MFA without evidence.\n- Suggest creating a new account.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: CSD3 login issue\n\nDear Vijay,\n\nIt appears that your account has been temporarily locked due to multiple unsuccessful login attempts. This lock will clear automatically after approximately 30 minutes.\n\nOnce it has cleared, please ensure that you are entering your current UIS/Raven password when prompted in the terminal, followed by your CSD3 MFA code.\n\nIf you continue to experience issues after waiting for the lock to clear, please let us know the exact hostname you are connecting to and the full error message shown.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not disclose internal authentication logs.\n- Do not confirm password correctness.\n- Avoid manual intervention unless lock persists abnormally.\n- UIS/Raven password changes automatically propagate to CSD3 authentication.",
    "metadata": {
      "keyword": "CSD3",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97739",
    "summary": "HPC password",
    "query": "Hello,\nPlease advise, I do not know the password. I do not think I ever set up my password when I first activated my account.\nfaylindfield@user0252 ~ % ssh fl483@login-icelake.hpc.cam.ac.uk\n<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n<>                                                        <>\n<>                        \n[FILE ATTACHMENT]\n                       <>\n<>                                                        <>\n<>                    RCS CSD3 Facility                   <>\n<>             Unauthorised Access Prohibited             <>\n<>    Use of this system constitutes acceptance of our    <>\n<>                     policies - see                     <>\n<> [http://docs.hpc.cam.ac.uk/hpc/user-guide/policies.html](http://docs.hpc.cam.ac.uk/hpc/user-guide/policies.html) <>\n<>        All data is stored in the United Kingdom.       <>\n<>                                                        <>\n<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n(fl483@login-icelake.hpc.cam.ac.uk) Password:\nThank you for your help,\nBest wishes,\nFay",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / SSH Login / Password Clarification\nTechnical Action Required: No (user guidance only)\nEscalation Required: No\nUrgency: Medium (access blocked)\n\nQUICK ASSESSMENT\nUser is attempting to SSH to login-icelake.hpc.cam.ac.uk and reports not knowing their HPC password, stating they may never have set one.\n\nOn CSD3, there is no separate HPC-specific password for internal users. Authentication uses the user’s UIS/Raven password, followed by the appropriate CSD3 TOTP token for SSH access.\n\nACTION\n\n1) Clarify authentication model\n   - HPC password = UIS/Raven password.\n   - There is no separate password to set during activation.\n\n2) Clarify SSH login sequence\n   When connecting via:\n     ssh <CRSid>@login-icelake.hpc.cam.ac.uk\n\n   The user should:\n     a) Enter their current UIS/Raven password.\n     b) Enter their TOTP code associated with CSD3 SSH (typically labelled “CSD3:ssh” or similar in their authenticator app).\n\n3) If login still fails\n   Ask:\n     - Have you recently changed your UIS/Raven password?\n     - Do you have the correct CSD3 SSH MFA token configured?\n     - What exact error message appears after entering password/TOTP?\n\n4) Do NOT:\n   - Reset password directly.\n   - Provide or request passwords.\n   - Create a separate HPC password.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: HPC password\n\nDear Fay,\n\nThere is no separate HPC password to set up. Your HPC login password is the same as your UIS/Raven password.\n\nWhen connecting via SSH, please enter your current UIS/Raven password when prompted, followed by the TOTP code associated with your CSD3 SSH login (usually labelled “CSD3:ssh” in your authenticator app).\n\nIf you continue to experience issues, please let us know the exact error message shown after entering your credentials.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Never request or store user passwords.\n- Do not imply a separate HPC password exists.\n- Escalate only if authentication failures persist after confirming correct credential usage.",
    "metadata": {
      "keyword": "password",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97675",
    "summary": "SRCP Epi platform - storage query",
    "query": "Hi,\nWe would like to archive some old projects on our platform. Is RFS the only storage option available, or is RCS available from within the SRCP?\nThanks\nTom",
    "expected_answer": "CLASSIFICATION\nCategory: SRCP / Storage Architecture / RFS\nTechnical Action Required: No (clarification only)\nEscalation Required: No\nUrgency: Low\n\nFOCUS OF RESPONSE\nUser’s second query:\n\n“My understanding is that somehow storage for SRCP is provisioned on RFS. Is this RFS dedicated hardware that is isolated for use by a particular platform? Or does it work differently? In which case, is there the option to have dedicated RCS hardware, which will be cheaper?”\n\nQUICK ASSESSMENT\nSRCP storage is provisioned using RFS-backed storage that is logically and operationally isolated per platform/project. It is not pooled with general-purpose RDS/RCS environments, and users cannot directly mount external RCS storage within SRCP. There is no model for attaching separate “dedicated RCS hardware” to SRCP as a cheaper alternative.\n\nACTION\n\n1) Clarify storage model\n   - SRCP storage is provisioned as isolated RFS shares associated specifically with SRCP projects.\n   - These shares are separated from other storage systems and not user-configurable.\n   - The isolation is by design due to SRCP security requirements.\n\n2) Clarify hardware/cost question\n   - SRCP storage is not dynamically attached from arbitrary RCS pools.\n   - There is no option to provision separate “dedicated RCS hardware” for SRCP at a reduced cost.\n   - SRCP environments have a defined architecture and approved storage pathways only.\n\n3) If cost optimisation is the underlying concern\n   - Suggest reviewing retention strategy (e.g., archive/export workflow outside SRCP if appropriate).\n   - Explain that external storage (e.g., RCS) would require controlled export and re-import, not live mounting.\n\n4) Do NOT:\n   - Suggest bypassing SRCP isolation.\n   - Imply that SRCP storage can be merged with other RCS/RDS resources.\n   - Promise alternative hardware provisioning unless formally approved.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: SRCP storage architecture\n\nDear Tom,\n\nSRCP storage is provisioned using RFS shares that are logically and operationally isolated for SRCP projects. These are separate from other RCS/RDS storage systems and are not directly accessible or mountable from outside the platform.\n\nIt is not possible to attach separate “dedicated RCS hardware” to SRCP as a lower-cost alternative. The SRCP architecture and its storage pathways are fixed by design due to the security requirements of the platform.\n\nIf you are looking to reduce storage costs, one option would be to export data from SRCP and archive it to a nominated RCS project outside the platform, but this would not be a live integration.\n\nPlease let us know if you would like to discuss possible workflows for archiving data.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Maintain SRCP security model integrity.\n- Do not suggest storage sharing across security domains.\n- Avoid speculative statements about hardware provisioning beyond documented architecture.",
    "metadata": {
      "keyword": "SRCP/ Secure Research Computing Platform",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97660",
    "summary": "Adding new user to the lab RFS/RDS space",
    "query": "Dear HPC support team, \nApologies for bothering you about this.\nA couple of days ago, I added Victor (CC’d; CRSid: vj292) to our lab RDS (rds-djh1002-hodson-rds) and RFS-NFS (rfs-djh1002-hodson-rfs-nfs) space. However, in the self-service portal he is still showing as “Provisioning”.\nI just wanted to check whether there is anything we can do to speed up access to the data, or whether there is something we may have missed on our end.\nMany thanks for your help.\nBest wishes, \nJoanna",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RDS & RFS / Provisioning\nTechnical Action Required: No (user action pending)\nEscalation Required: No\nUrgency: Low–Medium (access pending)\n\nQUICK ASSESSMENT\nA new user (vj292) has been added to both an RDS and RFS-NFS project via the self-service portal but remains in “Provisioning” state. The most common cause of this status is that the user has not yet accepted the storage service Terms & Conditions in the UIS Self Service portal. Until this step is completed, the provisioning workflow cannot proceed and access will not be activated.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm provisioning state (internal)\n   - Verify that the user appears as “Provisioning” on the relevant RDS and RFS projects.\n   - Check whether Terms & Conditions acceptance is outstanding.\n\n2) Instruct user to accept Terms & Conditions\n   Ask the new member (vj292) to:\n   - Visit: https://selfservice.uis.cam.ac.uk/account/\n   - Click on each relevant storage project:\n       • rds-djh1002-hodson-rds\n       • rfs-djh1002-hodson-rfs-nfs\n   - Accept the storage Terms & Conditions for each project.\n\n3) Explain workflow\n   - Once Terms & Conditions are accepted, a provisioning request is automatically generated.\n   - HPC Support will then activate access.\n   - All relevant parties will receive automatic email confirmation once provisioning completes.\n\n4) Do NOT:\n   - Manually override provisioning before Terms & Conditions are accepted.\n   - Escalate unless provisioning remains stalled after acceptance.\n\nIF PROVISIONING DOES NOT COMPLETE AFTER ACCEPTANCE\n\n- Confirm acceptance timestamp in portal.\n- Check whether an automatic provisioning ticket was generated.\n- Escalate to storage administration only if backend provisioning fails after acceptance.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Provisioning status for new lab member\n\nDear Joanna,\n\nThank you for your message.\n\nVictor’s access is currently showing as “Provisioning” because he has not yet accepted the storage service Terms & Conditions.\n\nCould he please visit:\n\n  https://selfservice.uis.cam.ac.uk/account/\n\nand click on the relevant project links to accept the Terms & Conditions for each storage project?\n\nOnce this is completed, a provisioning request will automatically be generated and we will activate his access. All parties will receive confirmation by email once this has been processed.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not bypass the mandatory Terms & Conditions acceptance process.\n- Do not grant access before provisioning workflow completes.\n- Ensure both RDS and RFS projects are covered if both were requested.",
    "metadata": {
      "keyword": "RDS/ Research Data Storage",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97622",
    "summary": "TumourVue SRCP platform",
    "query": "Dear Team,\nI appreciate your help with setting up a level 3 (given high IP value) SRCP platform for TumourVue - cancer surgery trials data. Three of my masters students will have access. The data sharing agreement is in place.\nThe students will start next week and I would be grateful if I could please have your support as a high priority.\nThanks a lot\nGita\n—\nGita Khalili Moghaddam, PhD\nPrincipal Investigator\nDepartment of Clinical Neurosciences\nUniversity of Cambridge\nCB2 0QQ\nRoyal Society Industry Fellow\nGSK Global Health\nWhile it suits me to email outside normal working hours, I do not expect a response outside your own.",
    "expected_answer": "CLASSIFICATION\nCategory: SRCP / New Platform Provisioning / Resource Request\nTechnical Action Required: Yes (new SRCP platform setup)\nPortal/Form Action Required: Yes (SRCP resource request form submission)\nEscalation Required: Yes (SRCP platform provisioning team)\nUrgency: Medium–High (students starting next week; high IP / Level 3 data)\n\nQUICK ASSESSMENT\nThe PI is requesting the creation of a new SRCP Level 3 platform (high IP value) for a new project (TumourVue), separate from an existing platform (Predict), with funding in place for one year. Three Masters students will require access with permissions similar to an existing workflow (Python-based image analysis).\n\nThis is not a simple user-access request. It requires submission of an SRCP resource request form to initiate provisioning of a new platform, including:\n- Platform creation\n- Security level confirmation (Level 3)\n- Funding confirmation\n- Storage and compute allocation\n- Role definitions\n\nACTION STEPS (HELPDESK)\n\n1) Clarify request type\n   - Confirm that this is a new SRCP platform (not just additional users).\n   - Confirm that Level 3 classification is required.\n\n2) Direct PI to SRCP resource request form\n   Provide link:\n     https://www.hpc.cam.ac.uk/form/srcp-resource-request\n\n   Explain that this form initiates:\n     - New platform provisioning\n     - Security review\n     - Resource allocation\n     - Funding linkage\n\n3) Provide documentation for security levels\n   Refer to:\n     https://docs.hpc.cam.ac.uk/srcp/user-onboarding/index.html#user-onboarding\n\n   Clarify:\n     - Level 3 environments are designed for high IP / sensitive research data.\n     - Appropriate role selection must be specified for student users.\n\n4) After submission (internal)\n   - Confirm form submission received.\n   - Attach updates to the resource request ticket.\n   - Escalate to SRCP provisioning team.\n   - Do not promise specific timelines until confirmed.\n\n5) Do NOT:\n   - Manually create the platform outside the formal workflow.\n   - Add users before platform provisioning is complete.\n   - Confirm approval until security and provisioning checks are complete.\n\nQUESTIONS TO ASK (ONLY IF NOT CLEAR IN FORM)\n- Confirm expected duration (1 year initially).\n- Confirm estimated storage requirements.\n- Confirm number of users and roles required.\n- Confirm whether this is fully isolated from existing Predict platform.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: TumourVue SRCP Platform Request\n\nDear Gita,\n\nTo create a new SRCP platform for TumourVue, you will need to submit an SRCP resource request using the following form:\n\n  https://www.hpc.cam.ac.uk/form/srcp-resource-request\n\nThis will initiate the provisioning process for a new Level 3 platform, including security review and resource allocation.\n\nYou may also find the following documentation helpful when selecting the appropriate level and roles:\n\n  https://docs.hpc.cam.ac.uk/srcp/user-onboarding/index.html#user-onboarding\n\nOnce the form has been submitted, we will process the request and provide updates via the associated ticket.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not bypass the SRCP resource request process.\n- Ensure Level 3 designation is formally captured in the request.\n- Do not grant access prior to completion of provisioning and security approval.\n- Avoid committing to deadlines until the provisioning team confirms scheduling.",
    "metadata": {
      "keyword": "SRCP/ Secure Research Computing Platform",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97621",
    "summary": "Help creating python/conda environment on CSD3",
    "query": "Hi,\nI am a new user of CSD3 and I am trying to create a virtual environment for the few Python libraries I need for my project (those in requirements.txt file), but I run into the following error when trying to install them by the usual conda -c conda-forge [library]:\nCollecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\nCollecting package metadata (repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nSolving environment: /\nFound conflicts! Looking for incompatible packages.\nThis can take several minutes.  Press CTRL-C to abort.\nfailed\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\nPackage sqlite conflicts for:\npython=3.12 -> sqlite[version='>=3.41.2,<4.0a0|>=3.45.3,<4.0a0|>=3.50.2,<4.0a0']\nPackage tzdata conflicts for:\npython=3.12 -> tzdata\nPackage libcblas conflicts for:\nnumpy=2.3.5 -> libcblas[version='>=3.9.0,<4.0a0']\nPackage libffi conflicts for:\npython=3.12 -> libffi[version='>=3.4,<4.0a0|>=3.4.6,<3.5.0a0|>=3.5.2,<3.6.0a0']\nPackage expat conflicts for:\npython=3.12 -> expat[version='>=2.5.0,<3.0a0|>=2.6.2,<3.0a0|>=2.6.3,<3.0a0|>=2.6.4,<3.0a0|>=2.7.1,<3.0a0']\nPackage mkl_random conflicts for:\nnumpy=2.3.5 -> mkl_random\nPackage bzip2 conflicts for:\npython=3.12 -> bzip2[version='>=1.0.8,<2.0a0']\nPackage libexpat conflicts for:\npython=3.12 -> libexpat[version='>=2.5.0,<3.0a0|>=2.6.2,<3.0a0|>=2.6.3,<3.0a0|>=2.6.4,<3.0a0|>=2.7.0,<3.0a0|>=2.7.1,<3.0a0']\nPackage readline conflicts for:\npython=3.12 -> readline[version='>=8.0,<9.0a0|>=8.1.2,<9.0a0|>=8.2,<9.0a0']\nPackage pip conflicts for:\npython=3.12 -> pip\nPackage blas conflicts for:\nnumpy=2.3.5 -> blas[version='*|1.0',build='mkl|openblas']\nPackage tk conflicts for:\npython=3.12 -> tk[version='>=8.6.12,<8.7.0a0|>=8.6.13,<8.7.0a0|>=8.6.14,<8.7.0a0|>=8.6.15,<8.7.0a0']\nPackage libxcrypt conflicts for:\npython=3.12 -> libxcrypt[version='>=4.4.36']\nPackage openssl conflicts for:\npython=3.12 -> openssl[version='>=3.0.11,<4.0a0|>=3.0.12,<4.0a0|>=3.0.13,<4.0a0|>=3.0.14,<4.0a0|>=3.0.15,<4.0a0|>=3.0.16,<4.0a0|>=3.0.18,<4.0a0|>=3.1.3,<4.0a0|>=3.2.0,<4.0a0|>=3.2.1,<4.0a0|>=3.3.1,<4.0a0|>=3.3.2,<4.0a0|>=3.4.0,<4.0a0|>=3.4.1,<4.0a0|>=3.5.0,<4.0a0|>=3.5.4,<4.0a0']\nPackage libblas conflicts for:\nnumpy=2.3.5 -> libblas[version='>=3.9.0,<4.0a0']\nPackage libsqlite conflicts for:\npython=3.12 -> libsqlite[version='>=3.43.0,<4.0a0|>=3.44.2,<4.0a0|>=3.45.1,<4.0a0|>=3.45.2,<4.0a0|>=3.46.0,<4.0a0|>=3.46.1,<4.0a0|>=3.47.0,<4.0a0|>=3.48.0,<4.0a0|>=3.49.1,<4.0a0|>=3.50.0,<4.0a0|>=3.50.4,<4.0a0']\nPackage python_abi conflicts for:\npython=3.12 -> python_abi=3.12[build=*_cp312]\nnumpy=2.3.5 -> python_abi[version='3.11.|3.12.|3.13.|3.13.|3.14.|3.14.',build='_cp313t|_cp311|_cp312|_cp313|_cp314|_cp314t']\nPackage libuuid conflicts for:\npython=3.12 -> libuuid[version='>=1.41.5,<2.0a0|>=2.38.1,<3.0a0|>=2.41.2,<3.0a0']\nPackage ld_impl_linux-64 conflicts for:\npython=3.12 -> ld_impl_linux-64[version='>=2.35.1|>=2.36.1']\nPackage __glibc conflicts for:\nnumpy=2.3.5 -> __glibc[version='>=2.17,<3.0.a0|>=2.28,<3.0.a0']\npython=3.12 -> __glibc[version='>=2.17,<3.0.a0']\nPackage xz conflicts for:\npython=3.12 -> xz[version='>=5.2.6,<6.0a0|>=5.4.2,<6.0a0|>=5.4.5,<6.0a0|>=5.4.6,<6.0a0|>=5.6.4,<6.0a0']\nPackage libgcc conflicts for:\npython=3.12 -> libgcc[version='>=13|>=14']\nnumpy=2.3.5 -> libgcc[version='>=14|>=14,>=14']\nPackage libgcc-ng conflicts for:\npython=3.12 -> libgcc-ng[version='>=11.2.0|>=12']\nPackage libstdcxx conflicts for:\nnumpy=2.3.5 -> libstdcxx[version='>=14']\nPackage numpy-base conflicts for:\nnumpy=2.3.5 -> numpy-base==2.3.5[build='py314h5cadfd5_0|py312h4bc27c9_0|py311h4bc27c9_0|py311h00548fb_0|py312h00548fb_0|py313h00548fb_0|py313h4bc27c9_0|py314h7c74580_0']\nPackage ncurses conflicts for:\npython=3.12 -> ncurses[version='>=6.4,<7.0a0|>=6.4.20240210,<7.0a0|>=6.5,<7.0a0']\nPackage libopenblas conflicts for:\nnumpy=2.3.5 -> libopenblas[version='>=0.3.30,<1.0a0']\nPackage libnsl conflicts for:\npython=3.12 -> libnsl[version='>=2.0.0,<2.1.0a0|>=2.0.1,<2.1.0a0']\nPackage mkl_fft conflicts for:\nnumpy=2.3.5 -> mkl_fft\nPackage liblzma conflicts for:\npython=3.12 -> liblzma[version='>=5.6.3,<6.0a0|>=5.6.4,<6.0a0|>=5.8.1,<6.0a0']\nPackage libzlib conflicts for:\npython=3.12 -> libzlib[version='>=1.2.13,<2.0.0a0|>=1.3.1,<2.0a0']\nPackage mkl conflicts for:\nnumpy=2.3.5 -> mkl[version='>=2025.0.0,<2026.0a0']\nPackage zlib conflicts for:\npython=3.12 -> zlib[version='>=1.2.13,<2.0.0a0|>=1.2.13,<2.0a0']\nPackage mkl-service conflicts for:\nnumpy=2.3.5 -> mkl-service[version='>=2.3.0,<3.0a0']\nPackage liblapack conflicts for:\nnumpy=2.3.5 -> liblapack[version='>=3.9.0,<4.0a0']\nWhich seems to be due to conflicts, with the central packages?\nI already tried using virtualenv and pip, but some packages (such as pytorch) need to be installed outside of it, so I was trying to use conda for it. I also tried using the central packages, but I cannot make it work. Do you know what is the problem here? Is it something specific to CSD3? Is there a best practice for installing packages in CSD3 in this case?\nBest,\nNico",
    "expected_answer": "CLASSIFICATION\nCategory: Software / Python / Conda Environment Management\nTechnical Action Required: No (user-side configuration guidance)\nEscalation Required: No\nUrgency: Low–Medium (environment setup blocked)\n\nQUICK ASSESSMENT\nUser attempted to create a conda environment with:\n\n  conda create -n gnn-crn -c conda-forge python=3.12\n  conda activate gnn-crn\n  conda install -c conda-forge numpy=2.3.5\n\nand encountered an UnsatisfiableError involving Python 3.12, NumPy 2.3.5, and multiple core libraries.\n\nThe conflict is caused by incompatible package constraints — specifically NumPy 2.x binary compatibility changes combined with packages in the user’s requirements that depend on NumPy <2. This is not specific to CSD3; it is a dependency resolution issue.\n\nAdditionally, users should avoid relying on any centrally provided miniconda installation if outdated, and instead install their own Miniconda in their `hpc-work` space.\n\nACTION STEPS (HELPDESK)\n\n1) Recommend personal Miniconda installation (if not already done)\n\n   Install Miniconda into hpc-work following:\n     https://www.anaconda.com/docs/getting-started/miniconda/install#linux-terminal-installer\n\n   Install into:\n     ~/hpc-work/miniconda3\n\n   Then initialise:\n     source ~/hpc-work/miniconda3/bin/activate\n     conda init\n     source ~/.bashrc\n\n2) Create a clean environment with explicit Python version\n\n   Use a specific patch version to reduce solver ambiguity:\n\n     conda create -n HPCSSUP-97621 python=3.12.4 -y\n     conda activate HPCSSUP-97621\n\n3) Avoid forcing NumPy 2.x unless all dependencies support it\n\n   The earlier failure occurred because:\n     - NumPy 2.x introduces binary-breaking changes.\n     - Some common packages (e.g. matplotlib 3.8.x) require numpy<2.\n     - Explicitly installing numpy=2.3.5 makes the environment logically unsatisfiable.\n\n   Therefore:\n     - Do NOT pin NumPy 2.x unless required.\n     - Let conda resolve NumPy automatically.\n\n4) Install special packages from conda-forge if required\n\n   For example (if rdkit needed):\n\n     conda install -c conda-forge rdkit=2025.09.2 -y\n\n5) Install remaining requirements via pip\n\n     pip install -r requirements.txt\n\n   (This assumes no incompatible hard pins inside requirements.txt.)\n\n6) Best Practice on CSD3\n\n   - Install Miniconda in hpc-work (not /home).\n   - Avoid using outdated central conda modules.\n   - Avoid mixing incompatible major-version pins (e.g. numpy==2.x with legacy stack).\n   - Build environments on login nodes (lightweight), not compute nodes.\n\nIF PROBLEM PERSISTS\n\nAsk for:\n  - Full requirements.txt\n  - Exact conda commands used\n  - Output of:\n      conda info\n      conda list\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Conda environment conflicts on CSD3\n\nHi Nico,\n\nThe error you’re seeing is due to incompatible package constraints rather than anything specific to CSD3. In particular, NumPy 2.x introduces binary changes and some packages (e.g. matplotlib 3.8.x) require `numpy<2`, which makes the environment logically impossible to solve when you explicitly request `numpy==2.3.5`.\n\nWe recommend installing your own Miniconda into your `hpc-work` area and creating a clean environment, for example:\n\n  conda create -n myenv python=3.12.4 -y\n  conda activate myenv\n\nThen allow conda to resolve NumPy automatically rather than forcing version 2.x. After that, you can install your remaining requirements using:\n\n  pip install -r requirements.txt\n\nThis approach should resolve the dependency conflict.\n\nPlease let us know if you continue to experience issues.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not recommend modifying system Python.\n- Do not suggest installing packages into central/shared environments.\n- Avoid advising installation on compute nodes.\n- Keep all installs within user-controlled directories (e.g. hpc-work).",
    "metadata": {
      "keyword": "CSD3",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-97477",
    "summary": "Getting write permission to rds project",
    "query": "Hi,\nI need read and write access to the rds project rds-bru20-ukbb with the project ID isCN5CtgAac. I don’t think it’s possible to add two data managers, but I don’t think I can upload files to that rds without write permission. How else can this permission be given? Thanks!\nBest wishes,\nEmilia",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RDS Permissions / Project Roles\nTechnical Action Required: No (permission governance guidance)\nEscalation Required: No\nUrgency: Medium (write access blocked)\n\nQUICK ASSESSMENT\nUser requests read/write access to an RDS project and assumes only one Data Manager can exist. On RDS projects, multiple Data Managers are allowed. If the user is currently a Data User, write capability should be granted by project governance (Data Owner/Data Manager) either by role reassignment or by applying permissions according to RDS permissions guidance.\n\nACTION\n\n1) Clarify role model\n   - Data Owners can appoint multiple Data Managers.\n   - Data Users cannot self-assign elevated permissions.\n\n2) Direct user to project governance path\n   - Ask user to contact the Data Owner or existing Data Manager for:\n     a) role change to Data Manager, or\n     b) permission grant aligned with RDS ACL guidance.\n\n3) Provide documentation link\n   https://docs.hpc.cam.ac.uk/storage/rds/permissions.html\n\nEXAMPLE CUSTOMER REPLY\n\nHello Emilia,\n\nData Owners can add as many Data Managers as required on an RDS project.\n\nIf you are currently a Data User, you will need to ask the Data Owner or a Data Manager to either:\n1) reassign your role to Data Manager, or  \n2) grant the required permissions in line with the RDS permissions guidance:\nhttps://docs.hpc.cam.ac.uk/storage/rds/permissions.html\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not change project roles/permissions without Data Owner or Data Manager authorisation.\n- Do not imply Helpdesk can bypass project governance for access control.",
    "metadata": {
      "keyword": "permissions",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-96933",
    "summary": "SSH login failing",
    "query": "Hi Sir/Madam,\nHope you are doing well! I am having some troubles in SSHing into the HPC.\nI can successfully log into [https://login-web.hpc.cam.ac.uk](https://login-web.hpc.cam.ac.uk/) using my CRSid (vd327), and my HPC account appears to be active.\nHowever, when I try to log in via SSH (e.g. ssh vd327@login-cpu.hpc.cam.ac.uk), I am prompted for my password and then a TOTP verification code, but after three attempts I receive “Permission denied”.\nI have verified that:\n- I am using my Cambridge Raven/UIS password\n- MFA/TOTP is set up and working for login-web\n- The SSH host key has been accepted correctly\nCould you please advise me on what I should do/ how to fix this?\nMany thanks,\nVivek Dherani",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / SSH / MFA (TOTP)\nTechnical Action Required: No (user configuration issue)\nEscalation Required: No\nUrgency: Medium (access blocked)\n\nQUICK ASSESSMENT\nUser can successfully log into login-web.hpc.cam.ac.uk but receives “Permission denied” when attempting SSH login to login-cpu.hpc.cam.ac.uk.\n\nUser reports:\n- Raven/UIS password is correct.\n- MFA works for login-web.\n- SSH host key accepted.\n- Password recently changed.\n\nWeb login MFA and SSH MFA are separate enrolments. A common cause of this behaviour is that MFA has not yet been enrolled for SSH access (multi.hpc.cam.ac.uk), even though web MFA works.\n\nACTION\n\n1) Explain SSH MFA requirement\n   - Clarify that SSH requires a separate MFA enrolment from login-web.\n\n2) Provide documentation\n   https://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html#walkthrough-ssh-to-multi-hpc-cam-ac-uk\n\n3) Instruct user to enrol SSH MFA via:\n\n   ssh <CRSid>@multi.hpc.cam.ac.uk\n\n   They should:\n   - Enter UIS/Raven password.\n   - Scan the displayed QR code into their authenticator app.\n   - Confirm a new token is created (label similar to “CSD3:SSH Login”).\n\n4) After enrolment, retry SSH to:\n\n   ssh <CRSid>@login-cpu.hpc.cam.ac.uk\n\n5) If this does not resolve the issue:\n   - Ask the user to paste the full SSH output (including prompts but excluding any codes).\n   - Escalate only if MFA enrolment succeeds but authentication still fails.\n\nEXPECTED OUTCOME\nOnce SSH MFA is enrolled correctly, authentication via SSH should succeed using:\n- UIS/Raven password\n- TOTP from the new “CSD3:SSH Login” token\n\nEXAMPLE CUSTOMER REPLY\n\nHello Vivek,\n\nAlthough your MFA is working for login-web, SSH access requires MFA to be set up separately.\n\nA common cause of this behaviour is that SSH MFA has not yet been enrolled. Please follow the walkthrough here:\n\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html#walkthrough-ssh-to-multi-hpc-cam-ac-uk\n\nYou can begin the enrolment process by running:\n\n  ssh <CRSid>@multi.hpc.cam.ac.uk\n\nYou will be prompted for your UIS/Raven password and then shown a QR code to scan into your authenticator app.\n\nOnce this is complete, please try SSH again to:\n\n  ssh <CRSid>@login-cpu.hpc.cam.ac.uk\n\nIf the issue persists, please paste the full SSH output and we’ll investigate further.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume password error if login-web works.\n- Do not reset MFA without identity verification.\n- Do not suggest bypassing MFA.\n- Avoid implying internal verification of MFA enrolment unless explicitly confirmed.",
    "metadata": {
      "keyword": "login",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-96776",
    "summary": "Fw: Unable to submit slurm jobs on callender+",
    "query": "Hi,\nI can’t submit any Slurm jobs using the Callender+ account, PI Dr Callender copied in,  because it doesn’t seem to have any partitions attached to it, so every sbatch call fails with an “Invalid account or account/partition combination” error. I can install packages on the login node, but can’t see that my environment works end-to-end without running jobs on compute nodes. I would be grateful for your advice on whether a CPU partition is available under callender+ and how to resolve this please?\nBest wishes,\nMegha",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / Account & Partition Configuration\nTechnical Action Required: No (user-side correction)\nEscalation Required: No (unless account not present in mybalance)\nUrgency: Medium (job submission blocked)\n\nQUICK ASSESSMENT\nUser reports:\n\n  sbatch: error: Batch job submission failed:\n  Invalid account or account/partition combination specified\n\nThey attempted to submit using:\n  #SBATCH --account=callender+\n  #SBATCH --partition=cclake\n\nThis error typically indicates one of the following:\n\n1) The Slurm account name is incorrect (account names are case-sensitive).\n2) The account name does not exactly match the allocation shown in Slurm.\n3) The account/partition combination is not valid for the user.\n4) The user is not using the correct Slurm account identifier format.\n\nThe correct way to determine valid accounts is via the `mybalance` command.\n\nACTION STEPS (HELPDESK)\n\n1) Ask the user to confirm available accounts\n   Instruct them to run:\n\n     mybalance\n\n   This will display the exact account names and usage limits.\n\n2) Confirm account string matches exactly\n   - Account names are case-sensitive.\n   - Hyphens and underscores are distinct.\n   - Plus signs (“+”) are not normally used in Slurm account names.\n\n3) Ensure correct SBATCH syntax\n   In the submission script:\n\n     #SBATCH --account=<EXACT_ACCOUNT_NAME_FROM_MYBALANCE>\n     #SBATCH --partition=<VALID_PARTITION_FOR_ACCOUNT>\n\n4) Validate partition compatibility\n   If unsure, ask the user to provide:\n\n     mybalance\n     sinfo -s\n\n   Confirm that:\n   - The account is listed.\n   - The partition is valid for that account.\n   - The user is a member of that Slurm account.\n\n5) If the account does not appear in `mybalance`\n   - Verify account membership.\n   - Escalate to cluster administration if necessary.\n\nCOMMON CAUSES TO CHECK\n\n- Typographical error in account name.\n- Using an informal label (e.g. project nickname) instead of the Slurm account identifier.\n- Case mismatch.\n- Replacing hyphens with underscores.\n- Using outdated account name.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Unable to submit Slurm jobs on callender+\n\nHello Megha,\n\nThe error you’re seeing usually indicates that the account name or account/partition combination is not valid.\n\nCould you please run:\n\n  mybalance\n\nThis will show the exact Slurm account names available to you. Account names are case-sensitive and must match exactly in your submission script.\n\nOnce you have confirmed the correct account name, update your script to include:\n\n  #SBATCH --account=<exact account name>\n  #SBATCH --partition=<valid partition>\n\nIf you’re still seeing the error after confirming the account name from `mybalance`, please send the output of `mybalance` and your updated submission script so we can take a closer look.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume account misconfiguration without verifying `mybalance`.\n- Do not manually alter Slurm accounts unless membership is confirmed missing.\n- Avoid guessing account names; always use system-reported identifiers.",
    "metadata": {
      "keyword": "Slurm",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-96609",
    "summary": "Adding student collaborators to RDS disk and supervisor's accounts",
    "query": "Hello HPC support team,\nWe'd like to add Cambridge MPhil students Boqiao Zhang (bz317) and Oszkar Urban (ou222) to:\n1) RDS disk\n`rds-posecraft-5SVx1x5hoTY`\n2) My supervisor Prof Cengiz Oztireli's accounts\nOZTIRELI-SL2-CPU\nOZTIRELI-SL2-GPU\nOZTIRELI-SL3-CPU\nOZTIRELI-SL3-GPU\nPlease let us know if anything or any approval is needed. Many thanks!\nBest wishes,\nZhilin",
    "expected_answer": "CLASSIFICATION\nCategory: Storage Membership / Slurm Account Access\nTechnical Action Required: Yes (conditional – Slurm account membership)\nPortal Action Required: Yes (RDS membership via Self Service)\nEscalation Required: Conditional (PI approval required for Slurm accounts)\nUrgency: Low–Medium (collaborator onboarding)\n\nQUICK ASSESSMENT\nUser requests:\n\n1) Add two students to an RDS storage project:\n   rds-posecraft-5SVx1x5hoTY\n\n2) Add the same users to multiple Slurm accounts:\n   OZTIRELI-SL2-CPU\n   OZTIRELI-SL2-GPU\n   OZTIRELI-SL3-CPU\n   OZTIRELI-SL3-GPU\n\nKey principles:\n\n- RDS project membership is managed by the Data Owner or Data Manager via the Self Service portal. HPC Support does not directly administer storage memberships.\n- Slurm account access can only be granted with explicit approval from the PI/account owner.\n\nACTION STEPS (HELPDESK)\n\n1) RDS Membership\n   - Inform user that storage project membership is controlled via the Self Service portal.\n   - Direct them to confirm that the Data Owner or Data Manager has added the students.\n   - If necessary, advise them to check:\n       https://selfservice.uis.cam.ac.uk/\n\n2) Slurm Account Access\n   - Confirm whether explicit approval from the account owner (PI) has been provided.\n   - If not, request written approval from the PI before proceeding.\n   - Once approval is received, add the users to the specified Slurm accounts.\n\n3) Do NOT:\n   - Add users to Slurm accounts without PI approval.\n   - Modify storage memberships manually.\n   - Assume requester has authority unless they are confirmed Data Owner/PI.\n\nQUESTIONS TO ASK (IF APPROVAL NOT CLEAR)\n\n- Are you the Data Owner or Data Manager of the RDS project?\n- Can the PI/account owner confirm approval for adding these users to the Slurm accounts?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Adding collaborators to RDS and Slurm accounts\n\nDear Zhilin,\n\nRegarding the RDS project, storage memberships are managed by the Data Owner or Data Manager via the Self Service portal. We do not directly administer storage project memberships. Please ensure the relevant Data Owner/Data Manager has added the students through:\n\n  https://selfservice.uis.cam.ac.uk/\n\nFor the Slurm accounts, we can add the students once we have explicit approval from the account owner (PI). If this has not yet been provided, could the PI please confirm approval?\n\nOnce confirmed, we will proceed accordingly.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Storage access must follow Data Owner governance.\n- Slurm account additions require PI approval.\n- Do not bypass project ownership controls.\n- Maintain auditability of account access changes.",
    "metadata": {
      "keyword": "disk",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-96250",
    "summary": "\"[UKAEA] AIRR Access Last Resort\"",
    "query": "I have been invited to join the UKAEA early adopters project on AIRR.  Could you please create a local keycloak entry for me.\nName: Waqar Butt ,\nOrganisation: (UKAEA)\nemail: waqar.butt@ukaea.uk\npartition: UKAEA Early Adopters\nThanks,\nWaqar\n________________________________\nFrom: airr-admin@hpc.cam.ac.uk <airr-admin@hpc.cam.ac.uk>\nSent: 04 December 2025 15:56\nTo: Butt, Waqar <waqar.butt@ukaea.uk>\nSubject: Invitation to UKAEA Early Use project\nYou don't often get email from airr-admin@hpc.cam.ac.uk. Learn why this is important<[https://aka.ms/LearnAboutSenderIdentification](https://aka.ms/LearnAboutSenderIdentification)>\nHello!\nAleksander Dubas has invited you to join UKAEA Early Use project in Researcher (project member) role.\nPlease visit this page<[https://access.hpc.cam.ac.uk/invitation/0aae954f566c4c7485c219c393818a52/](https://access.hpc.cam.ac.uk/invitation/0aae954f566c4c7485c219c393818a52/)> to sign up and accept your invitation. Please note: this invitation expires at 11.12.2025 15:56!\nYou are invited to join the UKAEA Early Adopters project on AIRR. Once you have registered, please continue to follow the instructions at [https://nucleus.ukaea.uk/page/7190?SearchId=7342533](https://nucleus.ukaea.uk/page/7190?SearchId=7342533)\nKind regards,\nResearch Computing Services\nUniversity Information Services\nUniversity of Cambridge\nPlease email support@hpc.cam.ac.uk<[support@hpc.cam.ac.uk](mailto:support@hpc.cam.ac.uk)> if any problems are encountered.",
    "expected_answer": "CLASSIFICATION\nCategory: AIRR / Authentication / Keycloak (External User)\nTechnical Action Required: Yes (local Keycloak account creation)\nEscalation Required: Yes (AIRR team)\nUrgency: Medium (invitation expiry deadline)\n\nQUICK ASSESSMENT\nUser has been invited to join the “UKAEA Early Adopters” project on AIRR and requests creation of a local Keycloak account.\n\nKey points:\n- User is external (UKAEA email).\n- Invitation link has an expiry date.\n- “Local Keycloak entry” indicates use of the documented “without MyAccessID (last resort)” workflow.\n- Account must be created in Keycloak before the user can accept the AIRR invitation via “Sign in with Keycloak”.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm invitation context (internal)\n   - Verify invitation exists for the user email.\n   - Confirm project name: “UKAEA Early Adopters”.\n   - Confirm partition/access level requested.\n\n2) Escalate to AIRR team (if not directly handled by HPC Support)\n   - Forward ticket to AIRR administrators.\n   - Include:\n       • Full name\n       • Organisation\n       • Email\n       • Project/partition name\n   - Mark urgency if invitation expiry is near.\n\n3) Create local Keycloak account (AIRR/admin action)\n   - Create account using provided email.\n   - Trigger password setup email.\n   - Ensure account is linked to the appropriate realm.\n\n4) Notify user\n   - Inform them they will receive a password setup email.\n   - Advise checking spam/junk folders.\n   - Instruct them to:\n       a) Set password,\n       b) Visit invitation link,\n       c) Choose “Sign in with Keycloak”,\n       d) Use newly created credentials.\n\n5) Provide documentation link\n   - Reference official guidance for “Accessing without MyAccessID (last resort)”.\n\n6) Do NOT:\n   - Manually approve the project membership unless that is part of the AIRR workflow.\n   - Ask for passwords.\n   - Override the invitation process.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- Has the invitation expired?\n- Have you received the password setup email?\n- Are you able to access the invitation link successfully?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: UKAEA Early Adopters – AIRR Access\n\nHi Waqar,\n\nYour local Keycloak account has now been set up. You should shortly receive an email to set your password (please check your spam/junk folder if it does not appear).\n\nOnce your password is set, please visit the invitation link and select “Sign in with Keycloak”, then log in using your new credentials.\n\nFor reference, please see:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/pvc.html#accessing-without-myaccessid-last-resort\n\nPlease let us know if you encounter any issues.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not transmit passwords via email.\n- Do not request sensitive credentials.\n- Ensure external accounts are created only through the approved Keycloak workflow.\n- Respect invitation expiry deadlines but do not bypass approval controls.",
    "metadata": {
      "keyword": "AIRR",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-96168",
    "summary": "HPC Job Scheduling Error",
    "query": "To whom it may concern,\nI have submitted a job on the HPC, and it took almost 2 weeks to get scheduled. I was wondering if there were any issues with my setup. The job script I used is below.\n#!/bin/bash\n#SBATCH -J job_gpu_1\n#SBATCH -o /home/av670/logfiles/job_gpu_1.log\n#SBATCH -A SVOBODA-SL3-GPU\n#SBATCH -p ampere\n#SBATCH --gres=gpu:4\n#SBATCH --time=12:00:00\necho \"This job is running on: $(hostname)\"\necho \"Present working directory: $(pwd)\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Job start time: $(date)\"\nmodule purge\necho \"loading python\"\nmodule load python/3.11.0-icl\necho \"loading cuda\"\nmodule load cuda/12.1\necho \"activating venv\"\nsource /rds/user/av670/hpc-work/venvs/venv/bin/activate\necho \"running file\"\npython -u /home/av670/projects/part-ii-project/job.py\nKind regards,\nAmogh Vishwakarma",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / Scheduling / Service Level Priority\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Low–Medium (long wait time but system functioning)\n\nQUICK ASSESSMENT\nUser reports a GPU job taking almost two weeks to start and asks whether there is an issue with their setup.\n\nFrom the job script:\n- Account: SVOBODA-SL3-GPU\n- Partition: ampere\n- GPUs requested: 4\n- Wall time: 12:00:00\n\nThere are no configuration errors visible in the submission script. The most likely cause of delay is scheduler priority and service level.\n\nKey scheduling principles:\n- SL3 accounts operate at the non-paying service level.\n- Paying service-level jobs receive higher priority.\n- Priority increases over time while waiting (aging factor).\n- Large GPU requests (e.g. 4 GPUs) may experience longer queue times depending on demand.\n- Cancelling and resubmitting resets priority and can increase overall wait time.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm no configuration errors\n   - The script contains valid SBATCH directives.\n   - Correct partition and account syntax used.\n   - No obvious resource mis-specification.\n\n2) Explain scheduling behaviour\n   - Jobs are prioritised according to service level and fair-share policy.\n   - Paying activity can temporarily delay SL3 jobs.\n   - Jobs steadily gain priority the longer they wait.\n\n3) Provide best practice guidance\n   - Do not cancel and resubmit while waiting.\n   - Consider reducing resource request (e.g., fewer GPUs) if flexible.\n   - Use `squeue -u <CRSid>` to monitor queue position.\n   - Use `scontrol show job <jobid>` to inspect priority factors if desired.\n\n4) Do NOT:\n   - Escalate unless there is evidence of scheduler malfunction.\n   - Promise faster scheduling.\n   - Modify account priority.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- What is the Job ID?\n- Has the job been pending (PD) the entire time?\n- What does `scontrol show job <jobid>` report under Reason= ?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: HPC Job Scheduling Delay\n\nDear Amogh,\n\nThere is nothing wrong with your account or submission script. Your job is running under an SL3 (non-paying) GPU account, and at times there can be significant demand from higher-priority paying accounts.\n\nPlease note that each paying job can occupy its allocated GPUs for up to 36 hours. Jobs steadily increase in priority the longer they wait, so cancelling and resubmitting would be counterproductive as it resets that priority.\n\nYour job will run when sufficient resources become available.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply scheduler malfunction without evidence.\n- Do not recommend priority manipulation.\n- Avoid suggesting users circumvent service-level policy.\n- Reinforce correct queueing behaviour and expectations.",
    "metadata": {
      "keyword": "Slurm",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95566",
    "summary": "Support for job submisions.",
    "query": "Hello,\nI am trying to submit jobs to run my analysis using my GPU allocation for my project.\nIt seems that there is a 36 hour wall time limit and max number of jobs running at the same time.\nCurrently I am setting of my analysis with:\n#SBATCH --array=0-99\n#SBATCH --time=35:30:00\nSo I don’t think I can make max use of my allocation. Are you able to advice what I can do for this?\nAlso can you advice what settings to use in the slurm script if I do not need a GPU?\nMany thanks\nSam",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / GPU Scheduling / Walltime Limits\nTechnical Action Required: No (policy explanation + workflow guidance)\nEscalation Required: No\nUrgency: Medium (jobs being cancelled due to time limit)\n\nQUICK ASSESSMENT\nUser reports GPU jobs being cancelled after ~36 hours with error:\n\n  *** JOB <jobid> CANCELLED DUE TO TIME LIMIT ***\n\nThey would like to run jobs longer than 3 days and maximise GPU allocation usage.\n\nKey principles:\n- GPU partitions enforce a maximum walltime limit (e.g. 36 hours).\n- This is a scheduler policy limit, not a configuration error.\n- The limit applies regardless of remaining budgeted GPU hours.\n- Allocated GPU hours (account balance) ≠ maximum walltime per job.\n- Long-running workflows should use checkpointing and job chaining.\n\nACTION STEPS (HELPDESK)\n\n1) Confirm policy limit\n   - Explain that the 36-hour walltime is an enforced scheduler limit.\n   - It cannot be overridden per job at the user level.\n\n2) Clarify allocation model\n   - GPU hour budget defines total consumption allowed.\n   - Walltime limit defines maximum duration of a single job.\n   - Having remaining hours does not permit exceeding the per-job limit.\n\n3) Recommend checkpointing (best practice)\n   - Modify code to periodically write restart/checkpoint files.\n   - Submit jobs so that they resume from the last checkpoint.\n   - Use job arrays or chained submissions to continue computation.\n\n4) Optional: suggest job chaining\n   - Use dependent jobs:\n       sbatch --dependency=afterok:<jobid> next_job.sh\n   - Or include automatic resubmission logic if checkpoint detected.\n\n6) Do NOT:\n   - Promise walltime extensions.\n   - Modify partition policy.\n   - Suggest splitting allocation across multiple concurrent long jobs to bypass limits.\n\nQUESTIONS TO ASK (IF NEEDED)\n- Which partition/account are you using?\n- How long does your full workflow need to run?\n- Does your code support checkpoint/restart?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Support for job submissions\n\nHi Sam,\n\nThe 36-hour walltime limit is a scheduler policy on the GPU partition and cannot be extended for individual jobs. Even if you have sufficient GPU hours remaining in your allocation, the maximum runtime per job is still capped.\n\nTo run longer workflows, we recommend implementing checkpointing so that your job can resume from the last saved state once the walltime is reached. You can then submit follow-up jobs to continue from the checkpoint.\n\nPlease let us know if you would like guidance on setting up checkpointing or job dependencies.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Walltime limits are enforced at scheduler level.\n- Allocation balance does not override per-job limits.\n- Avoid suggesting policy circumvention.\n- Promote checkpoint/restart as standard HPC best practice.",
    "metadata": {
      "keyword": "checkpoint",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95541",
    "summary": "Q about how to request CPUs on Wilkes",
    "query": "Hi,\nJust a quick Q please - if I running a process on Wilkes using 1 node and 1 GPU and 1 task but I want to increase the number of CPUs used from 32 to 76, how would I do that?\nI only need 1 GPU, but I would like to use 76 CPUs, can I increase the task number to achieve this?\nThank you for the information.\nCheers,\nMel",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm / GPU-CPU Resource Ratios / Wilkes3 (Ampere)\nTechnical Action Required: No (scheduler policy clarification)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser is running on Wilkes3 (Ampere) requesting:\n- 1 node\n- 1 GPU\n- 1 task\n- Wants to increase CPUs from 32 to 76 while still using only 1 GPU.\n\nOn Wilkes3 Ampere nodes, Slurm enforces a CPU-per-GPU ratio:\n- Maximum 32 CPUs per GPU.\n- This is a scheduler-level constraint.\n- Users cannot request arbitrary CPU counts independent of GPU count.\n\nTherefore:\n- 1 GPU → maximum 32 CPUs.\n- To request ~76 CPUs, the user must request 3 GPUs (32 × 3 = 96 CPUs maximum available).\n\nThe scheduler will not allow 76 CPUs with only 1 GPU.\n\nACTION STEPS (HELPDESK)\n\n1) Explain enforced policy\n   - Ampere nodes allocate CPUs in proportion to GPUs.\n   - 32 CPUs per GPU maximum.\n   - This prevents GPU starvation and ensures fair resource use.\n\n2) Calculate requirement\n   - 76 CPUs ÷ 32 CPUs per GPU ≈ 2.38 GPUs.\n   - Since GPUs are indivisible, user must request 3 GPUs.\n\n3) Provide correct submission guidance\n   Example directives:\n     #SBATCH --gres=gpu:3\n     #SBATCH --cpus-per-task=76\n   (Ensure CPUs requested do not exceed 32 × GPUs requested.)\n\n4) Clarify limitations\n   - Cannot exceed enforced CPU-per-GPU ratio.\n   - Cannot request fractional GPUs.\n   - Cannot override scheduler policy.\n\n5) Do NOT:\n   - Suggest bypassing CPU limits.\n   - Recommend modifying system configuration.\n   - Escalate unless a policy exception is formally requested.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- Which Wilkes partition are you using?\n- Are you using --ntasks or --cpus-per-task?\n- Do you require 76 CPUs per task or across tasks?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: CPU request on Wilkes\n\nHello Mel,\n\nOn Wilkes3 (Ampere), Slurm enforces a maximum of 32 CPUs per GPU to avoid starving GPUs of CPU resources. This means that if you request 1 GPU, you can receive at most 32 CPUs.\n\nTo obtain approximately 76 CPUs, you would need to request 3 GPUs (since 32 × 3 = 96 CPUs available). GPUs must be requested in whole numbers, so fractional allocations are not possible.\n\nFor example:\n\n  #SBATCH --gres=gpu:3\n  #SBATCH --cpus-per-task=76\n\nPlease let us know if you need help adjusting your submission script.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- CPU-per-GPU ratio is scheduler-enforced.\n- Do not imply the limit can be overridden.\n- Avoid suggesting inefficient resource usage beyond policy.\n- Ensure guidance aligns with documented Wilkes3 resource allocation rules.",
    "metadata": {
      "keyword": "Wilkes",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95504",
    "summary": "Issues with GPU compilation of a package",
    "query": "Hello,\nI have been trying to compile the following package on GPU for a Molecular Dynamics code called LAMMPS ([https://github.com/wcwitt/symmetrix/blob/main/pair_symmetrix/README.md](https://github.com/wcwitt/symmetrix/blob/main/pair_symmetrix/README.md)). Given that some of the listed requirements aren’t directly available on CSD3, I have downloaded and compiled CUDA 12.4.1 and CMAKE 3.29. Before compilation I run module purge, then load the following modules:\n1) gcc/9   2) openmpi/gcc/9.3/4.0.4   3) gcc/11\nGcc 11 is required to compile, therefore I have checked that it overrides the automatically loaded gcc/9 bundled with openmpi. Furthermore, I have checked nvcc –version to check CUDA is correctly included in my path, and the same for CMAKE. I then run the following cmake command:\ncmake \\\n-B build-gpu \\\n-D CMAKE_BUILD_TYPE=Release \\\n-D CMAKE_CXX_STANDARD=20 \\\n-D CMAKE_CUDA_COMPILER=$CUDA_HOME/bin/nvcc \\\n-D CMAKE_CXX_COMPILER=$CUDA_HOME/bin/nvcc \\\n-D CUDAToolkit_ROOT=$CUDA_HOME \\\n-D CUDA_TOOLKIT_ROOT_DIR=$CUDA_HOME \\\n-D CMAKE_CXX_STANDARD_REQUIRED=ON \\\n-D CMAKE_CXX_COMPILER=$(pwd)/lib/kokkos/bin/nvcc_wrapper \\\n-D CMAKE_CXX_FLAGS=\"${CMAKE_CXX_FLAGS} -ffast-math\" \\\n-D CMAKE_CUDA_COMPILER=~/rds/rds-ukaea-ap002-mOlK9qn0PlQ/ir-dick3/codes/lammps-symmetrix/cuda-12.4.1/bin/nvcc \\\n-D CUDA_TOOLKIT_ROOT_DIR=~/rds/rds-ukaea-ap002-mOlK9qn0PlQ/ir-dick3/codes/lammps-symmetrix/cuda-12.4.1 \\\n-D CUDA_INCLUDE_DIRS=~/rds/rds-ukaea-ap002-mOlK9qn0PlQ/ir-dick3/codes/lammps-symmetrix/cuda-12.4.1/include \\\n-D CUDA_CUDART_LIBRARY=~/rds/rds-ukaea-ap002-mOlK9qn0PlQ/ir-dick3/codes/lammps-symmetrix/cuda-12.4.1/lib64 \\\n-D CMAKE_CUDA_ARCHITECTURES=80 \\\n-D BUILD_SHARED_LIBS=ON \\\n-D BUILD_OMP=ON \\\n-D BUILD_MPI=ON \\\n-D PKG_KOKKOS=ON \\\n-D Kokkos_ENABLE_SERIAL=ON \\\n-D Kokkos_ENABLE_OPENMP=ON \\\n-D Kokkos_ENABLE_AGGRESSIVE_VECTORIZATION=ON \\\n-D Kokkos_ENABLE_CUDA=ON \\\n-D Kokkos_ARCH_AMPERE80=ON \\\n-D SYMMETRIX_KOKKOS=ON \\\n-D SYMMETRIX_SPHERICART_CUDA=ON \\\ncmake\nThis runs without any errors (except for a CMAKE FIND_CUDA routine deprecated warning, however this shouldn’t be an issue as cmake shows it has found all the correct CUDA libraries, hence the cmake flags for different paths). After running make on this cmake config the binary compiles without any errors. When I run the binary, I get the following error:\n‘’’\n[gpu-q-49:22268] mca_base_component_repository_open: unable to open mca_btl_openib: libosmcomp.so.4: cannot open shared object file: No such file or directory (ignored) [gpu-q-49:22268] mca_base_component_repository_open: unable to open mca_btl_usnic: libfabric.so.1: cannot open shared object file: No such file or directory (ignored) [gpu-q-49:22268] mca_base_component_repository_open: unable to open mca_mtl_ofi: libfabric.so.1: cannot open shared object file: No such file or directory (ignored) -------------------------------------------------------------------------- Primary job terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. -------------------------------------------------------------------------- -------------------------------------------------------------------------- mpirun noticed that process rank 0 with PID 22268 on node gpu-q-49 exited on signal 4 (Illegal instruction).\n‘’’\nThis is from running on a single GPU on the ukaea-amp partition. I load my CUDA 12.4.1 compilation, the same openmpi module used for compilation of lammps and gcc/11. I can’t understand why exactly there is a problem here. Any help on this issue would be hugely appreciated!\nMany thanks\nAshley Dickson",
    "expected_answer": "CLASSIFICATION\nCategory: Software / Compilation / GPU (Ampere, ukaea-amp)\nTechnical Action Required: No (user rebuild with supported stack)\nEscalation Required: No (unless failure persists after rebuild)\nUrgency: Medium (runtime failure on GPU partition)\n\nQUICK ASSESSMENT\nUser successfully compiled LAMMPS + symmetrix with a custom CUDA 12.4.1 toolchain, but runtime fails on ukaea-amp with:\n\n- Open MPI component warnings (libosmcomp.so.4, libfabric.so.1 not found)\n- Process exited on signal 4 (Illegal instruction)\n\nLikely causes:\n1) Incompatible Open MPI module (built for Skylake + RHEL7) being used on Ampere (Zen3 + RHEL8).\n2) Instruction-set mismatch (e.g. -march=native during cross-compilation on login node).\n3) Mixed CUDA/MPI/compiler stack leading to ABI/runtime incompatibilities.\n\nResolution is to rebuild using the supported Ampere test software stack with a consistent compiler/CUDA/MPI toolchain and disable native CPU flags when cross-compiling.\n\nACTION STEPS (HELPDESK)\n\n1) Ensure clean environment\n   - Start a fresh shell.\n   - Remove old build directory:\n       cd /path/to/lammps\n       rm -rf build-gpu\n\n2) Load base modules\n       module purge\n       module load rhel8/global rhel8/slurm\n\n3) Enable Ampere test stack\n       module use /usr/local/software/spack/csd3/spack-modules/test/ampere-2025-06-01/linux-rocky8-zen3\n\n4) Load compatible toolchain\n       module load gcc/14.3.0/vlhhcp6m \\\n                   cuda/12.8.1/gcc/kdeps6ab \\\n                   openmpi/4.1.8/gcc/hemliivg \\\n                   cmake/3.31.10/gcc/7ddsybx7\n\n   Rationale:\n   - OpenMPI built for Zen3 and RHEL8\n   - CUDA ≥12 satisfied\n   - Consistent compiler/runtime stack\n\n5) Reconfigure build\n       cmake \\\n           -B build-gpu \\\n           -D CMAKE_BUILD_TYPE=Release \\\n           -D CMAKE_CXX_STANDARD=20 \\\n           -D CMAKE_CXX_STANDARD_REQUIRED=ON \\\n           -D CMAKE_CXX_COMPILER=$(pwd)/lib/kokkos/bin/nvcc_wrapper \\\n           -D CMAKE_CXX_FLAGS=\"${CMAKE_CXX_FLAGS} -march=znver3 -ffast-math\" \\\n           -D BUILD_SHARED_LIBS=ON \\\n           -D BUILD_OMP=ON \\\n           -D BUILD_MPI=ON \\\n           -D PKG_KOKKOS=ON \\\n           -D Kokkos_ENABLE_SERIAL=ON \\\n           -D Kokkos_ENABLE_OPENMP=ON \\\n           -D Kokkos_ARCH_ZEN3=ON \\\n           -D Kokkos_ENABLE_AGGRESSIVE_VECTORIZATION=ON \\\n           -D Kokkos_ENABLE_CUDA=ON \\\n           -D Kokkos_ARCH_AMPERE80=ON \\\n           -D SYMMETRIX_KOKKOS=ON \\\n           -D SYMMETRIX_SPHERICART_CUDA=ON \\\n           -D SPHERICART_ARCH_NATIVE=OFF \\\n           cmake\n\n   Important:\n   - Disable -march=native (SPHERICART_ARCH_NATIVE=OFF) when building on login node.\n   - Ensure CMake does not mix multiple CMAKE_CXX_COMPILER definitions.\n\n6) Build\n       cmake --build build-gpu -j\n\n7) Runtime\n   - Load the same modules used for compilation before executing.\n   - Submit via Slurm on ukaea-amp.\n   - Do not mix MPI modules between build and run.\n\nEXPECTED OUTCOME\n- No illegal instruction at runtime.\n- No Open MPI component mismatch warnings.\n- GPU-enabled LAMMPS executable runs on ukaea-amp partition.\n\nIF ISSUE PERSISTS\nRequest:\n- Full submission script\n- Full build log\n- Exact runtime command\n- Output of:\n      module list\n      ldd <lammps_binary>\n      ompi_info | head\n\nSAFETY / POLICY NOTES\n- Do not recommend installing custom system-level CUDA drivers.\n- Avoid mixing MPI modules from different OS/compiler stacks.\n- Ensure consistent module environment between compilation and runtime.\n- Do not suggest modifying system libraries under /usr.",
    "metadata": {
      "keyword": "compiler",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95411",
    "summary": "Path to my rfs storage",
    "query": "Dear HPC support,\nI am trying to find the path my my rfs folder on HPC:\nProject name: storageactive\nDirectory: rfs-hem48-storageactive\nProject ID: B1vWzswnwnE\nI looked in /rfs/project/ , but it doesn’t exist there. Where can I find it?\nThank you!\nBest,\nHeather\n–\nHeather Machado\nGroup Leader\nCRUK Career Development Fellow\nDepartment of Pathology\nDepartment of Genetics\nUniversity of Cambridge\nCambridge, UK",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RFS / Access Method\nTechnical Action Required: No (guidance + possible storage conversion request)\nEscalation Required: Yes (if user requests NFS-RFS conversion)\nUrgency: Low–Medium (workflow clarification)\n\nQUICK ASSESSMENT\nUser is attempting to locate an RFS (Research File Share) project under /rfs/project/ on HPC and assumes it should be accessible similarly to RDS. This is incorrect.\n\nStandard RFS storage:\n- Is not directly mounted on CSD3/HPC compute or login nodes.\n- Is accessed via OS-level mounting from institutional machines (SMB on Windows, SMB/NFS on macOS/Linux).\n- Cannot be accessed through the HPC filesystem unless converted to NFS-RFS.\n\nUser’s intended workflow:\n- Active work on RDS\n- Backed-up storage on RFS\n- Copy between RDS and RFS\n\nThis requires clarification of storage models and possibly a request to convert the RFS project to NFS-RFS.\n\nACTION STEPS (HELPDESK)\n\n1) Clarify storage separation\n   Explain that:\n   - RDS = high-performance Lustre storage mounted on HPC under /rds\n   - RFS = Research File Share (backed-up institutional storage)\n   - Standard RFS is not accessible from HPC\n\n2) Answer direct question\n   The path does not exist on HPC because RFS is not mounted there.\n   It must be accessed via:\n   - Windows/macOS mount instructions\n   - SMB/NFS mount from a workstation\n\n3) Provide supported HPC-access option\n   If user needs RFS accessible from HPC, instruct:\n   - Request conversion to NFS-RFS\n   - Provide documentation link (RFS on CSD3 – NFS-RFS)\n   - Clarify that NFS-RFS allows HPC-side mounting but may not offer RDS-level performance\n\n4) Clarify common workflow\n   Typical pattern:\n   - Active datasets → RDS\n   - Backed-up archival data → RFS or RCS\n   - Copy data between systems as required\n   - NFS-RFS if active + backed-up + HPC access needed\n\n5) If user confirms NFS-RFS is desired\n   - Ask them to email support@hpc.cam.ac.uk with:\n       Project name\n       Project ID\n       Confirmation they want RFS converted to NFS-RFS\n   - Escalate to storage team\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- Do you require the RFS space to be mounted directly on HPC?\n- Is this for active HPC workflows or archival/backup only?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Path to RFS storage\n\nHello Heather,\n\nRFS storage is not directly mounted on the HPC filesystem, which is why you cannot find it under /rfs/project/.\n\nStandard RFS is accessed by mounting it from your local machine (e.g. via SMB). It is not available on CSD3 login or compute nodes by default.\n\nIf you require your RFS project to be accessible from HPC, you can request that it be converted to NFS-RFS. This allows it to be mounted on CSD3, although performance characteristics differ from RDS.\n\nPlease see:\nhttps://docs.hpc.cam.ac.uk/storage/rfs/accessing-rfs/rfs-nfs.html\n\nIf you would like to proceed with conversion to NFS-RFS, please email support@hpc.cam.ac.uk with the project name and ID and we can arrange this for you.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply RFS is mounted on HPC by default.\n- Do not suggest manual mounting inside compute jobs.\n- Do not guarantee performance equivalence between RDS and NFS-RFS.\n- Escalate only if conversion request is explicitly confirmed.",
    "metadata": {
      "keyword": "RFS/ Research File Share",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95370",
    "summary": "MPI-related runtime error",
    "query": "Dear Sir/Madam,\nI am encountering an MPI-related runtime error when attempting to use the gmx_mpi executable from the gromacs/2024.5 module on the pvc9 partition. Here is a job submission script as a minimal example to reproduce the issue:\n#!/bin/bash\n#SBATCH --job-name=LSN_prep\n#SBATCH --partition=pvc9\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=gpu:1\n#SBATCH --account=airr-p15-dawn-gpu\n#SBATCH --time=2:00:00\n. /etc/profile.d/modules.sh\nmodule purge\nmodule use /usr/local/dawn/software/spack-rocky9/spack-modules/dawn-2025-03-23/linux-rocky9-sapphirerapids\nmodule load gromacs/2024.5\nmodule load intel-oneapi-compilers/2025.1.0/gcc/5berjkxu\nmpirun -np 1 gmx_mpi -h\nAnd here is the SLURM output I got:\nLoading gromacs/2024.5/oneapi/intel-oneapi-mpi/6c6ghrys\nLoading requirement: glibc/2.34/gcc/wzlt7jl4 gcc-runtime/14.2.0/gcc/nykri6wk\nlibiconv/1.17/gcc/qigiow5l libunistring/1.2/gcc/pfn6mgyq\nlibidn2/2.3.7/gcc/wvlq7snl nghttp2/1.65.0/gcc/wwkph6fx\nopenssl/3.2.2/gcc/3ujzcve5 zlib-ng/2.2.3/gcc/ktid6u24\ncurl/8.11.1/gcc/kwum77u6 gmake/4.4.1/gcc/igeiap6r ncurses/6.5/gcc/goa6wr6a\ncmake/3.31.6/gcc/afkypz52 intel-oneapi-tbb/2022.1.0/oneapi/dzifcpo4\nintel-oneapi-mkl/2025.1.0/oneapi/hc67rita\nintel-oneapi-mpi/2021.15.0/oneapi/ufie2hgm\nheffte/2.4.1/oneapi/intel-oneapi-mpi/fou7zqpv libpciaccess/0.17/gcc/y3rehifv\nxz/5.6.3/gcc/rpltstvq libxml2/2.13.5/gcc/txngsb5d\noneapi-level-zero/1.14/gcc/czctrcfw hwloc/2.11.1/gcc/upz5b3tq\nLoading intel-oneapi-compilers/2025.1.0/gcc/5berjkxu\nLoading requirement: zstd/1.5.6/gcc/yywdc4uz binutils/2.43.1/gcc/o5vnsbyp\n[mpiexec@pvc-s-28] Error: Unable to run bstrap_proxy on pvc-s-28 (pid 65338, exit code 65280)\n[mpiexec@pvc-s-28] poll_for_event (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:157): check exit codes error\n[mpiexec@pvc-s-28] HYD_dmx_poll_wait_for_proxy_event (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:206): poll for event error\n[mpiexec@pvc-s-28] HYD_bstrap_setup (../../../../../src/pm/i_hydra/libhydra/bstrap/src/intel/i_hydra_bstrap.c:1063): error waiting for event\n[mpiexec@pvc-s-28] Error setting up the bootstrap proxies\n[mpiexec@pvc-s-28] Possible reasons:\n[mpiexec@pvc-s-28] 1. Host is unavailable. Please check that all hosts are available.\n[mpiexec@pvc-s-28] 2. Cannot launch hydra_bstrap_proxy or it crashed on one of the hosts.\n[mpiexec@pvc-s-28]    Make sure hydra_bstrap_proxy is available on all hosts and it has right permissions.\n[mpiexec@pvc-s-28] 3. Firewall refused connection.\n[mpiexec@pvc-s-28]    Check that enough ports are allowed in the firewall and specify them with the I_MPI_PORT_RANGE variable.\n[mpiexec@pvc-s-28] 4. slurm bootstrap cannot launch processes on remote host.\n[mpiexec@pvc-s-28]    You may try using -bootstrap option to select alternative launcher.\n[mpiexec@pvc-s-28] HYD_spawn (../../../../../src/pm/i_hydra/libhydra/spawn/intel/hydra_spawn.c:177): execvp error on file /usr/bin/srun (No such file or directory)\n[mpiexec@pvc-s-28] Error: Unable to run bstrap_proxy on pvc-s-28 (pid 65361, exit code 65280)\n[mpiexec@pvc-s-28] poll_for_event (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:157): check exit codes error\n[mpiexec@pvc-s-28] HYD_dmx_poll_wait_for_proxy_event (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:206): poll for event error\n[mpiexec@pvc-s-28] HYD_bstrap_setup (../../../../../src/pm/i_hydra/libhydra/bstrap/src/intel/i_hydra_bstrap.c:1063): error waiting for event\nCould you please advise me how I can resolve this issue? When using an interactive node launched by the command srun --partition=pvc9 -n 1 -c 8 --gres=gpu:1 --nodes=1 --account=airr-p15-dawn-gpu --pty bash, everything worked fine for me.\nThanks a lot for your help.\nBest,\nWei-Tse\n-----------------------------------------------------------------------\nWei-Tse Hsu\nEmail: wei-tse.hsu@bioch.ox.ac.uk\nWebsite: [https://weitsehsu.com](https://weitsehsu.com/)\nPostdoctoral Research Associate in Drug Design in the Biggin Group\nDepartment of Biochemistry\nUniversity of Oxford",
    "expected_answer": "CLASSIFICATION\nCategory: MPI / Slurm Environment / Dawn (PVC9)\nTechnical Action Required: No (user-side environment correction)\nEscalation Required: No (unless issue persists after environment fix)\nUrgency: Medium (job fails in batch mode)\n\nQUICK ASSESSMENT\nThe user reports an MPI bootstrap failure when running:\n\n    mpirun -np 1 gmx_mpi -h\n\nwithin a batch job on the pvc9 partition.\n\nError excerpt:\n    HYD_spawn ... execvp error on file /usr/bin/srun (No such file or directory)\n\nKey observations:\n- The job works correctly in an interactive session launched via `srun`.\n- The batch script manually purges modules and loads a custom module path.\n- The error indicates Intel MPI’s Hydra bootstrap cannot locate `srun`.\n\nMost likely cause:\nThe batch script is not loading the correct Dawn node environment (RHEL9 default stack), so Slurm integration and related paths (including srun) are not correctly initialised inside the job environment.\n\nACTION STEPS (HELPDESK)\n\n1) Ensure correct base environment is loaded\n   Instruct user to add:\n\n       module load rhel9/default-dawn\n\n   immediately after `module purge` in their submission script.\n\n   This ensures:\n   - Correct RHEL9 runtime environment\n   - Proper Slurm integration\n   - Required system paths (including srun)\n   - Compatible MPI environment\n\n2) Recommend simplified module order\n   Example structure:\n\n       module purge\n       module load rhel9/default-dawn\n       module use /usr/local/dawn/software/spack-rocky9/spack-modules/dawn-2025-03-23/linux-rocky9-sapphirerapids\n       module load gromacs/2024.5\n\n   Avoid mixing incompatible stacks unless required.\n\n3) Retest with minimal command\n   After modification, test again with:\n\n       mpirun -np 1 gmx_mpi -h\n\n4) If issue persists, request:\n   - Full submission script\n   - Complete stdout and stderr files\n   - Output of:\n         which mpirun\n         which srun\n         module list\n\nEXPECTED SIGNAL AFTER FIX\n- No Hydra bootstrap error.\n- mpirun executes successfully.\n- gmx_mpi help output prints normally.\n\nBRANCH: If still failing\n- Confirm Intel MPI is correctly integrated with Slurm.\n- Verify that `srun` exists in PATH within job:\n      echo $PATH\n      ls -l /usr/bin/srun\n- Escalate to platform admins only if Slurm binary is genuinely unavailable on compute node (unlikely).\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: MPI-related runtime error on pvc9\n\nHello Wei-Tse,\n\nIt looks like the Dawn node environment is not being fully initialised in your batch job.\n\nPlease add the following line after `module purge` in your submission script:\n\n  module load rhel9/default-dawn\n\nThis loads the correct RHEL9 runtime and Slurm integration required for MPI jobs on pvc9.\n\nAfter adding this, please resubmit the job and let us know if the issue persists. If so, please send the full submission script and output files.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not suggest installing MPI manually.\n- Do not recommend modifying system Slurm paths.\n- Avoid blaming firewall or host availability unless independently verified.\n- Keep troubleshooting limited to supported module stacks.",
    "metadata": {
      "keyword": "MPI",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95197",
    "summary": "Unable to connect to GPU node (gpu-r-4) and start new VNC session",
    "query": "Dear HPC team\nI hope you are well.\nI’m currently experiencing an issue when trying to start a new VNC session. The password window in the VNC viewer keeps blinking so I can not enter the password, and the connection never stabilises. I also tried to connect to the GPU node (gpu-r-4) directly to check whether a session was running, but the connection closes immediately after entering my password with the message: Connection closed by 10.43.79.4 port 22\nIt seems I am unable to access gpu-r-4 to create or manage a new session. Could you please advise on how to resolve this and re-establish a working VNC connection? Since I really need to finish some of the work and it just stopped working.\nThank you very much for your help.\nIryna",
    "expected_answer": "CLASSIFICATION\nCategory: VNC / GPU node access\nTechnical Action Required: No (user guidance)\nEscalation Required: No (unless multiple nodes affected)\nUrgency: Medium (user blocked)\n\nQUICK ASSESSMENT\nUser reports:\n- When starting a new VNC session, the VNC viewer password prompt “keeps blinking” and they cannot enter the password.\n- When attempting to SSH to gpu-r-4, the connection closes immediately after entering their password:\n    \"Connection closed by 10.43.79.4 port 22\"\n\nThis suggests gpu-r-4 is not currently usable for their session (e.g. node-specific issue). The quickest mitigation is to start a new session on a different r node.\n\nACTION STEPS (HELPDESK)\n\n1) Advise user to start a new session on a different r node\n   - Suggest an alternative node such as gpu-r-5.\n\n2) If the user cannot start a session on any r node\n   Request:\n   - A full screenshot of the steps taken in the terminal (showing the command run and the resulting output).\n   - Any error messages shown by the VNC viewer.\n\n3) Escalation criteria (only if needed)\n   - If multiple r nodes show the same behaviour, escalate to systems to check r-node availability/VNC service health.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Iryna,\n\nIt looks like gpu-r-4 is not currently usable for starting or managing your VNC session.\n\nPlease set up a new session on a different r node (for example gpu-r-5). If you still cannot start a session, please send a full screenshot of the steps you are taking in the terminal (including the command and output) and any error messages from the VNC viewer.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not claim a specific root cause (e.g. SSH daemon failure) without evidence.\n- Prefer the minimal workaround (use a different node) before deeper debugging.\n- Escalate only if the issue affects multiple nodes or multiple users.",
    "metadata": {
      "keyword": "VNC",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-95016",
    "summary": "Dawn A100 allocation not visible on AIRR portal",
    "query": "Hello,\nI have accepted the PI invitation for the project:\n\"Agentic AI Workflow Benchmarking and GPU Optimization for Verifiable Automation\"\n(Hypereum Ltd – UKRI AIRR)\nMy PI role is active and I can access the AIRR portal, however:\n- The Dawn (A100 80GB) allocation is not visible under \"UKRI Allocatable Offerings\"\n- I only see Isambard-AI offerings (IAG, IAI, IARA)\nCould you please attach the Dawn allocation to my AIRR project and enable my Dawn user account provisioning?\nThank you!\nGiovanni De Lillo\nPrincipal Investigator – Hypereum Ltd",
    "expected_answer": "CLASSIFICATION\nCategory: AIRR / Resource Allocation / Dawn Hardware Clarification\nTechnical Action Required: No\nPortal Action Required: No\nEscalation Required: No\nUrgency: Low–Medium (allocation visibility query)\n\nQUICK ASSESSMENT\nUser reports that after accepting a PI invitation for an AIRR project, they cannot see a “Dawn (A100 80GB)” allocation under “UKRI Allocatable Offerings” in the AIRR portal, and instead only see Isambard-AI offerings.\n\nThe key issue is a misunderstanding of Dawn hardware. Dawn does not provide NVIDIA A100 GPUs. Dawn is based on Intel Data Centre GPU Max (Ponte Vecchio, PVC) hardware. Therefore, an “A100 allocation” would not be expected for Dawn.\n\nACTION STEPS (HELPDESK)\n\n1) Clarify hardware type\n   - Explain that Dawn operates Intel PVC (Data Centre GPU Max) GPUs.\n   - Confirm that NVIDIA A100 GPUs are not part of the Dawn system.\n\n2) Direct user to documentation\n   - Provide official Dawn hardware documentation link.\n   - Encourage review of the hardware specifications to confirm suitability.\n\n3) Do not:\n   - Attempt to manually attach an A100 allocation.\n   - Modify AIRR portal settings.\n   - Escalate unless there is evidence of a genuine portal misconfiguration.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Giovanni,\n\nThere may have been some confusion regarding the hardware.\n\nDawn operates Intel Data Centre GPU Max (Ponte Vecchio, PVC) GPUs rather than NVIDIA A100 GPUs. As such, an A100 allocation would not be expected to appear for Dawn in the AIRR portal.\n\nYou can find details of Dawn hardware here:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/pvc.html#hardware\n\nPlease let us know if you have any further questions.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply NVIDIA A100 hardware is available on Dawn.\n- Do not suggest attaching incompatible GPU allocations.\n- Avoid making portal-side changes unless a confirmed provisioning error exists.",
    "metadata": {
      "keyword": "AIRR",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-94974",
    "summary": "Quota exceeded with no apparent reason",
    "query": "- Your name\nLiyou Zhou\n- Your account/PI\nlz307/Sebastian pattinson\n- your job submission script\n#!/bin/bash\n#!\n#! Example SLURM job script for Wilkes3 (AMD EPYC 7763, ConnectX-6, A100)\n#! Last updated: Fri 30 Jul 11:07:58 BST 2021\n#!\n#!#############################################################\n#!#### Modify the options in this section as appropriate ######\n#!#############################################################\n#! sbatch directives begin here ###############################\n#! Name of the job:\n#SBATCH -J smolvla_mikasa\n#! Which project should be charged (NB Wilkes2 projects end in '-GPU'):\n#SBATCH -A T2-CS178-GPU\n#! How many whole nodes should be allocated?\n#SBATCH --nodes=1\n#! How many (MPI) tasks will there be in total?\n#! Note probably this should not exceed the total number of GPUs in use.\n#SBATCH --ntasks=1\n#! Specify the number of GPUs per node (between 1 and 4; must be 4 if nodes>1).\n#! Note that the job submission script will enforce no more than 32 cpus per GPU.\n#SBATCH --gres=gpu:1\n#! How much wallclock time will be required?\n#SBATCH --time=36:00:00\n#! What types of email messages do you wish to receive?\n#SBATCH --mail-type=ALL\n#! Uncomment this to prevent the job from being requeued (e.g. if\n#! interrupted by node failure or system downtime):\n#SBATCH --no-requeue\n#! Do not change:\n#SBATCH -p ampere\n#! sbatch directives end here (put any additional directives above this line)\n#! Notes:\n#! Charging is determined by GPU number*walltime.\n#! Number of nodes and tasks per node allocated by SLURM (do not change):\nnumnodes=$SLURM_JOB_NUM_NODES\nnumtasks=$SLURM_NTASKS\nmpi_tasks_per_node=$(echo \"$SLURM_TASKS_PER_NODE\" | sed -e 's/^([0-9][0-9]).$/\\1/')\n#! ############################################################\n#! Modify the settings below to specify the application's environment, location\n#! and launch method:\n#! Optionally modify the environment seen by the application\n#! (note that SLURM reproduces the environment at submission irrespective of ~/.bashrc):\n. /etc/profile.d/modules.sh # Leave this line (enables the module command)\nmodule purge # Removes all modules still loaded\nmodule load rhel8/default-amp # REQUIRED - loads the basic environment\n#! Insert additional module load commands after this line if needed:\n#! Full path to application executable:\nnvidia-smi\napplication=\"./convert.sh\"\n#! Run options for the application:\noptions=\"\"\n#! Work directory (i.e. where the job will run):\nworkdir=\"$SLURM_SUBMIT_DIR\" # The value of SLURM_SUBMIT_DIR sets workdir to the directory\n1. in which sbatch is run.\n#! Are you using OpenMP (NB this is unrelated to OpenMPI)? If so increase this\n#! safe value to no more than 128:\nexport OMP_NUM_THREADS=1\n#! Number of MPI tasks to be started by the application per node and in total (do not change):\nnp=$[${numnodes}*${mpi_tasks_per_node}]\n#! Choose this for a pure shared-memory OpenMP parallel program on a single node:\n#! (OMP_NUM_THREADS threads will be created):\nCMD=\"$application $options\"\n#! Choose this for a MPI code using OpenMPI:\n#CMD=\"mpirun -npernode $mpi_tasks_per_node -np $np $application $options\"\n###############################################################\n1. 1. 1. You should not have to change anything below this line ####\n###############################################################\ncd $workdir\necho -e \"Changed directory to `pwd`.\\n\"\nJOBID=$SLURM_JOB_ID\necho -e \"JobID: $JOBID\\n======\"\necho \"Time: `date`\"\necho \"Running on master node: `hostname`\"\necho \"Current directory: `pwd`\"\nif [ \"$SLURM_JOB_NODELIST\" ]; then\n#! Create a machine file:\nexport NODEFILE=`generate_pbs_nodefile`\ncat $NODEFILE | uniq > archive_logs/machine.file.$JOBID\necho -e \"\\nNodes allocated:\\n================\"\necho `cat archive_logs/machine.file.$JOBID | sed -e 's/\\..*$//g'`\nfi\necho -e \"\\nnumtasks=$numtasks, numnodes=$numnodes, mpi_tasks_per_node=$mpi_tasks_per_node (OMP_NUM_THREADS=$OMP_NUM_THREADS)\"\necho -e \"\\nExecuting command:\\n==================\\n$CMD\\n\"\neval $CMD\n- the working directories or paths you are using (e.g. /rds/user/$USER/hpc-work/foo/bar/), and\n/home/lz307/rds/hpc-work/any4lerobot/openx2lerobot\n- as much detail as possible on what you are working on, what you have tried, and error messages you receive.\nI am working on a dataset building job, literally translating a dataset from 1 format into another format. But I keep running into quota warnings half way through the job.\nCreating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\nCreating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 391.08ba/s]\n12047it [1:05:18,  2.29it/s]Processing episode: 12047, elapsed time: 3919.00 seconds, per episode: 0.33 seconds\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000054.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000054.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000056.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000056.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000055.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000055.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000057.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000057.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000058.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000058.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000059.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000059.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000060.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000060.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000061.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000061.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000062.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000062.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000063.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000063.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000064.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000064.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000065.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000065.png'\nError writing image /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000066.png: [Errno 122] Disk quota exceeded: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000066.png'\nMap:   0%|          | 0/67 [00:00<?, ? examples/s]\u001b[A\nMap:  81%|████████  | 54/67 [00:00<00:00, 1086.58 examples/s]\n12047it [1:05:18,  3.07it/s]\nTraceback (most recent call last):\nFile \"/rds/user/lz307/hpc-work/any4lerobot/openx2lerobot/openx_rlds.py\", line 309, in <module>\nmain()\nFile \"/rds/user/lz307/hpc-work/any4lerobot/openx2lerobot/openx_rlds.py\", line 305, in main\ncreate_lerobot_dataset(**vars(args))\nFile \"/rds/user/lz307/hpc-work/any4lerobot/openx2lerobot/openx_rlds.py\", line 232, in create_lerobot_dataset\nsave_as_lerobot_dataset(lerobot_dataset, raw_dataset, keep_images=keep_images)\nFile \"/rds/user/lz307/hpc-work/any4lerobot/openx2lerobot/openx_rlds.py\", line 168, in save_as_lerobot_dataset\nlerobot_dataset.save_episode()\nFile \"/rds/user/lz307/hpc-work/lerobot/src/lerobot/datasets/lerobot_dataset.py\", line 877, in save_episode\nself._save_episode_table(episode_buffer, episode_index)\nFile \"/rds/user/lz307/hpc-work/lerobot/src/lerobot/datasets/lerobot_dataset.py\", line 927, in _save_episode_table\nep_dataset = embed_images(ep_dataset)\nFile \"/rds/user/lz307/hpc-work/lerobot/src/lerobot/datasets/utils.py\", line 140, in embed_images\ndataset = dataset.map(embed_table_storage, batched=False)\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 562, in wrapper\nout: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3079, in map\nfor rank, done, content in Dataset._map_single(**dataset_kwargs):\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3495, in _map_single\nfor i, example in iter_outputs(shard_iterable):\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3469, in iter_outputs\nyield i, apply_function(example, i, offset=offset)\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3392, in apply_function\nprocessed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/table.py\", line 2270, in embed_table_storage\narrays = [\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/table.py\", line 2271, in <listcomp>\nembed_array_storage(table[name], feature) if require_storage_embed(feature) else table[name]\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/table.py\", line 1795, in wrapper\nreturn pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/table.py\", line 1795, in <listcomp>\nreturn pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/table.py\", line 2140, in embed_array_storage\nreturn feature.embed_storage(array)\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/features/image.py\", line 273, in embed_storage\n[\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/features/image.py\", line 274, in <listcomp>\n(path_to_bytes(x[\"path\"]) if x[\"bytes\"] is None else x[\"bytes\"]) if x is not None else None\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 309, in wrapper\nreturn func(value) if value is not None else None\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/features/image.py\", line 268, in path_to_bytes\nwith xopen(path, \"rb\") as f:\nFile \"/home/lz307/miniconda3/envs/lerobot/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 943, in xopen\nreturn open(main_hop, mode, *args, **kwargs)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.images.image/episode_012047/frame_000054.png'\nI paid for rds-vlamemory-R8GV8V7LHCg 2TB and it is no where near full. I have no idea why would it fail. I tried manually creating this file:\ndd if=/dev/zero of=/home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/datasets/mikasa_robo_tfds_all_1.0.0_lerobot/images/observation.png bs=1M count=1\nIt works no problem",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RDS / Quota (Inode Limit)\nTechnical Action Required: No (user action required to reduce file count)\nEscalation Required: No\nUrgency: Medium (job failing due to quota)\n\nQUICK ASSESSMENT\nThe user reports “Disk quota exceeded” errors while writing files to their RDS project directory:\n\n  /home/lz307/rds/rds-vlamemory-R8GV8V7LHCg/...\n\nAlthough the project has a 2 TB storage quota and is only using ~614 GB, the `quota` output shows:\n\n  rds-R8GV8V7LHCg   files: 1024000*   quota: 1024000   limit: 1024000\n\nThe asterisk (*) indicates that the inode (file count) quota has been reached.\n\nOn RDS, inode quotas are proportional to storage allocation:\n  512,000 inodes per TiB\n\nFor a 2 TiB allocation:\n  2 × 512,000 = 1,024,000 inodes\n\nThe project has reached this inode limit, which prevents creation of additional files, even though space (GB/TB) remains available.\n\nThis explains:\n- “[Errno 122] Disk quota exceeded” when writing new files\n- Bash errors such as:\n  “cannot create temp file for here-document: Disk quota exceeded”\n\nACTION STEPS (HELPDESK)\n\n1) Confirm inode exhaustion\n   Ask user to run:\n     quota\n   and check for an asterisk (*) in the “files” column for the relevant RDS project.\n\n2) Explain distinction between storage quota and inode quota\n   - Storage quota = total disk space (GB/TB)\n   - Inode quota = total number of files\n   - Hitting either limit blocks new file creation\n\n3) Provide remediation options\n\n   Option A — Reduce file count (recommended)\n   - Delete unnecessary small files\n   - Consolidate many small files into archives (e.g. tar)\n   - Combine image files into fewer container formats if possible\n\n   Example:\n     tar -czf archive_episodes_12000_12100.tar.gz episode_0120*\n\n   Option B — Increase storage allocation\n   - Inode quota increases only when total RDS storage allocation increases.\n   - This requires purchasing additional storage capacity.\n   - Refer to RDS quota documentation.\n\n4) Clarify policy\n   - Inode quotas are fixed per TiB.\n   - They cannot be increased independently of storage allocation.\n\n5) Close once user confirms understanding.\n\nQUESTIONS TO ASK (ONLY IF NEEDED)\n- Approximately how many files are being generated?\n- Are these many small image files that could be batched/archived?\n- Would you like guidance on estimating file counts (e.g. using `find | wc -l`)?\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Quota exceeded on RDS project\n\nHello Liyou,\n\nAlthough your project has a 2 TB storage allocation and is not near the space limit, the RDS project has reached its inode (file count) quota.\n\nFrom your `quota` output:\n\n  rds-R8GV8V7LHCg   1024000* files used (limit 1024000)\n\nThe asterisk indicates the inode limit has been reached. On RDS, inode quotas are set at 512,000 files per TiB of storage. For a 2 TiB allocation, this gives a maximum of 1,024,000 files.\n\nOnce this file-count limit is reached, new files cannot be created, even if disk space remains available.\n\nTo proceed, you will need to either:\n- Remove or consolidate a significant number of small files, or\n- Increase the total RDS storage allocation (which would proportionally increase the inode quota).\n\nYou can find more details here:\nhttps://docs.hpc.cam.ac.uk/storage/rds/details.html#usage-quotas\n\nPlease let us know if you would like advice on consolidating files or estimating file counts.\n\nBest wishes,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply a filesystem fault; this is expected quota behaviour.\n- Do not suggest modifying system quota settings directly.\n- Inode quotas are proportional to purchased storage and are not adjustable independently.\n- Avoid recommending unsafe bulk deletion commands.",
    "metadata": {
      "keyword": "quota",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-94925",
    "summary": "Quote",
    "query": "Dear All,\nI’m not sure if this is the correct address to use, I’m trying to purchase the following but Chemistry Stores are insisting on a quote, can you help please?\nCumulus CPU 200,000 hours * £0.01/CPU hour = £2000 PI is Prof Sir David Klenerman dk10012@cam.ac.uk\nand\nResearch Data Store (RDS) 10TB for 3 years = £162/TB* 10 TB = £1620 PI is Prof Sir David Klenerman dk10012@cam.ac.uk\nRegards\nDave\nMr David Pratt (he/him/his)\nSenior Support Technician\nTelephone Liaison Officier\nYusuf Hamied Department Of Chemistry,\nUniversity of Cambridge,\nLensfield Rd. Cambridge, CB2 1EW\nDirect Line 01223 747611\nMurphy's Law\nIf everything seems to be going well, you have obviously overlooked something.",
    "expected_answer": "CLASSIFICATION\nCategory: Procurement / Pricing Information\nTechnical HPC Action Required: No\nFinance Action Required: No (information only)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nThe user is requesting a formal quote for:\n- Cumulus CPU 200,000 hours at £0.01/CPU hour\n- Research Data Store (RDS) 10 TB for 3 years at £162/TB\n\nHPC Support does not issue formal quotes. Pricing information is published and departments must raise purchase orders via their own finance processes.\n\nACTION STEPS (HELPDESK)\n\n1) Clarify that formal quotes are not issued.\n2) Confirm the published pricing model (if appropriate).\n3) Advise that departmental finance teams should proceed using the published rates.\n4) Do not generate bespoke quotation documents.\n5) Do not raise purchase orders on behalf of departments.\n\nEXAMPLE CUSTOMER REPLY\n\nSubject: Re: Quote request for Cumulus CPU and RDS\n\nDear Dave,\n\nThank you for your message.\n\nWe do not issue formal quotation documents for Cumulus CPU time or RDS storage. The services are charged according to the published pricing, and your departmental finance team can proceed based on those rates.\n\nFor reference:\n- Cumulus CPU usage is charged per CPU hour at the published rate.\n- RDS storage is charged per TB per year according to the current pricing schedule.\n\nIf your department requires confirmation of the published pricing, this is available on the HPC documentation pages.\n\nPlease let us know if you have any further questions.\n\nBest regards,  \nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not generate unofficial quote documents.\n- Do not commit to pricing beyond the published rates.\n- Do not raise purchase orders on behalf of users.\n- Ensure any pricing referenced aligns with the current published documentation.",
    "metadata": {
      "keyword": "quote",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-94250",
    "summary": "Could We Add Newer CUDA modules?",
    "query": "Dear HPC Team,\nCould we add a few more recent CUDA modules to the cluster? At the moment I can load cuda/12.1, but many current DL packages (e.g., recent PyTorch/TensorFlow wheels) target CUDA 12.4 or 12.6. Having cuda/12.4 and cuda/12.6available would greatly improve compatibility and avoid building from source.\nIf feasible, installing these alongside the existing cuda/12.1 (plus matching cuDNN where applicable) would be ideal. I’m happy to test the new modules once they’re deployed.\nMany thanks!\nBest regards,\nYijiang Dong",
    "expected_answer": "CLASSIFICATION\nCategory: Software Stack / CUDA Module Availability\nTechnical Action Required: No (guidance only)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser requests CUDA 12.4/12.6 modules for package compatibility. CUDA 12.4 and 12.6 are not officially supported, and CUDA 12.8.1 is available in the Ampere test stack.\n\nACTION\n1) Clarify that CUDA 12.4 and 12.6 are not currently supported.\n2) Provide test stack commands for CUDA 12.8.1.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Yijiang,\n\nWe do not currently provide official CUDA 12.4 or 12.6 modules. We do, however, have a newer test software stack that includes CUDA 12.8.1.\n\nPlease run:\n\nmodule purge\nmodule load rhel8/slurm\nmodule load rhel8/global\nmodule use /usr/local/software/spack/csd3/spack-modules/test/ampere-2025-06-01/linux-rocky8-zen3\nmodule load cuda/12.8.1/gcc/kdeps6ab\n\nPlease let us know if you run into any issues.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not promise unsupported module versions.\n- Distinguish test stack from officially supported stack.",
    "metadata": {
      "keyword": "module",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-93979",
    "summary": "Turbo VNC running issues",
    "query": "Dear HPC Support team\nI hope this email finds you well!\nI was trying to open Turbo VNC but it does not work unfortunately.\nI took following steps to run it:\nI run this. - /opt/TurboVNC/bin/vncserver\nthen i have gotten this started on display login-q-3:9\nthen I press exit\nthen type - ssh -L 5909:localhost:5909 iv285@login-q-3.hpc.cam.ac.uk\nthen password\nHowever, I am getting this error:\nbind [::1]:5909: Cannot assign requested address\nchannel_setup_fwd_listener_tcpip: cannot listen to port: 5909\nCould not request local forwarding.\nLast login: Thu Oct 16 15:26:53 2025 from login-q-3.hpc.cam.ac.uk\n[iv285@login-q-3 ~]$\nCold you advise me what can I do in this case ?\nBest wishes\nIryna",
    "expected_answer": "CLASSIFICATION\nCategory: Remote Access / TurboVNC\nTechnical Action Required: No (usage correction)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser launched VNC on a login node and attempted local port forward, causing binding/forwarding issues. Correct method is to run VNC on an interactive GPU node (e.g. `*-r-gpu`) following documented workflow.\n\nACTION\n1) Instruct user to create VNC session on an interactive node, not login node.\n2) Share the VNC guide and expected command sequence.\n3) Confirm resolution after user retest.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Iryna,\n\nPlease create your TurboVNC session on an interactive GPU node (`*-r-gpu`) rather than a login node. That should resolve the forwarding issue you’re seeing.\n\nPlease follow the attached guide and let us know if you run into further issues.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not encourage running long-lived GUI sessions on login nodes.",
    "metadata": {
      "keyword": "VNC",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-93947",
    "summary": "Lost private side of the SSH key",
    "query": "Dear IT team\nI am Alejo Nevado-Holgado, user \"n_neva1\" of Dawn. A couple of months ago I was trying to SSH into dawn from my laptop, but in order to do that I first had to create an SSH key with the following command:\nssh-keygen -t ed25519 -a 100 -C \"dn_neva1@dawn (AIRR-p19)\"\nThen I sent the public key to this email address in a file called \"id_ed25519.pub\". A couple of days ago I reinstalled Ubuntu, and now I am not able to SSH into Dawn again. I didn't save the private side of the key that I generated before. I was wondering, how could I connect to Dawn again from my laptop? Claude says that I have to generate a new key and send to you the new \"id_ed25519.pub\" file. I attach it in case this would actually work.\nVery best and many thanks for your help\nAlejo\nAlejo Nevado-Holgado\nAssociate Professor\nDepartment of Psychiatry & Big Data Institute | University of Oxford | Oxford | OX3 7JX\nalejo.nevado-holgado@psych.ox.ac.uk<[alejo.nevado-holgado@psych.ox.ac.uk](mailto:alejo.nevado-holgado@psych.ox.ac.uk)>\n[cid:108711ff-f78d-4b60-b2a2-bb8b3941fc7a]  [cid:af762036-bceb-4b4b-a09e-6924cd209afa]",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / SSH Keys (Dawn)\nTechnical Action Required: Yes (append public keys)\nEscalation Required: No\nUrgency: Medium (access blocked)\n\nQUICK ASSESSMENT\nUser lost private key after reinstall and needs replacement key registered. Later issue was device-specific key mismatch: each client device requires its own valid key pair, and the corresponding public key must be appended server-side.\n\nACTION\n1) Append new public key for replacement laptop key.\n2) Clarify per-device key model (one key pair per device).\n3) If attachments fail, request public key pasted as plain text in email body.\n4) Append second device key and confirm access restored.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Alejo,\n\nYou will need a valid SSH key pair on each device you connect from. Please send the public key for your office machine (you can paste it directly as plain text in the email body), and we will append it to your account.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Never request private key material.\n- Accept only public keys.\n- Confirm key addition only after staff-side update is complete.",
    "metadata": {
      "keyword": "private key",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-93204",
    "summary": "sync of rcs data",
    "query": "Dear support team,\nI need to rsync a large dataset from our rcs project (cc234). Doing it from a login node is probably not the way, as I'm being kicked out, I guess because the rsync is too heavy. Is there a dedicated data transfer node I can ssh to, or should I do it via an interactive job?\nThanks,\nYaniv\nSent from Outlook for Android<[https://aka.ms/AAb9ysg](https://aka.ms/AAb9ysg)>",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RCS Data Transfer\nTechnical Action Required: No (best-practice guidance)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser is running heavy `rsync` from login node and getting disconnected. Recommended approach is session persistence (screen/tmux) or GUI transfer workflow; follow RCS transfer best practices.\n\nACTION\n1) Recommend using `screen`/`tmux` for long transfer sessions.\n2) Optionally direct to supported GUI transfer methods for RCS.\n3) Share RCS best-practice guidance.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Yaniv,\n\nFor long RCS transfers, please use a persistent terminal session (for example `screen`/`tmux`) so the transfer can continue if your SSH session drops.\n\nYou can also use a supported GUI transfer method:\nhttps://docs.hpc.cam.ac.uk/storage/rcs/gui.html\n\nBest-practice guidance:\nhttps://docs.hpc.cam.ac.uk/storage/rcs/best-practice.html\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Avoid advising large unsupervised transfers directly from transient login sessions.",
    "metadata": {
      "keyword": "RCS/ Research Cold Store",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-93140",
    "summary": "Gaia Arcus cloud ceph connection issues",
    "query": "Hi,\nWe are having some issues with various ceph shares on the Arcus cloud \ntoday and yesterday. Seeing metadata servers switch to reconnecting e.g.:\n> a900cf30-f8a3-42bf-98d6-af7ce92f1a1a.client1188705722/mds_sessions\n> ::::::::::::::\n> global_id 1188705722\n> name \"iris-gaia-code-spec-ro\"\n> mds.1 reconnecting\n> a900cf30-f8a3-42bf-98d6-af7ce92f1a1a.client1188705717/mds_sessions\n> ::::::::::::::\n> global_id 1188705717\n> name \"process-space-1\"\n> mds.1 reconnecting\nThis is causing filesystem access to hang.\nRebooting the instances seems to be clearing this in some cases but it \nis happening on multiple instances and also recurring. Is there any \nindication of an underlying issue?\nCheers,\nPatrick.",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / CephFS / MDS connectivity (Arcus cloud)\nTechnical Action Required: Yes (service-side investigation/mitigation)\nEscalation Required: Yes (Ceph/storage administrators + network)\nUrgency: High (filesystem hangs across multiple instances)\n\nQUICK ASSESSMENT\nUser reports widespread CephFS mount and access hangs across multiple Arcus cloud instances, with client session state showing MDS-related reconnect/closing behaviour, repeatedly implicating mds.1:\n\n- mds.1 reconnecting\n- mds.1 closing\n- Some shares using mds.0 remain open/working while those on mds.1 are unavailable.\n- Mount attempts can time out (mount error 110 = Connection timed out).\n\nBecause the issue affects multiple instances and is correlated with one MDS, this is likely a service-side CephFS/MDS or network path issue rather than a per-instance client misconfiguration. Rebooting instances may temporarily clear symptoms but does not address underlying cause.\n\nIMMEDIATE OBJECTIVE\nRestore stable CephFS access (especially for shares served by mds.1) and prevent recurrence.\n\nACTION STEPS (HELPDESK)\n\n1) Acknowledge and treat as service incident\n   - Confirm the issue affects multiple instances and is correlated with mds.1 session state.\n   - Prioritise service-side checks over user-side troubleshooting.\n\n2) Collect minimal high-signal diagnostics from user (non-disruptive)\n   Ask for:\n   - Example affected instance name(s)\n   - Exact mount command(s) used\n   - Timestamp(s) of failures\n   - One example of:\n       mount output / error\n       and any relevant Ceph mount logs if readily available\n\n   Do NOT request extensive debugging from user while service appears degraded.\n\n3) Service-side investigation (storage/admin)\n   Check CephFS health and MDS status, focusing on mds.1:\n   - Ceph cluster health summary and recent warnings\n   - CephFS filesystem status and active/standby MDS state\n   - MDS daemon logs around the failure window\n   - Client session counts, reconnect/eviction activity, and caps issues\n   - Any ongoing rebalancing/recovery that could impact metadata latency\n\n4) Network path verification (admin)\n   Because symptoms include timeouts and MDS reconnecting/closing, verify:\n   - Routing and interface configuration on the MDS host(s)\n   - Any recent network changes affecting the MDS VLAN/overlay\n   - That any required additional routes are present and persistent\n\n5) Mitigation\n   If mds.1 is unhealthy or unreachable:\n   - Correct the underlying MDS connectivity issue (preferred)\n   - If needed, fail over / restart MDS appropriately per operational runbooks\n   - Confirm clients can re-establish sessions without repeated reconnect/closing\n\n6) Communicate workaround (optional, cautious)\n   Rebooting instances can re-establish mounts in some cases, but it may be disruptive.\n   Only suggest rebooting as a workaround if:\n   - The user confirms it is operationally acceptable, and\n   - Service stability has been restored on the backend.\n\n7) Confirm resolution\n   - Ask user to retry mounting an affected share and confirm responsiveness.\n   - Monitor for recurrence over a short window.\n\nDO NOT\n- State that the issue is definitively caused by the client or by instance configuration without evidence.\n- Advise changes to client mount options as the primary fix when service-side MDS correlation is strong.\n- Rely on instance reboots as the only resolution.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Patrick,\n\nThanks for the detailed report — the repeated “mds.1 reconnecting/closing” messages and the fact that this is affecting multiple instances suggests a service-side CephFS/MDS connectivity issue rather than a per-instance configuration problem.\n\nWe’re investigating the CephFS state now, focusing on mds.1. In the meantime, could you please share:\n- the names of one or two affected instances,\n- the approximate timestamps of the most recent timeouts,\n- and the exact mount command/output (e.g. the “Connection timed out” example)?\n\nOnce we’ve stabilised the backend, we can discuss whether rebooting affected instances is worthwhile for clean remount on startup (only if it won’t be disruptive for you).\n\nCheers,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Treat multi-instance CephFS hangs as a service incident; avoid shifting burden to users.\n- Avoid making claims about root cause unless supported by service-side evidence.\n- Suggest disruptive workarounds (instance reboot) only with explicit user confirmation and after backend stability is restored.\n- Do not request or expose sensitive infrastructure details in user-facing messages.",
    "metadata": {
      "keyword": "Arcus",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-93090",
    "summary": "Quote for extra GPU core times and RDS disk spaces",
    "query": "Dear support team,\nOur team wanted to top up some more GPU core times and extra rds disk spaces for project 'QAKG-SL2-GPU'? Could you please give us a quote for the price of 10000 extra GPU hours and 20 GB extra rds space?\nBest wishes,\nWenyu\nThe University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336. Is e buidheann carthannais a th' ann an Oilthigh Dh?n ?ideann, cl?raichte an Alba, ?ireamh cl?raidh SC005336.",
    "expected_answer": "CLASSIFICATION\nCategory: CORE Procurement / Quoting\nTechnical Action Required: No (quote and procurement guidance only)\nFinance Action Required: Yes\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser requests a quote for 10,000 additional GPU hours and 10TB additional RDS storage. This should be handled as a pricing/quote response with a clear cost breakdown and next-step PO guidance.\n\nACTION STEPS (HELPDESK)\n\n1) Validate internally before replying\n   - Confirm current published rates and terms with the relevant team/process.\n   - Confirm units, duration, and VAT treatment assumptions.\n   - If any part is bespoke or unclear, get storage/finance confirmation first.\n\n2) Confirm quote inputs\n   - Additional GPU hours: 10,000\n   - Additional RDS storage: 10TB\n   - Applicable pricing:\n     - GPU hours: £0.55 per hour\n     - RDS storage: £54 per TB per year\n\n3) Provide calculation breakdown\n   - GPU: 10,000 × £0.55 = £5,500.00\n   - RDS: 10 × £54.00 = £540.00 (per year)\n   - Total ex VAT: £6,040.00\n\n4) Perform final pre-send check\n   - Reconfirm arithmetic and units in the drafted reply.\n   - Ensure VAT wording and assumptions are explicit.\n\n5) Provide procurement next step\n   - Ask user to raise and submit a PO referencing the project/account.\n   - State that VAT is additional at applicable rate.\n\n6) Scope guard\n   - Keep response limited to the quote request unless user explicitly asks for account changes or additional actions.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Wenyu,\n\nPlease accept this as a written quote (ex VAT) for your request:\n\n- GPU hours: 10,000 × £0.55 = £5,500.00\n- RDS storage: 10TB × £54.00/TB/year = £540.00\n- Total (ex VAT): £6,040.00\n\nVAT is additional at the applicable rate.\n\nPlease send the PO to this ticket when ready.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Keep quote assumptions explicit (units, duration, ex VAT).\n- Do not apply resource or storage changes before PO receipt/approval.\n- Do not send pricing calculations externally until rates and assumptions are internally verified.",
    "metadata": {
      "keyword": "CORE",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-92768",
    "summary": "check point example",
    "query": "To whom it may concern,\nWe have a job on HPC that takes more than 36 hours to run, and we would like to try the check point method. Could you provide some examples of how to use the check point?\nThanks,\nQi",
    "expected_answer": "CLASSIFICATION\nCategory: Job Runtime / Checkpointing Guidance\nTechnical Action Required: No (advisory)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser asks for checkpointing examples for jobs exceeding wall time. Provide concise, generic checkpoint patterns (periodic save + resume path), point to the linked reference, and clarify that implementation must be adapted to the user’s application and CSD3 environment.\n\nACTION\n1) Explain user-level checkpointing concept.\n2) Provide 1-2 generic checkpoint patterns the user can apply.\n3) Provide external reference as conceptual guidance.\n4) Clarify that commands/examples may need adaptation for local environment.\n\nEXAMPLE CHECKPOINT PATTERNS (GENERIC)\n1) Periodic state save + restart\n   - Run application so it writes restart files every N minutes/iterations to a checkpoint directory.\n   - On next job run, detect latest checkpoint and resume from it instead of starting from scratch.\n\n2) Pre-timeout save + requeue/next run\n   - Configure application (or wrapper) to write a final checkpoint before wall-time expiry.\n   - Submit the next job to resume from that checkpoint file.\n\nGeneric wrapper pattern:\n```\nif [ -f checkpoints/latest.ckpt ]; then\n  my_app --resume checkpoints/latest.ckpt\nelse\n  my_app --start\nfi\n```\n\nEXAMPLE CUSTOMER REPLY\n\nHi Qi,\n\nWe don’t have a ready CSD3-specific checkpoint script to provide, but the standard approach is:\n\n1) write restart/checkpoint files periodically during the run, and  \n2) start the next job by resuming from the latest checkpoint.\n\nTwo generic patterns you can use are:\n- Periodic checkpointing: save state every N iterations/minutes, then resume from the most recent checkpoint on the next run.\n- Pre-timeout checkpointing: write a final checkpoint before wall-time is reached, then submit the next job to resume from it.\n\nFor example, your wrapper logic can be:\n\n```\nif [ -f checkpoints/latest.ckpt ]; then\n  my_app --resume checkpoints/latest.ckpt\nelse\n  my_app --start\nfi\n```\n\nThis overview may help:\nhttps://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html#the-checkpointing-technique\n\nPlease note these are generic patterns and may need adaptation for your application and CSD3.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply native automatic checkpoint support unless confirmed.",
    "metadata": {
      "keyword": "checkpoint",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-92750",
    "summary": "Accessing HPC after maintenance works",
    "query": "Hello,\nI am having problems logging in to hpc via ssh irs38@login-gpu.hpc.cam.ac.uk as usual, even though an email has been sent out confirming the end of the maintenance work. I just wanted to check if there isn't something I am missing, or if it is just taking time until hpc will be fully available.\nKind regards,\nIeva",
    "expected_answer": "CLASSIFICATION\nCategory: Service Availability / Post-Maintenance\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser reports inability to SSH shortly after maintenance completion notice. Correct response is that services may still be coming online; direct users to service status and communications.\n\nACTION\n1) Inform user service restoration is still in progress.\n2) Provide status page and advise monitoring updates.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Ieva,\n\nWe are still bringing services online following maintenance. Please keep an eye on service communications and the status page for updates:\nhttps://www.hpc.cam.ac.uk/service-status\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not declare full service restoration until confirmed.",
    "metadata": {
      "keyword": "maintenance",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-92485",
    "summary": "Request for ~/rfs Symlink in Home Directory",
    "query": "Hi HPC Support Team,\nI noticed that my home directory does not have a symlink ~/rfs pointing to the RFS project folder. Currently, I can access the project data using the absolute path /rfs/project/rfs-iCNyzSAaucw, but having the symlink would make it much more convenient.\nCould you please create the ~/rfs symlink in my home directory?\nThank you so much for your help!\nBest regards,\nHansheng",
    "expected_answer": "CLASSIFICATION\nCategory: Filesystem / Symlink\nTechnical Action Required: No (command guidance only)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser requests a convenience symlink `~/rfs` pointing to their RFS project path.\n\nACTION\n1) Confirm the user’s RFS project path (if not already known).\n2) Instruct user to create the symlink from their home directory:\n   - `cd $HOME`\n   - `ln -s <RFS_PROJECT_PATH> $HOME/rfs`\n3) Ask user to verify:\n   - `ls -ld $HOME/rfs`\n4) Confirm that `~/rfs` is a convenience link to the project path, not a separate storage area.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Hansheng,\n\nYou can create the `~/rfs` symlink by running:\n\ncd $HOME\nln -s <RFS_PROJECT_PATH> $HOME/rfs\n\nThen verify it with:\n\nls -ld $HOME/rfs\n\nThis creates a convenience link in your home directory to your RFS project path.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not remove or overwrite an existing `~/rfs` path without checking what it currently points to.",
    "metadata": {
      "keyword": "RFS/ Research File Share",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-91750",
    "summary": "problems reading module mpi.mod",
    "query": "hello,\nafter upgrading from gfortran-5.4 to gfortran-13.3 my fortran based mpi \ncode produced interface related warnings during compile and crashed \nduring execution. Replacing 'include mpif.h' statements with 'use mpi' \nstatements (plus a few other changes) fixed the problems on my local \nlinux box. However, when compiling on CSD3 I get the following mpi.mod \nerror:\nf951: Fatal Error: Reading module \n'/usr/local/software/spack/csd3/opt-2024-06-01/linux-rocky8-cascadelake/gcc-13.3.0/intel-oneapi-mpi-2021.12.1-cvatknvvaxe4ohzbsri6keicyjoy3w7f/mpi/2021.12/include/mpi/mpi.mod' \nat line 1 column 2: Unexpected EOF\nThe code is in /home/mh526/Atham/svn/branches/FlAshPlume. The compiler \nsettings are in \n/home/mh526/Atham/svn/branches/FlAshPlume/source/Makefile.COMPILE\nFor the compile I load the following modules:\nmodule load rhel8/cclake/base\nmodule load gcc/13.3.0/7ukda7ns\nmodule load netcdf-fortran-pnetcdf/4.6.1/gcc/intel-oneapi-mpi/ptuvn62k\nThe compile command  that produces the error is\n/usr/local/software/spack/csd3/opt-2024-06-01/linux-rocky8-cascadelake/gcc-13.3.0/intel-oneapi-mpi-2021.12.1-cvatknvvaxe4ohzbsri6keicyjoy3w7f/mpi/2021.12/bin/mpif90 \n-c -DMPI -g -O3 -ffree-line-length-none -mcmodel=medium \n-I/usr/local/software/spack/csd3/opt-2024-06-01/linux-rocky8-cascadelake/gcc-13.3.0/netcdf-fortran-4.6.1-ptuvn62k6et3sihhin2vrwbtyjuycp4o/include \n-I/usr/local/software/spack/csd3/opt-2024-06-01/linux-rocky8-cascadelake/gcc-13.3.0/parallel-netcdf-1.12.3-sn2y3yw32wdavsrvyy6g2dgru3x363d3/include \n/home/mh526/Atham/svn/branches/FlAshPlume/source/Basic/precision.F90\nAre you able to tell me what the problem is? Am I using the wrong \nmpi.mod file?\nMany thanks,\nMichael",
    "expected_answer": "CLASSIFICATION\nCategory: Build/Compiler Toolchain / MPI Fortran Modules\nTechnical Action Required: Yes (workaround instruction)\nEscalation Required: Optional (packaging improvement)\nUrgency: Medium\n\nQUICK ASSESSMENT\n`gfortran 13.3` with Intel MPI hit `mpi.mod` read error (`Unexpected EOF`) because Intel MPI Fortran module files in that build were available only for older gfortran ABI versions. Workaround: explicitly include compatible `gfortran/11.1.0` (or `10.2.0`) MPI module path.\n\nACTION\n1) Confirm exact module environment and minimal repro.\n2) Explain ABI/module version mismatch.\n3) Provide compile workaround with explicit include path.\n4) Confirm successful compile with user.\n5) Track packaging improvement for future stack.\n\nEXAMPLE CUSTOMER REPLY\n\nHi Michael,\n\nPlease try compiling with an explicit MPI Fortran module include path:\n\nmpif90 -I /usr/local/software/spack/csd3/opt-2024-06-01/linux-rocky8-cascadelake/gcc-13.3.0/intel-oneapi-mpi-2021.12.1-cvatknvvaxe4ohzbsri6keicyjoy3w7f/mpi/2021.12/include/mpi/gfortran/11.1.0 -c -DMPI precision.F90\n\n(Using `.../gfortran/10.2.0` is also a valid workaround.)\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Be explicit this is a compatibility workaround, not ideal long-term packaging.",
    "metadata": {
      "keyword": "MPI",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-91551",
    "summary": "Run MPI process on cluster",
    "query": "Hi,\nIs it possible to run MPI-enabled processes on nodes?\n[mf810@cpu-q-209 fasta]$ mpirun -n 1 ~/meme-mpi/bin/meme --help\nMPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found\nAbort(1090959) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Init: Other MPI error, error stack:\nMPIR_Init_thread(178)........:\nMPID_Init(1532)..............:\nMPIDI_OFI_mpi_init_hook(1474):\n(unknown)(): Other MPI error\nThanks in advance!\nBest regards,\nMilena Flankova",
    "expected_answer": "CLASSIFICATION\nCategory: Job Execution / MPI under Slurm\nTechnical Action Required: No (user guidance with documented launch pattern)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser attempted direct `mpirun` and saw MPI startup/initialisation errors. For this environment, MPI jobs should be run inside a Slurm allocation using the documented submission pattern. Running directly from a login shell is a common cause of PMPI/launcher errors.\n\nACTION STEPS (HELPDESK)\n1) Confirm launch context\n   - Ask whether command was run:\n     - directly on a login node, or\n     - inside a Slurm job allocation.\n\n2) Provide documented MPI job pattern\n   - Instruct user to submit via `sbatch` with account/partition/resources set.\n   - Recommend launching tasks with `srun` inside the job script.\n\n3) Provide minimal working template\n```bash\n#!/bin/bash\n#SBATCH -A <ACCOUNT>\n#SBATCH -p <PARTITION>\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:10:00\n\nmodule purge\nmodule load <required-modules>\n\nsrun ~/meme-mpi/bin/meme --help\n```\n\n4) If user needs interactive testing\n   - First allocate resources interactively (documented method), then run with `srun` in that allocation.\n\n5) Reference documentation\n   - Share the MPI jobs section for the relevant partition.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Milena,\n\nThis error usually occurs when MPI is launched outside a proper Slurm allocation.\n\nPlease run your MPI command through a Slurm job script, for example:\n\n```bash\n#!/bin/bash\n#SBATCH -A <ACCOUNT>\n#SBATCH -p <PARTITION>\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:10:00\n\nmodule purge\nmodule load <required-modules>\n\nsrun ~/meme-mpi/bin/meme --help\n```\n\nSubmit with:\n\n`sbatch <script_name>.sh`\n\nPlease also refer to:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/a100.html?highlight=mpirun#jobs-requiring-mpi\n\nUsing the documented Slurm launcher pattern should resolve this startup issue.\n\nRegards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Avoid ad-hoc launcher advice that bypasses Slurm resource allocation.\n- Keep examples aligned with documented partition/account usage.",
    "metadata": {
      "keyword": "MPI",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-91107",
    "summary": "Copying large file to RCS",
    "query": "Dear Support\nI started to copy a 108GB zip file to RCS when it occurred to me that this might be a bad idea. It quickly completed anyway. Do you recommend that I don't do this or is it OK?\nKind regards\nSimon",
    "expected_answer": "CLASSIFICATION\nCategory: Storage / RCS Transfer Best Practice\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser asks if transferring a 108GB zip file to RCS is problematic. Large archives are acceptable. For RCS workflows, tar is usually preferred; zip is still acceptable where password protection or Windows tooling requires it. Bigger single archives are typically slower to stage/retrieve, so archive layout should match expected access/retrieval patterns.\n\nACTION\n1) Confirm transfer is acceptable.\n2) Recommend best-practice docs.\n3) Prefer tar where feasible; zip acceptable when required (e.g. password protection).\n4) Explain operational trade-off of archive sizing (fewer large files vs faster partial retrieval).\n5) Recommend integrity verification after transfer (e.g. checksum comparison).\n\nEXAMPLE CUSTOMER REPLY\n\nHello Simon,\n\nThat is generally fine. For RCS, tar archives are usually preferred, but zip is acceptable if needed (for example for password protection).\n\nThe main trade-off is that very large archives can take longer to retrieve from cold storage. If you expect partial restores, splitting data into sensible archive chunks can help operationally.\n\nAfter transfer, it is also good practice to verify integrity (for example with checksums) between source and destination.\n\nBest-practice guidance:\nhttps://docs.hpc.cam.ac.uk/storage/rcs/best-practice.html\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not present size threshold as a hard prohibition unless policy defines one.\n- Do not overstate archive-size rules; frame them as performance/operational trade-offs.",
    "metadata": {
      "keyword": "RCS/ Research Cold Store",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-90029",
    "summary": "Unable to connect to rcs.uis.cam.ac.uk",
    "query": "Hi,\nI have noticed that me and my colleague are unable to connect to the RCS. We both are connecting by SFTP, rcs.uis.cam.ac.uk on port 22 and both get the same unable to connect message.\nWe have also had a ticket from a user in our department explaining that she is also unable to connect where she was able to last week.\nIs there a problem with the service at the moment? Or has anything changed that might be causing this?\nThanks\nKind Regards,\nShaun Cooke (he/him)\nIT Service Manager\nDepartment of Plant Sciences\nUniversity of Cambridge\nDowning Street, Cambridge, CB2 3EA\nTel: 44 (0)1223 748962 | stc48@cam.ac.uk<[stc48@cam.ac.uk](mailto:stc48@cam.ac.uk)>\n[https://www.plantsci.cam.ac.uk<https://www.plantsci.cam.ac.uk/](https://www.plantsci.cam.ac.uk%3Chttps://www.plantsci.cam.ac.uk/)>\nThis email (together with any files transmitted with it) is intended only for the use of the individual(s) to whom it is addressed. It may contain information which is confidential and/or legally privileged. If you have received this email in error, please notify the sender and delete the original message.",
    "expected_answer": "CLASSIFICATION\nCategory: Access / RCS SSH-MFA\nTechnical Action Required: No (user onboarding guidance)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nFollow-up query: one user can log in to login-web with MFA, but still gets “Access Denied” on `rcs.uis.cam.ac.uk`. In this scenario, the most likely cause is missing SSH/storage MFA enrollment. The web MFA token and SSH/storage MFA token are separate.\n\nACTION\n1) Clarify token separation:\n   - login-web MFA token is for web access only.\n   - SSH/storage services require the SSH TOTP token configured via `multi.hpc.cam.ac.uk`.\n2) Instruct affected users to complete SSH MFA setup using official documentation.\n3) Ask each affected user to raise their own ticket for account-specific checks if issue persists.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Shaun,\n\nIf a user can access login-web but still gets “Access Denied” on `rcs.uis.cam.ac.uk`, this usually means SSH MFA is not yet set up for storage access.\n\nThe web MFA token and SSH/storage MFA token are separate.\n\nFor users seeing access denied on `rcs.uis.cam.ac.uk`, please ensure they complete SSH MFA setup using the documented process (via `multi.hpc.cam.ac.uk`):\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html\n\nIf any user still has issues after setup, please ask them to raise their own ticket so we can verify account-specific details directly.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not troubleshoot third-party user accounts through one reporter without user-specific consent/context.",
    "metadata": {
      "keyword": "RCS/ Research Cold Store",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-89914",
    "summary": "SKA-SRC: error creating blazar lease in ska-src-science-validations tenancy",
    "query": "We are having problems creating a GPU platform on the “ska-src-science-validations” tenancy on Azimuth (with OpenHPC v3).\nI always seem to have the error:\nerror creating blazar lease - Not enough hosts available\nand it doesn’t matter which flavour (standard or v2) to use for login and control nodes.\nDoes this mean that I’m doing something wrong in the setup, or is there an issue with blazar on that tenancy?\nThanks a lot,\nEd",
    "expected_answer": "CLASSIFICATION\nCategory: Cloud Capacity / Azimuth / Blazar Leasing\nTechnical Action Required: No (capacity explanation)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\n`Not enough hosts available` in Blazar indicated resource exhaustion, not a user misconfiguration.\n\nACTION\n1) Explain that message reflects current capacity saturation.\n2) Advise retry when capacity frees.\n\nEXAMPLE CUSTOMER REPLY\n\nHi Edward,\n\nThat error indicates capacity is currently exhausted in the tenancy; it is not necessarily a setup error on your side.\n\nPlease retry when resources become available.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not suggest configuration changes if capacity is the real limiter.",
    "metadata": {
      "keyword": "SKA",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-89749",
    "summary": "HPC login TOTP",
    "query": "Dear HPC support,\nI use the university MS authenticator on my phone and I would like to \nset it up as my TOTP for HPC login\nIs it possible and how do I do this?\nI tried using the TOTP when I attempt to login but it does not seem to \nwork.\nThank you!\nCheers, Claudio\n– \nClaudio Castelnovo\nProfessor of Theoretical Physics and Fellow of Trinity College\n/Theory of Condensed Matter Group/\nDepartment of Physics, Cavendish Laboratory\nUniversity of Cambridge\n19, J J Thomson Avenue, Cambridge CB3 0HE, UK\nphone: +44 (0)1223 337433\nfax: +44 (0)1223 337356\nweb: [http://www.tcm.phy.cam.ac.uk/profiles/cc726/](http://www.tcm.phy.cam.ac.uk/profiles/cc726/)",
    "expected_answer": "CLASSIFICATION\nCategory: Account Provisioning / MFA\nTechnical Action Required: No (process guidance)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser asks about setting up TOTP for HPC login, but reports login credentials are not accepted. This may indicate either missing CSD3 account provisioning or credential mismatch. The correct flow is to confirm account status first, then proceed with MFA enrollment.\n\nACTION\n1) Ask user to confirm whether they already have an active CSD3 account.\n2) If account is not active/provisioned, direct user to the application form.\n3) If account is active, proceed with standard SSH MFA setup guidance.\n\nEXAMPLE CUSTOMER REPLY\n\nHi Claudio,\n\nBefore proceeding with MFA setup, we first need to confirm your CSD3 account is active.\n\nIf you do not yet have an active CSD3 account, please apply here:\nhttps://www.hpc.cam.ac.uk/rcs-application\n\nOnce your account is active, we can continue with MFA setup for login.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume provisioning status without account verification data.\n- Do not troubleshoot TOTP before confirming account existence/provisioning state.",
    "metadata": {
      "keyword": "TOTP",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-89520",
    "summary": "[UKAEA] Request Machine Info",
    "query": "Good morning, I am doing a benchmarking exercise on the ukaea-spr and ukaea-spr-hbm nodes.\nI've managed to find most of the hardware info I need with some probing,\nbut for completeness I could do with some information as follows:\nNetwork Interconnect\nNetwork Topology\nStorage Device Type (on the RDS filesystem) i.e. SSD/HDD/NVMe\nMany Thanks in Advance,\nAleks",
    "expected_answer": "CLASSIFICATION\nCategory: System Information / Hardware Specs\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser requested platform-level hardware details for benchmarking. Provided high-level interconnect/topology/storage architecture info.\n\nACTION\nProvide requested specs:\n- Interconnect: one Infiniband NDR 200 per node\n- Topology: fat-tree\n- Storage type: Lustre filesystem\n\nEXAMPLE CUSTOMER REPLY\n\nDear Aleks,\n\nFor `ukaea-spr-hbm`, the requested details are:\n- Network interconnect: one Infiniband NDR 200 per node\n- Network topology: fat tree\n- Storage type (RDS): Lustre filesystem\n\nPlease let us know if you need anything further.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Keep responses at approved/public detail level.",
    "metadata": {
      "keyword": "UKAEA",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-89172",
    "summary": "Citrix problem",
    "query": "Hi,\nI have been able to log onto the VPN but when I try to login to Citrix Workspace I just get the error message attached. Are you able to help?\nBest,\nDan",
    "expected_answer": "CLASSIFICATION\nCategory: SRCP Access / Citrix Client Troubleshooting\nTechnical Action Required: No (diagnostic guidance)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser on VPN unable to login via Citrix Workspace app. Initial diagnostic should verify web launch path through `anywhere.cam.ac.uk` and whether SRCP desktop launches from browser. Ticket auto-closed due no response.\n\nACTION\n1) Ask user to test web launch via `https://anywhere.cam.ac.uk/`.\n2) Verify SRCP desktop visibility and app handoff behavior.\n3) If no response, close with reopen guidance.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Daniel,\n\nWhile connected to VPN, please log in via:\nhttps://anywhere.cam.ac.uk/\n\nDo you see your SRCP virtual desktop there, and does launching from the browser open Citrix correctly?\n\nRegards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume Citrix backend fault without basic path validation.",
    "metadata": {
      "keyword": "Citrix",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-89057",
    "summary": "Add DiRAC project to account",
    "query": "Dear Support Team,\nI've recently added another DiRAC project (DP371) to my user account (rab200) via SAFE, but the resources are not showing up as available on CSD3/Wilkes. 'mybalance' doesn't show the project. I think the account code is probably \"DIRAC-DP371-GPU\" but I submissions with that account fail. Could help with this?\nCheers,\nRichard\n==========================================================\nDr. Richard Booth\nRoyal Society University Research Fellow & University Academic Fellow\nSchool of Physics & Astronomy\nW. H. Bragg Building\nUniversity of Leeds, UK\nEmail: r.a.booth@leeds.ac.uk<[r.a.booth@leeds.ac.uk](mailto:r.a.booth@leeds.ac.uk)>\nMy working hours may differ from yours\nI do not expect a reply outside of your own working hours",
    "expected_answer": "CLASSIFICATION\nCategory: Account Provisioning / Project Membership\nTechnical Action Required: Yes (provisioning completion)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser added DiRAC project in SAFE but account not yet visible in `mybalance`. Provisioning is not immediate; staff completed provisioning and issue resolved.\n\nACTION\n1) Check provisioning status for requested project association.\n2) Complete pending provisioning.\n3) Confirm account now visible/usable.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Richard,\n\nProvisioning for this project is not automatic. I have now completed the required provisioning step, so the account should be available.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not treat SAFE change as instantly reflected in Slurm/accounting systems.",
    "metadata": {
      "keyword": "DiRAC",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-88966",
    "summary": "Long queue time",
    "query": "Hello,\nI hope this email finds you well!\nI am working on CSD3 for some cpu jobs, particularly nodes cclake and Icelake. I submitted some jobs last week and I found the queueing time very long. I am wondering if you can give advice on this regarding if there is anything I did wrong causing the long wait. Some general info:\n- Name: Hongyu Zhou\n- Account/PI: N-LAWRENCE-SL3-CPU\n- Working directory: /home/hz488/project/ai_ranking_report\n- Task:\n- Data: I work with bibliographic data on hundreds of millions of paper including their title, abstract, and citation relationships\n- Task:\n- I am trying to calculate the cooccurence frequency of concepts in papers (two concepts used in one paper), for which the concepts are extracted already in our current data (one paper associated with a list of paper).\n- I am building these cooccurrence networks for using papers since 1990 to current and group them to a interval of 3 years\n- I generate networks for every three years.\n- Problem:\n- The queueing time is quite long.\n- Slurm Job_id=10784619 Name=build_cooccur_net Began, Queued time 17:19:49\n- I tried to minimise the potential RAM usage by using smaller intervals, using smaller samples and use sql-like ducked to calculate the concurrences. But in the future, there will definitely be larger jobs I need to run considering the size of my data in nature, especially if we were use Node2Vec to embed the networks we obtained.\n- Job submission script:\n#!/bin/bash\n#SBATCH -J build_cooccur_net\n#SBATCH -A N-LAWRENCE-SL3-CPU\n#SBATCH -p icelake\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=6\n#SBATCH --mem=30G\n#SBATCH --time=4:00:00\n#SBATCH --output=1_build_net_duck_%j.out\n#SBATCH --error=1_build_net_duck_%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=hz488@cam.ac.uk\nset -euo pipefail\necho \"[$(date '+%Y-%m-%d %T')] Job started\"\nmodule purge\nmodule load python/3.8\nsource \"$HOME/hzenv/bin/activate\"\nexport OMP_NUM_THREADS=1\nexport MKL_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=1\nTEMP_DIR=\"${SLURM_TMPDIR:-./temp_edges}\"\nmkdir -p \"$TEMP_DIR\"\npython /home/hz488/project/ai_ranking_report/scripts/1_build_net_save_duckdb.py \\\n--input-dir /home/hz488/project/dimensions/publications/ \\\n--output-dir /home/hz488/project/data/ai_ranking_report/data/1_build_net \\\n--batch-size 200000 \\\n--batches-per-file 50 \\\n--sample-frac 1\necho \"[$(date '+%Y-%m-%d %T')] Job finished”\nCould you kindly advise me on this?\nBest,\nHongyu",
    "expected_answer": "CLASSIFICATION\nCategory: Scheduling / Queue Wait Times / Service Levels\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser on non-paying SL3 CPU saw long waits. Correct explanation: paid workloads have higher priority; queue times fluctuate; cancelling/resubmitting loses accrued priority.\n\nACTION\n1) Reassure no job/account fault detected.\n2) Explain SL3 prioritisation and variable wait times.\n3) Advise against cancel/resubmit behavior.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Hongyu,\n\nThere is nothing wrong with your account or submission. Your jobs are on the non-paying SL3 service level, and paid activity is prioritised ahead of it.\n\nQueue time can vary significantly. Please avoid cancelling/resubmitting, as that can lose accrued priority.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Avoid promising specific start-time estimates on shared queues.",
    "metadata": {
      "keyword": "wait times",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-88843",
    "summary": "Setting Up Local Conda Environment for Intel GPU Usage on DAWN Cluster",
    "query": "Dear HPC Team,\nI am currently using the DAWN cluster and would like to leverage the Intel GPUs for running my PyTorch code.\nAccording to the documentation, the following setup works well:\nmodule load intelpython-conda\nconda activate pytorch-gpu\nAnd in Python:\nimport torch\nimport intel_extension_for_pytorch as ipex\n...\n1. Enable GPU\nmodel = model.to('xpu')\ndata = data.to('xpu')\nmodel = ipex.optimize(model, dtype=torch.float32)\nThis setup functions as expected.\nHowever, the pytorch-gpu environment appears to be a system-provided environment. For the purposes of my project, I would like to create and manage my own local Conda environment that is compatible with the Intel GPUs (i.e., includes PyTorch and the Intel Extension for PyTorch with xpu support).\nI have attempted to clone the pytorch-gpu environment as well as install all of its modules manually, but I run into repeated HTTP 403 errors when trying to fetch packages from the Intel Conda channel. This seems to indicate restricted access or deprecation of some packages or channels.\nCould you please advise on the recommended way to:\n1.  Create a local Conda environment compatible with the Intel GPU setup used on DAWN,\n2.  Ensure installation of all necessary packages without running into access issues?\nIf there is a supported method to export or replicate the pytorch-gpu environment into a user-managed Conda environment, that would be ideal.\nThank you very much for your assistance!\nBest wishes,\nMoritz",
    "expected_answer": "CLASSIFICATION\nCategory: Software Environment / Dawn PVC / PyTorch-IPEX\nTechnical Action Required: No (user setup guidance)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser wants a modifiable local environment equivalent to centrally managed `pytorch-gpu`. Central env is immutable. Supported options:\n- activate central `pytorch-gpu` then layer user virtualenv, or\n- build user conda env with compatible pinned versions from Intel channel and test on PVC nodes.\n\nACTION\n1) Confirm central env baseline works.\n2) Explain central env cannot be modified.\n3) Provide two supported approaches:\n   - `pytorch-gpu` + user virtualenv layering\n   - standalone conda env with compatible pins (`intel-extension-for-pytorch`, `pytorch`, python range) and Intel repo\n4) Require testing on PVC/PVC9 interactive node.\n5) Set required env vars for XPU runtime test.\n\nEXAMPLE CUSTOMER REPLY\n\nHi Moritz,\n\n`pytorch-gpu` is centrally managed and cannot be modified directly.\n\nSupported options are:\n1) Activate `pytorch-gpu`, then activate your own virtualenv for extra packages.\n2) Create your own conda env with compatible versions (e.g. pinned `intel-extension-for-pytorch` + `pytorch`) and test it on PVC nodes.\n\nPlease use the Dawn guidance here:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/pvc.html#machine-learning-data-science-frameworks\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not advise modifying centrally managed environments.",
    "metadata": {
      "keyword": "usage",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-88772",
    "summary": "AIRR/Dawn Storage",
    "query": "Hi\nI am at a bit of a loss as to how access storage provided by AIRR/Dawn?\nMy storage name is airr-p12-storage\nMy user id is dn-stan1.\nI have looked extensively in the support/help docs but must be looking straight past this information.\nThanks\nPhill\n–\nProf Phillip J. Stansfeld (he/him)\nCo-Director of the Sir Howard Dalton Centre | Professor in Computational Biochemistry | School of Life Sciences & Department of Chemistry | University of Warwick\nphillip.stansfeld@warwick.ac.uk<[phillip.stansfeld@warwick.ac.uk](mailto:phillip.stansfeld@warwick.ac.uk)>| External: +44 (0) 24 7652 3864 | Internal: x23864 | I can also be contacted through Microsoft Teams\nStansfeld Research Group<[https://stansfeldresearchgroup.wordpress.com/](https://stansfeldresearchgroup.wordpress.com/)> | Office IBR2.26 | School of Life Sciences | Gibbet Hill Road | Coventry | CV4 7AL | Find us on the interactive map<[https://campus.warwick.ac.uk/search/623c895c421e6f5928c0fa77?projectId=warwick](https://campus.warwick.ac.uk/search/623c895c421e6f5928c0fa77?projectId=warwick)>\n[Logo  Description automatically generated][signature_104923545][signature_4232682299]",
    "expected_answer": "CLASSIFICATION\nCategory: Storage Path Discovery\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser could not locate AIRR/Dawn storage path in docs. The exact project path is user/project-specific and should be discovered from the user account rather than assumed.\n\nACTION\n1) Ask user to check available RDS links under home:\n   - `ls -ld ~/rds/*`\n2) Ask user to identify the AIRR project path from quota/project listing:\n   - `quota`\n3) Confirm the matching path format once identified (typically `~/rds/rds-<project-id>` symlink).\n4) Clarify that `~/rds/...` is a convenience link to the underlying project path.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Phil,\n\nYour AIRR storage path is user/project specific. Please run:\n\n`ls -ld ~/rds/*`\n`quota`\n\nThis will show the project link available in your account (typically `~/rds/rds-<project-id>`).  \nThat entry is a symlink to the underlying project storage location.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not hardcode project-specific paths unless verified for that user/account.",
    "metadata": {
      "keyword": "AIRR",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-87559",
    "summary": "'ReqNodeNotAvail, Reserved for maintenance'?",
    "query": "Hi -\nI’ve just sbatched a job on the HPC, using cclake, job 9082481_[1-2]. If I squeue -u has27 it tells me (under NODELIST) 'ReqNodeNotAvail, Reserved for maintenance’. But maintenance finished last week.\nCan you tell me what (if anything) I’ve done wrong? Script below.\nThanks,\nHamish\n#!/bin/bash\n#SBATCH -A GLOVER-SL3-CPU\n#SBATCH --nodes=1\n#SBATCH --mail-type=NONE\n#SBATCH -p cclake  # name of the partition to run job on\n#SBATCH -o /home/has27/rds/hpc-work/f2reads/bowtie2/bowtie2%a.log\n#SBATCH -c 8         # number of CPUs. Default: 1\n#SBATCH --mem=27360M     # RAM memory.\n#SBATCH -t 12:00:00  # time for the job HH:MM:SS. Default: 1 min\n#SBATCH -a 1-2\nsource ~/.bashrc\n1. activate conda environment\nconda activate bowtie2\ncd /home/has27/rds/hpc-work/f2reads/soapnuke/clean\n1. List all files in the folder starting with A or B,\n2. then get the relevant line\nINDIV_ID=$(ls -d [AB]* | head -n ${SLURM_ARRAY_TASK_ID} | tail -n 1)\ncd $INDIV_ID\nREAD1=$(ls *1.fq.gz)\nREAD2=$(ls *2.fq.gz)\ncd /home/has27/rds/hpc-work/Genomes/LSRagtag\nbowtie2 -p 8 -x LSRagtag -1 /home/has27/rds/hpc-work/f2reads/soapnuke/clean/$INDIV_ID/$READ1 -2 /home/has27/rds/hpc-work/f2reads/soapnuke/clean/$INDIV_ID/$READ2 | samtools sort > /home/has27/rds/hpc-work/f2reads/align-LSRagtag/$INDIV_ID.sorted.bam\n—\nDr Hamish Symington\nJunior Research Fellow, Queens’ College, Cambridge\nAcademic Skills Tutor and Bye-Fellow, Downing College, Cambridge\nPostdoctoral Researcher in the Glover Lab, Department of Plant Sciences, University of Cambridge",
    "expected_answer": "CLASSIFICATION\nCategory: Slurm Scheduling Status Interpretation\nTechnical Action Required: No (explanation + minor script advice)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser saw scheduler reason text and assumed post-maintenance fault. Actual job state was normal pending by priority. Additional optimisation: explicit `--mem` is redundant for cclake in this case.\n\nACTION\n1) Check live job state and reason (`squeue`, `scontrol`).\n2) Explain pending-by-priority behavior.\n3) Suggest removing redundant `--mem` directive when CPU request already implies desired memory.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Hamish,\n\nYour job has submitted correctly and is pending under normal scheduler priority.\n\nFor this script, `--mem=27360M` is redundant if you already request the corresponding CPU count on `cclake`; removing it can simplify scheduling requests.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Distinguish temporary reason strings from real submission errors.",
    "metadata": {
      "keyword": "maintenance",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-85932",
    "summary": "Blank lines through QR code when trying to set up MFA",
    "query": "Dear sir/madam,\nI have been told to contact this email to ask for an MFA because I see the error: ‘When performing ssh to multi.hpc.cam.ac.uk the QR code is replaced either by garbage or is broken up by blank lines, making it unusable.’.\nThank you very much for sorting.\nMany thanks,\nJames",
    "expected_answer": "CLASSIFICATION\nCategory: MFA Enrollment / Client Rendering Issue\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nMac terminal rendering can distort SSH QR code during MFA enrollment. Enrollment can still proceed by scanning if possible, or manually entering the secret key in authenticator app.\n\nACTION\n1) Confirm issue is known for some Mac terminal setups.\n2) Provide manual secret-key fallback method.\n3) Clarify expected token labels for SSH vs web.\n\nEXAMPLE CUSTOMER REPLY\n\nHi James,\n\nThis is a known issue on some Mac setups where the QR may render poorly. You can still complete setup by manually entering the secret key in your authenticator app if scanning fails.\n\nPlease follow:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html\n\nThe SSH token should appear as `CSD3: SSH Login` (distinct from login-web token naming).\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not reset MFA unnecessarily when manual enrollment path is available.",
    "metadata": {
      "keyword": "MFA",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-85626",
    "summary": "Freesurfer 8 crashing",
    "query": "Dear HPC team,\nGood afternoon, I am contacting you since I am trying to run Freesurfer 8.0.0 on HPC but I am getting the following error:\nFREESURFER: Undefined variable.\nThis makes me wonder if for some reason the path to freesurfer cannot be found. Since this is a new module can you please advise on the path where the application is so that I can use in my scripts? Has this been tested and was running as intended?\nNeedless to say I have loaded the module.\nBest wishes,\nMarialena",
    "expected_answer": "CLASSIFICATION\nCategory: Software Module Integration / Environment Setup\nTechnical Action Required: Yes (module/setup correction)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser loaded `freesurfer/8.0.0` but runtime failed with `FREESURFER: Undefined variable`, then later license-related failure and temporary modulefile regression during live edits. Working path required explicit environment setup and user license variable.\n\nACTION\n1) Reproduce issue and validate module behavior.\n2) Instruct explicit setup sequence:\n   - `module load freesurfer/8.0.0`\n   - `source $FREESURFER_HOME/SetUpFreeSurfer.sh`\n3) Handle license requirement:\n   - user obtains license file\n   - user exports `FS_LICENSE=/path/to/license.txt`\n4) If modulefile error introduced during updates, fix and ask user to retry.\n5) Confirm successful execution.\n\nEXAMPLE CUSTOMER REPLY\n\nHi Marialena,\n\nPlease run:\n\nmodule load freesurfer/8.0.0\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\nFor FreeSurfer 8, also set your license path in your environment, e.g.:\n\nexport FS_LICENSE=/path/to/license.txt\n\nThen retry `recon-all`.\n\nBest wishes,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not request users place license file in protected system directories.\n- Avoid modifying live module files without caution and rollback plan.",
    "metadata": {
      "keyword": "crash",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-83106",
    "summary": "SKA-SRC: Configuring an Azimuth instance with GPU and MPI compatibility",
    "query": "Hi Support Team,\nI’m working on some SRCNet related projects and am attempting to run a containerised workflow on Azimuth which requires both CUDA and MPI support. I have built the required software built into an Ubuntu container with specific CUDA and MPI versions, but these of course need to be compatible with what is possible to deploy on the instance itself.\nIs there a guide to how to set these up correctly? Or some experience with running GPU and MPI codes on Azimuth that I can leverage so that we can progress the SRC work?\nI’m happy to provide the Dockerfile so that you can see how things are being built, and information about the code I’m trying to run, if useful.\nKind regards,\nBradley Meyers\nSupport Scientist (Time Domain) | Australian SKA Regional Centre (AusSRC)\nResearch Fellow | Curtin Institute of Radio Astronomy (CIRA)\nInternational Centre for Radio Astronomy Research (ICRAR)\nCurtin University\nPronouns | he/him/his<[https://theconversation.com/what-are-gender-pronouns-and-why-is-it-important-to-use-the-right-ones-169025](https://theconversation.com/what-are-gender-pronouns-and-why-is-it-important-to-use-the-right-ones-169025)>\nTel | +61 8 9266 7569\nEmail | bradley.meyers@curtin.edu.au\nWeb | aussrc.org<[https://aussrc.org/](https://aussrc.org/)> | curtin.edu.au<[http://www.curtin.edu.au/](http://www.curtin.edu.au/)>\n[B+wafnCZ37R4AAAAAElFTkSuQmCC]\nCRICOS Provider Code 00301J\nCurti​n would like to pay respect to the Aboriginal and Torres Strait Islander members\nof our community by acknowledging the traditional owners of the land on which the\nPerth campus is located, the Whadjuk people of the Nyungar Nation; and on our\nKalgoorlie campus, the Wongutha people of the North-Eastern Goldfields.",
    "expected_answer": "CLASSIFICATION\nCategory: Cloud VM Configuration / GPU Containers / MPI\nTechnical Action Required: No (guidance)\nEscalation Required: Optional (if capacity constrained)\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser needed guidance for CUDA+MPI in Azimuth GPU VM. Recommended approach: install CUDA toolkit and Nvidia driver in VM, prefer Singularity for HPC container runtime with GPU bindings, and choose MPI runtime inside container as needed. Capacity caveat: GPU pool may be fully utilised.\n\nACTION\n1) Confirm current GPU capacity availability.\n2) Advise VM-level CUDA/driver install for GPU instances.\n3) Recommend Singularity/Apptainer over Docker for HPC runtime.\n4) Explain MPI runtime can be selected within container image.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Bradley,\n\nOn Azimuth GPU VMs, you can install the CUDA toolkit and NVIDIA open-source driver in the usual way. For HPC workflows we generally recommend Singularity/Apptainer rather than Docker, including GPU bindings to expose host GPUs.\n\nMPI runtime selection is up to your container build choice.\n\nPlease note GPU capacity may currently be fully allocated, so provisioning can be constrained.\n\nThanks,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply immediate GPU availability when capacity is saturated.",
    "metadata": {
      "keyword": "SKA",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-82270",
    "summary": "Activating Conda Environment in Jupyter Notebook (Login-Web Interface)",
    "query": "Dear Sir/Madam,\nI hope you are doing well.\nI am writing to inquire about activating a Conda environment in Jupyter\nNotebook within the Login-Web interface. I have already created a Conda\nenvironment with ipykernel installed. However, when I start a Jupyter\nNotebook session, I am unable to switch the kernel to my Conda environment.\nCould you please provide guidance on how to properly activate and use my\nConda environment in Jupyter Notebook?\nThank you for your time and assistance.\nBest regards,\nPatrick",
    "expected_answer": "CLASSIFICATION\nCategory: Jupyter / Login-Web / User Environment\nTechnical Action Required: No\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser could not select a conda environment kernel in Login-Web Jupyter. Resolution is to ensure the environment has `ipykernel` and is explicitly registered as a Jupyter kernel, then restart/reopen the notebook session.\n\nACTION\n1) Confirm environment prerequisites\n   - Environment exists and can be activated.\n   - `ipykernel` is installed in that environment.\n2) Register the environment as a Jupyter kernel\n   - Activate env and run:\n     `python -m ipykernel install --user --name <env_name> --display-name \"Python (<env_name>)\"`\n3) Ask user to restart/reopen Login-Web Jupyter session and select the new kernel.\n4) If kernel still does not appear, collect diagnostics:\n   - `jupyter kernelspec list`\n   - exact env activation/install commands used.\n5) Share official docs for full workflow/reference.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Patrick,\n\nPlease register your conda environment as a Jupyter kernel, then restart/reopen your Login-Web Jupyter session.\n\nExample:\n\n```bash\nconda activate <env_name>\npython -m ipykernel install --user --name <env_name> --display-name \"Python (<env_name>)\"\n```\n\nAfter that, select `Python (<env_name>)` from the Jupyter kernel menu.\n\nFull guidance:\nhttps://docs.hpc.cam.ac.uk/hpc/user-guide/login-web.html#adding-virtual-environments\n\nIf it still doesn’t appear, please share:\n- `jupyter kernelspec list`\n- the exact commands you used to create and register the environment.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Keep instructions aligned with the documented Login-Web kernel registration workflow.",
    "metadata": {
      "keyword": "login-web",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-82248",
    "summary": "Unable to install packages for R in Citrix Workspace",
    "query": "Dear whom it may concern,\nproject id: pnk0p5ltryy\nI hope this email finds you well. I am using RStudio as directed through RDS Farm. However, when I try to install packages(basic ones like dplyr and tidyverse),error occurs saying \"error reading from connection\". I tried to use web to access CRAN, but it seems I don't have internet connection on the virtual workspace. I would really appreciate your guidance, thank you very much.\nSincerely,\nYira",
    "expected_answer": "CLASSIFICATION\nCategory: SRCP / Package Management / Restricted Network\nTechnical Action Required: Yes (staff-managed package availability)\nEscalation Required: Yes (SRCP team)\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser on SRCP RStudio could not install CRAN packages due to no external internet by design. SRCP policy requires curated package availability managed by staff; users may need to use internal repo mirror and request missing packages.\n\nACTION\n1) Explain SRCP security model (no direct external repo access).\n2) Escalate to SRCP team for package provisioning.\n3) Ask user for package list.\n4) Provide internal repo usage (`options(repos = \"http://srcp-r-repo/\")`).\n5) Track/fork to dedicated package request ticket if needed.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Yira,\n\nOn SRCP, direct access to external package repositories is restricted by design. Please use the internal R repo:\n\noptions(repos = \"http://srcp-r-repo/\")\navailable.packages()\n\nIf required packages are still missing, please send the exact package list and we can add them through the SRCP process.\n\nRegards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not suggest bypassing SRCP network restrictions.",
    "metadata": {
      "keyword": "Citrix",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-81680",
    "summary": "Re: New Tier2 Account on CSD3 (jr908)",
    "query": "Dear HPC support,\nI am a PhD student with Pietro Lio project cs181 which has access to the Ampere node partition.\nI have a need for some CPU only runs on the icelake nodes and was wondering if there's anyway to get this?\nMany thanks\nJames Rowbottom - jr908\n________________________________\nFrom: support@hpc.cam.ac.uk <support@hpc.cam.ac.uk>\nSent: 09 December 2024 17:25\nTo: James Rowbottom <jr908@cam.ac.uk>\nSubject: New Tier2 Account on CSD3 (jr908)\nWelcome to the Cambridge Service for Data Driven Discovery (CSD3)\nfacility. CSD3 is comprised of distinct CPU and GPU systems:\nCSD3-CPU      - Intel Cascade Lake, Ice Lake and Sapphire Rapids\nHDR InfiniBand CPU clusters\nWilkes3-GPU   - NVIDIA A100/Mellanox HDR InfiniBand GPU cluster.\nThe O/S on CSD3 nodes is Rocky Linux 8, a rebuild of RHEL8,\nwhere RHEL=Red Hat Enterprise Linux which is compatible\nwith CentOS, Scientific Linux and others but NOT Ubuntu.\nFor more information on the service please see:\ndocs.hpc.cam.ac.uk\nConnecting\n==========\nBefore you login for the first time, please use your SSH\npublic key to login via SSH to\nmulti.hpc.cam.ac.uk\nto generate a TOTP MFA secret for your phone. multi.hpc has\nthe same host keys as the login nodes (see below). For more\ninformation about MFA on CSD3 please see\n[https://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html](https://docs.hpc.cam.ac.uk/hpc/user-guide/mfa.html) .\nOnce MFA is configured, please use your SSH public key to\nlogin via SSH to\nlogin.hpc.cam.ac.uk\nThis destination will take you to one of a set of login nodes which\nmay have either Cascade Lake or Ice Lake CPUs. If you prefer a\nparticular CPU type (e.g. because you are developing code to run on\nthat type) you may alternatively SSH to\nlogin-cascadelake.hpc.cam.ac.uk or login-icelake.hpc.cam.ac.uk to\nreach nodes of that type.\nWe allow unrestricted SSH access to the CSD3 login nodes,\nbut have protected them with Fail2Ban. This means that after\nrepeated failed login attempts from the same origin, that origin\nwill be blocked from further access for 20 minutes.\nThe SSH host keys used by all login nodes have the following\nfingerprints (which of these hashes you see will depend on your\nclient):\nRSA\nMD5:fd:5c:6b:7d:49:95:2f:da:7f:5c:50:9a:bb:ef:3f:24\nSHA256:2rl+MXd9rsrDzFZwEItmhhiHTlLTIqN0d3TSGLTgjTI\nED25519\nMD5:eb:e3:a1:f0:64:68:cf:9c:63:da:84:db:2e:ee:15:83\nSHA256:nFVSXK+VRGCaUupQEdhXzO6kp01m2fzzmbgPr0sc2so\nAs well as SSH login, connection is possible via x2go and VNC (the\nlatter needs to be tunnelled over SSH).\nStorage\n=======\nEach user has a hard quota of 50GiB on their home directory. This is\nprovided over NFS and hourly/daily/weekly snapshots are made\nautomatically which can be searched by the user. E.g. if you delete a\nfile by accident look in the hidden .snapshot directory and browse the\nbest snapshot for the file (which can be copied back to your home\ndirectory).\nThe scratch storage has been implemented as part of the Cambridge\nResearch Data Storage (RDS) service. Please look under ~/rds/ for your\nproject's scratch directory (rds-t2-cs181). Please note that\nthis is shared per project so each user will probably wish to create\ntheir own subdirectory.\nData to be read by jobs (including software installations) and output\nfiles from jobs should be placed in RDS areas, NOT the home\ndirectory. This is because /home is NFS-based and lacks the\nscalability of the lustre-based RDS. I/O to /home by jobs can create\nglobal system issues.\nUser environment\n================\nCSD3 nodes currently run a common RHEL8-compatible operating system\nbut present additional development packages tuned to the node\nhardware. We recommend that all optimised application binaries be\ncompiled on login nodes matching the target nodes as closely as\npossible.\nFor development on cclake* partitions, the best choice is\nlogin-cascadelake.hpc.cam.ac.uk .\nOn icelake* and sapphire* partitions, the best choice is\nlogin-icelake.hpc.cam.ac.uk .\nNote that the login-icelake nodes do not have GPUs and have a different\n(Intel) CPU type to the (AMD) ampere GPU nodes, so ampere development may\nrequire interactive use of a GPU node (see\n[https://docs.hpc.cam.ac.uk/hpc/user-guide/interactive.html#sintr](https://docs.hpc.cam.ac.uk/hpc/user-guide/interactive.html#sintr)\n).\nEnvironment modules loading additional software packages are now\nbeing generated by spack. There are different spack package trees\nfor each node hardware type.\nThe module names contain hashes uniquely identifying the build options\nand dependencies but the modules can be searched and located using the\nspack command. E.g. to search for modules providing HDF5 1.10.1\ncompiled using GCC, search for existing builds using\nspack find -v hdf5@1.10.1%gcc\nwhich will list matching builds and also show which configure options\nwere on :plus: or off (~) . Less version information will show more\nchoices, e.g. one might start off with just spack find hdf5%gcc.\nFull details of any matching builds including dependencies (e.g. which\nMPI was used) can be extracted with\nspack find -dvl hdf5@1.10.1%gcc@5.4.0\nExisting builds have modules which can be identified with a command\nsuch as\nspack module find hdf5@1.10.1+mpi %gcc@5.4.0 ^openmpi@1.10.7\nwhich returns the name of the corresponding module\n(hdf5-1.10.1-gcc-5.4.0-i52euam). Note that it is sometimes convenient\nto search using the short hash form identifying each build (printed at\nthe start of line of output by the spack find -dvl command). E.g.\nspack module find /i52euam\n(notice the / introducing the hash).\nMissing builds can be requested. (For more information about spack\nsee [https://eur03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fspack.readthedocs.io%2Fen%2Flatest%2Findex.html&data=05%7C02%7Cjr908%40universityofcambridgecloud.onmicrosoft.com%7Cb2a72e3346134cb8757b08dd187683f7%7C49a50445bdfa4b79ade3547b4f3986e9%7C1%7C0%7C638693619533696586%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=GUPkCeyukD4EZY8jc2oXOIpFpIpHahjHvOyx2dOSqx4%3D&reserved=0.)<http://spack.readthedocs.io/en/latest/index.html](https://eur03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fspack.readthedocs.io%2Fen%2Flatest%2Findex.html&data=05%7C02%7Cjr908%40universityofcambridgecloud.onmicrosoft.com%7Cb2a72e3346134cb8757b08dd187683f7%7C49a50445bdfa4b79ade3547b4f3986e9%7C1%7C0%7C638693619533696586%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=GUPkCeyukD4EZY8jc2oXOIpFpIpHahjHvOyx2dOSqx4%3D&reserved=0.)%3Chttp://spack.readthedocs.io/en/latest/index.html)>\nSubmitting jobs\n===============\nCSD3 uses Slurm for job scheduling. Please use the command\nmybalance\nto show your project, your current usage and the remaining balance in\ncompute unit hours.\nOn CSD3 the type of unit hour represented in mybalance output depends\non whether the project controls resources on a CPU partition (-CPU\nprojects) or a GPU partition (-GPU projects), i.e.\n- projects ending in -CPU use CPU core hours\n- projects ending in -GPU use GPU hours\nThe project listed by mybalance is the project you should specify in\nSlurm submissions either through\n#SBATCH -A t2-cs181-XXX\nin the job submission script or equivalently on the command line with\nsbatch -A t2-cs181-XXX ...\nCascade Lake\n------------\nCascade Lake jobs require the partitions cclake or cclake-himem, i.e.\n#SBATCH -p cclake\nor\n#SBATCH -p cclake-himem\nand will be allocated the number of cpus required for the number of\ntasks requested and a corresponding amount of memory. By default, the\ncclake partition provides 1 cpu and 3410MiB of RAM per task, and the\ncclake-himem partition provides 1 cpu and 6840MiB per task.\nRequesting more cpus per task, or more memory per task, may both\nincrease the number of cpus allocated (and hence the charge). It is\nmore cost efficient to submit jobs requiring more than 3410MiB per task\nto the -himem partition since more memory per cpu is available\nthere. Hyperthreading is disabled on the CPU nodes so there is no\ndistinction between cpus and cores - each node has 56 cores in total.\nIce Lake and Sapphire Rapids\n----------------------------\nIce Lake and Sapphire Rapids partitions work in a similar way with the\nfollowing differences:\nThe icelake partition provides 1 cpu and 3370MiB per task by default.\nThe icelake-himem partition provides 1 cpu and 6760MiB per task by default.\nEach node has 76 cpus.\nThe sapphire partition provides 1 cpu and 4580MiB per task by default.\nEach node has 112 cpus.\nWilkes3-GPU\n-----------\nGPU jobs require the partition ampere (#SBATCH -p ampere) and may\nrequest any number of GPUs per node from the range 1 to 4, which is\ndone via the directive\n#SBATCH --gres=gpu:N\nwhere 1 <=  N <= 4.\nEach GPU node contains 4 NVIDIA A100-SXM-80GB GPUs, with 1000GiB RAM and\ntwo AMD EPYC 7763 64-Core processors.\nAny jobs requesting more than one node must request 4 GPUs per\nnode. Jobs less than one node in size will be prevented from\nrequesting more than 32 CPUs per GPU. The enforcement is performed by\na job submission filter which will produce an explanatory message if\nit rejects a job outright.\nThe service levels defined for DiRAC jobs are currently:\n4256 CPU cores per user maximum for a maximum of 36 hours of wallclock time per job\n64 GPUs per user maximum for a maximum of 36 hours of wallclock time per job\nIn addition it is possible to submit low priority jobs of no more than\n12 hours duration to a special project project-sl4-type (where\ntype=cpu or gpu) which allows a limited number of jobs to run when the\nalternative is leaving nodes idle.\nPlease email support@hpc.cam.ac.uk if any problems are encountered.\nKind regards,\nResearch Computing Services",
    "expected_answer": "CLASSIFICATION\nCategory: Account Entitlements / CPU Access\nTechnical Action Required: Yes (entitlement check)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser on a Tier2 GPU project requested CPU-only runs on icelake. The correct response is to first verify current CPU account entitlements (e.g. via `mybalance`) before advising which account to use.\n\nACTION\n1) Verify user’s current account entitlements (`mybalance` or equivalent account view).\n2) If an SL3 CPU account is present, instruct user to submit CPU jobs with that account.\n3) If no CPU account is present, provide the correct request/provisioning path.\n4) Clarify SL2 path for larger/longer CPU requirements where applicable.\n\nEXAMPLE CUSTOMER REPLY\n\nHi James,\n\nPlease check your available project accounts with:\n`mybalance`\n\nIf you see an SL3 CPU account there, you can use that for CPU-only runs on the relevant CPU partition.\n\nIf you need larger/longer CPU resources than SL3 allows (or no CPU account is listed), please request the appropriate SL2 CPU access via your group lead.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Confirm actual entitlement list before advising new provisioning.",
    "metadata": {
      "keyword": "Tier2",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-81206",
    "summary": "Can't compile code: error: icc: warning #10352: The directory '/tmp' is full.  Please check disk space.",
    "query": "Dear Sir or Madame,\nCRSID: leh50\nAccount: CIA-DAMTP-SL2-CPU\nLocation:\n/home/leh50/PLUTOSTUFF_CSD3/PLUTO_v4pt4Patch2_MonJun282021Download/PLUTO/RESEARCH_PROJECTS/08BrokenDiskProject/03BrokenDiskQuasi2DSims_Re4687/VSTRHYDRO2DRes32Re4687BrokenDiskForcedBreak22a0pt05Displace8HzBCReflectiveLx60H_2DHDIsoth_1920x0p008x768_ZBReflective_NonIdealNoMassSourceTermICDifferentVelocityPerts\nCode I'm trying to compile: C using mpicc\nError:\nWhen I try to compile the code I get the following error\nicc: warning #10352: The directory '/tmp' is full.  Please check disk space.\nI didn't have any issues trying to compile the code a few days ago. According to \"quota\":\nFilesystem/Project    GB        quota     limit          grace           files    quota    limit   grace User/Grp/Proj\n/home                 11.8       50.0      55.0                                    -    ------- No File Quotas  -------     U:leh50\n/rds-d7                0.0     1099.5    1209.5                                 -        1  1048576  1048576             - P:45565\n/rds-d2                0.0        0.0       0.0                                           -        1        0        0       - G:rds\nrds-xg257ahCixg    19271.6    20000.0   20000.0           -  1096248 10240000 10240000   - P:92227\nI don't know what the /tmp folder is. As far as I know I haven't stored anything there. I would greatly appreciate your help with this.\nBest,\nDr Loren E. Held\nPostdoctoral Researcher\nAstrophysical Fluid Dynamics Group\nDepartment of Applied Mathematics and Theoretical Physics (DAMTP)\nUniversity of Cambridge",
    "expected_answer": "CLASSIFICATION\nCategory: Login Node Health / Temporary Filesystem\nTechnical Action Required: Yes (diagnose node-specific issue)\nEscalation Required: Optional (if persistent)\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser reports compiler warning that `/tmp` is full and asks whether this relates to their own quota. `/tmp` is node-local temporary storage and can fill independently of the user’s `/home` or project quota.\n\nACTION\n1) Explain what `/tmp` is:\n   - temporary local filesystem on the node,\n   - not the same as user/project quota shown by `quota`.\n2) Ask user to report the node where error occurs (`hostname`).\n3) Suggest immediate workaround: try compiling on another login node.\n4) Escalate if issue persists across multiple nodes or for multiple users.\n\nEXAMPLE CUSTOMER REPLY\n\nHi Loren,\n\nThe warning about `/tmp` means temporary space on the current node is full. This is separate from your `/home`/project quota, so your `quota` output can still look normal.\n\nPlease share the output of:\n`hostname`\n\nAs a workaround, please try compiling on a different login node. If the issue appears on multiple nodes, let us know and we will escalate.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not assume user quota issue when warning explicitly references `/tmp`.",
    "metadata": {
      "keyword": "disk",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-81010",
    "summary": "Assistance Required: Trouble Logging into Dirac",
    "query": "Hi ,\nI am writing to seek your assistance with logging into the Dirac machine. I encountered a similar issue before.\nI have completed all the initial setup steps and obtained the QR code from the web portal token some time ago. However, it has been a while since I last logged into the machine. While I can still access the safe account, I am unable to log in to the web portal( ( Link<[https://id.hpc.cam.ac.uk/auth/realms/CSD3/protocol/openid-connect/auth?response_type=code&scope=openid&client_id=login-web.hpc.cam.ac.uk&state=D0OAg-WHtN1N7fTXG-9dpjl1oOY&redirect_uri=https%3A%2F%2Flogin-web.hpc.cam.ac.uk%2Foidc&nonce=equ0MBTPKf0h82CBRborxJ-OMzVTwl-UTQPZ2TuGe4s](https://id.hpc.cam.ac.uk/auth/realms/CSD3/protocol/openid-connect/auth?response_type=code&scope=openid&client_id=login-web.hpc.cam.ac.uk&state=D0OAg-WHtN1N7fTXG-9dpjl1oOY&redirect_uri=https%3A%2F%2Flogin-web.hpc.cam.ac.uk%2Foidc&nonce=equ0MBTPKf0h82CBRborxJ-OMzVTwl-UTQPZ2TuGe4s)> ).\nCould you please help me resolve this issue?\nThank you for your support.\nBest regards,\nKarthika",
    "expected_answer": "CLASSIFICATION\nCategory: Authentication / Web Portal Login (Dirac)\nTechnical Action Required: No (credential workflow guidance)\nEscalation Required: No\nUrgency: Medium\n\nQUICK ASSESSMENT\nUser unable to log into web portal despite SAFE access. Correct flow: use SAFE-provided password and web TOTP (`CSD3:<userid>`). If login flow appears inconsistent, refresh/reset SAFE password and retry.\n\nACTION\n1) Confirm user ID and expected credential pair (SAFE password + web token label).\n2) If not prompted correctly or repeated failure, request password reset through SAFE.\n3) Confirm successful login after reset.\n\nEXAMPLE CUSTOMER REPLY\n\nHello Karthika,\n\nPlease use the SAFE-provided password together with the web token labelled `CSD3:dc-bhuv1`.\n\nIf this still fails, please request a new password through SAFE and retry.\n\nBest regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Distinguish web TOTP label from SSH token label.",
    "metadata": {
      "keyword": "DiRAC",
      "gold_version": "v1",
      "hop_count": 1
    }
  },
  {
    "id": "HPCSSUP-65611",
    "summary": "Using Checkpointing on SLURM (AlphaFold)",
    "query": "Dear HPC Support team,\ncould I receive advice on using checkpoints when running a slurm script for AlphaFold 2.3.2?\nI have found the following documentation useful:[https://cluster.hpcc.ucr.edu/~jhayes/slurm/16.05.4/checkpoint_blcr.html](https://cluster.hpcc.ucr.edu/~jhayes/slurm/16.05.4/checkpoint_blcr.html)\nsbatch\nSeveral options have been added to support checkpoint restart:\n- --checkpoint: Specifies the interval between periodic checkpoint of a batch job. By default, the job will have no checkpoints created. Acceptable time formats include \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:minutes:seconds\".\n- --checkpoint-dir:Specify the directory when the checkpoint image files of a batch job will be stored. The default value is the current working directory. Checkpoint files will be of the form \"<job_id>.ckpt\" for jobs and \"<job_id>.<step_id>.ckpt\" for job steps.\nEnvironment variables are available for all of these options:\nSLURM_CHECKPOINT is equivalent to --checkpoint:\n- SLURM_CHECKPOINT_DIR is equivalent to --checkpoint-dir\nhowever, the --checkpoint command does not work from a script of from terminal. According to the slurm-rest-23.02 update log (which I believe is the current version of slurm on HPC):\n\"* The checkpoint plugin interface and all associated API calls have been removed.\"\nDoes this mean it is not possible to use checkpoints on HPC?\nThank you for your help,\nLisa",
    "expected_answer": "CLASSIFICATION\nCategory: Job Resilience / Checkpointing Policy\nTechnical Action Required: No (capability clarification)\nEscalation Required: No\nUrgency: Low\n\nQUICK ASSESSMENT\nUser asked about Slurm native `--checkpoint` functionality. Platform does not provide native Slurm checkpoint plugin support; users must implement checkpointing at application/workflow level. `dmtcp` module exists but compatibility with AlphaFold is user-dependent.\n\nACTION\n1) Clarify no native Slurm checkpoint plugin on this cluster.\n2) Recommend user-level checkpoint/restart implementation.\n3) Mention `dmtcp` as optional tool to evaluate.\n\nEXAMPLE CUSTOMER REPLY\n\nDear Lisa,\n\nNative Slurm checkpoint/restart is not enabled on this cluster, so `--checkpoint` options are not available here.\n\nCheckpointing must be implemented at application/workflow level. We do provide a `dmtcp` module, though compatibility with AlphaFold would need to be tested in your workflow.\n\nKind regards,\nHPC Support\n\nSAFETY / POLICY NOTES\n- Do not imply Slurm-native checkpoint flags are supported when plugin is absent.",
    "metadata": {
      "keyword": "checkpoint",
      "gold_version": "v1",
      "hop_count": 1
    }
  }
]
