services:
  rag-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    environment:
      - POLARIS_CONFIG=/app/config/config.yaml
      - POLARIS_LLM_API_KEY=${POLARIS_LLM_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - JIRA_API_TOKEN=${JIRA_API_TOKEN}
      - EMBED_API_BASE=http://embed:80/v1
      # If you want to override API bases for LLM/embeddings inside Docker:
      # - LLM_API_BASE=http://llm:8080/v1
      # - EMBED_API_BASE=http://embed:8080/v1
    ports:
      - "8000:8000"
    volumes:
      # Mount config if you want to edit without rebuild
      - ./config:/app/config:ro
      # Persist dir for your StorageContext (highly recommended)
      - ./persist:/app/persist
      - ./data:/app/data:ro
    depends_on:
      qdrant:
        condition: service_started
      embed:
        condition: service_healthy

  qdrant:
    image: qdrant/qdrant:v1.14.1
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_data:/qdrant/storage

  embed:
    image: ghcr.io/huggingface/text-embeddings-inference:hpu-latest
    runtime: habana
    restart: unless-stopped
    ipc: host
    cap_add:
      - SYS_NICE
    environment:
      - HABANA_VISIBLE_DEVICES=7
      - HF_TOKEN=${HF_TOKEN}
      - RUST_BACKTRACE=1
    volumes:
      - /mnt/data/ac2650/test:/data
    ports:
      - "8081:80"
    command: >
      --model-id Qwen/Qwen3-Embedding-8B
      --dtype bfloat16
      --pooling last-token
      --auto-truncate
      --max-concurrent-requests 16
      --max-client-batch-size 8
      --max-batch-requests 8
      --max-batch-tokens 2048
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:80/health >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
      start_period: 120s

  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    environment:
      # Inside docker, use service name rag-api
      - RAG_API_BASE=http://rag-api:8000
    ports:
      - "8501:8501"
    depends_on:
      - rag-api
